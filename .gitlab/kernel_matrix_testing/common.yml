# --- Common scripts
.shared_filters_and_queries:
  - FILTER_TEAM="Name=tag:team,Values=ebpf-platform"
  - FILTER_MANAGED="Name=tag:managed-by,Values=pulumi"
  - FILTER_STATE="Name=instance-state-name,Values=running"
  - FILTER_PIPELINE="Name=tag:pipeline-id,Values=${CI_PIPELINE_ID}"
  - FILTER_ARCH="Name=tag:arch,Values=${ARCH}"
  - FILTER_INSTANCE_TYPE="Name=tag:instance-type,Values=${INSTANCE_TYPE}"
  - FILTER_TEST_COMPONENT="Name=tag:test-component,Values=${TEST_COMPONENT}"
  - QUERY_INSTANCE_IDS='Reservations[*].Instances[*].InstanceId'
  - QUERY_PRIVATE_IPS='Reservations[*].Instances[*].PrivateIpAddress'

.wait_for_instance:
  - !reference [.shared_filters_and_queries]
  # Touch .instance_not_found to mark the failure reason to the tag-ci-job task later
  - |
    COUNTER=0
    while [[ $(aws ec2 describe-instances --filters $FILTER_TEAM $FILTER_MANAGED $FILTER_STATE $FILTER_PIPELINE $FILTER_TEST_COMPONENT $FILTER_INSTANCE_TYPE --output text --query $QUERY_INSTANCE_IDS  | wc -l ) != "1" && $COUNTER -le 80 ]]; do COUNTER=$[$COUNTER +1]; echo "[${COUNTER}] Waiting for instance"; sleep 30; done
    # check that instance is ready, or fail
    if [ $(aws ec2 describe-instances --filters $FILTER_TEAM $FILTER_MANAGED $FILTER_STATE $FILTER_PIPELINE $FILTER_TEST_COMPONENT $FILTER_INSTANCE_TYPE --output text --query $QUERY_INSTANCE_IDS | wc -l) -ne "1" ]; then
        echo "Instance NOT found"
        touch ${CI_PROJECT_DIR}/instance_not_found
        "false"
    fi
    echo "Instance found"
    INSTANCE_ID=$(aws ec2 describe-instances --filters $FILTER_TEAM $FILTER_MANAGED $FILTER_STATE $FILTER_PIPELINE $FILTER_TEST_COMPONENT $FILTER_INSTANCE_TYPE --output text --query $QUERY_INSTANCE_IDS)
    aws ec2 wait instance-status-ok --instance-ids $INSTANCE_ID
    sleep 10

.write_ssh_key_file:
  - touch $AWS_EC2_SSH_KEY_FILE && chmod 600 $AWS_EC2_SSH_KEY_FILE
  - $CI_PROJECT_DIR/tools/ci/fetch_secret.sh $AGENT_QA_E2E ssh_key > $AWS_EC2_SSH_KEY_FILE || exit $?
  # Without the newline ssh silently fails and moves on to try other auth methods
  - echo "" >> $AWS_EC2_SSH_KEY_FILE
  - chmod 600 $AWS_EC2_SSH_KEY_FILE

# needs variables: ARCH, INSTANCE_TYPE
.get_instance_ip_by_type:
  - INSTANCE_IP=$(aws ec2 describe-instances --filters $FILTER_TEAM $FILTER_MANAGED $FILTER_STATE $FILTER_PIPELINE $FILTER_TEST_COMPONENT $FILTER_INSTANCE_TYPE --output text --query $QUERY_PRIVATE_IPS)
  - echo "$ARCH-instance-ip" $INSTANCE_IP

# needs variables: INSTANCE_IP, AWS_EC2_SSH_KEY_FILE
.setup_ssh_config:
  - mkdir -p ~/.ssh && chmod 700 ~/.ssh
  - echo -e "Host metal_instance\nHostname $INSTANCE_IP\nUser ubuntu\nStrictHostKeyChecking no\nIdentityFile $AWS_EC2_SSH_KEY_FILE\n" | tee -a ~/.ssh/config
  - chmod 600 ~/.ssh/config

.kmt_new_profile:
  - mkdir -p ~/.aws
  - $CI_PROJECT_DIR/tools/ci/fetch_secret.sh $AGENT_QA_E2E profile >> ~/.aws/config || exit $?
  - export AWS_PROFILE=agent-qa-ci

.define_if_collect_complexity:
  # Collect only from specific platforms to avoid high memory usage
  - PLATFORMS_FOR_COMPLEXITY_COLLECTION="amazon_5.4 debian_10 ubuntu_18.04 centos_8 opensuse_15.3 suse_12.5 fedora_38"
  - |
    if [ "${TEST_SET}" = "no_usm" ] && echo "${PLATFORMS_FOR_COMPLEXITY_COLLECTION}" | grep -qw "${TAG}" ; then
      export COLLECT_COMPLEXITY=yes
    fi
  - echo "COLLECT_COMPLEXITY=${COLLECT_COMPLEXITY}"

.collect_outcomes_kmt:
  - DD_API_KEY=$($CI_PROJECT_DIR/tools/ci/fetch_secret.sh $AGENT_API_KEY_ORG2 token) || exit $?; export DD_API_KEY
  - export MICRO_VM_IP=$(jq --exit-status --arg TAG $TAG --arg ARCH $ARCH --arg TEST_SET $TEST_SET -r '.[$ARCH].microvms | map(select(."vmset-tags"| index($TEST_SET))) | map(select(.tag==$TAG)) | .[].ip' $CI_PROJECT_DIR/stack.output)
  # Collect setup-ddvm systemd service logs
  - mkdir -p $CI_PROJECT_DIR/logs
  - ssh metal_instance "ssh ${MICRO_VM_IP} \"journalctl -u setup-ddvm.service\"" > $CI_PROJECT_DIR/logs/setup-ddvm.log || true
  - cat $CI_PROJECT_DIR/logs/setup-ddvm.log || true
  - ssh metal_instance "ssh ${MICRO_VM_IP} \"systemctl is-active setup-ddvm.service\"" | tee $CI_PROJECT_DIR/logs/setup-ddvm.status || true
  # Retrieve the junit generated
  - ssh metal_instance "scp ${MICRO_VM_IP}:/ci-visibility/junit.tar.gz /home/ubuntu/junit-${ARCH}-${TAG}-${TEST_SET}.tar.gz" || true
  - scp "metal_instance:/home/ubuntu/junit-${ARCH}-${TAG}-${TEST_SET}.tar.gz" $DD_AGENT_TESTING_DIR/ || true
  - ssh metal_instance "scp ${MICRO_VM_IP}:/ci-visibility/testjson.tar.gz /home/ubuntu/testjson-${ARCH}-${TAG}-${TEST_SET}.tar.gz" || true
  - scp "metal_instance:/home/ubuntu/testjson-${ARCH}-${TAG}-${TEST_SET}.tar.gz" $DD_AGENT_TESTING_DIR/ || true
  - ssh metal_instance "scp -r ${MICRO_VM_IP}:/tmp/test_pcaps /home/ubuntu/test_pcaps-${ARCH}-${TAG}-${TEST_SET}" || true
  - mkdir -p "$CI_PROJECT_DIR/pcaps" && scp -r "metal_instance:/home/ubuntu/test_pcaps-${ARCH}-${TAG}-${TEST_SET}" "$CI_PROJECT_DIR/pcaps/test_pcaps-${ARCH}-${TAG}-${TEST_SET}" || true
  # Retrieve complexity data
  - !reference [.define_if_collect_complexity]
  - |
    if [ "${COLLECT_COMPLEXITY}" = "yes" ]; then
      ssh metal_instance "scp ${MICRO_VM_IP}:/verifier-complexity.tar.gz /home/ubuntu/verifier-complexity-${ARCH}-${TAG}-${TEST_COMPONENT}.tar.gz" || true
      scp "metal_instance:/home/ubuntu/verifier-complexity-${ARCH}-${TAG}-${TEST_COMPONENT}.tar.gz" $DD_AGENT_TESTING_DIR/ || true
    fi
  - !reference [.tag_kmt_ci_job]

.upload_junit_kmt:
  - $CI_PROJECT_DIR/tools/ci/junit_upload.sh "$DD_AGENT_TESTING_DIR/junit-*.tar.gz"

# Important: this job needs DD_API_KEY to be set. Some caveats:
# - if this command is called in the after_script section, note that variables exported in the before_script and
#   script sections are not avilable to after_script sections
# - In KMT we change the AWS_PROFILE environment variable, and that might cause the ssm wrapper to fail getting the keys. Export
#   the DD_API_KEY variable before changing the AWS_PROFILE variable.
.tag_kmt_ci_job:
  - inv -e kmt.tag-ci-job

# -- Test dependencies

.package_dependencies:
  stage: kernel_matrix_testing_prepare
  image: 486234852809.dkr.ecr.us-east-1.amazonaws.com/ci/datadog-agent-buildimages/system-probe_x64$DATADOG_AGENT_SYSPROBE_BUILDIMAGES_SUFFIX:$DATADOG_AGENT_SYSPROBE_BUILDIMAGES
  before_script:
    - !reference [.kmt_new_profile]
    - !reference [.write_ssh_key_file]
  tags: ["arch:amd64"]
  script:
    # upload dependencies
    - !reference [.wait_for_instance]
    - !reference [.get_instance_ip_by_type]
    - !reference [.setup_ssh_config]
    - |
      if [ -d $DD_AGENT_TESTING_DIR/kmt-dockers-$ARCH ]; then
        tar czvf $DD_AGENT_TESTING_DIR/kmt-dockers-$ARCH.tar.gz -C $DD_AGENT_TESTING_DIR kmt-dockers-$ARCH
        scp $DD_AGENT_TESTING_DIR/kmt-dockers-$ARCH.tar.gz metal_instance:/opt/kernel-version-testing
      fi
  after_script:
    - DD_API_KEY=$($CI_PROJECT_DIR/tools/ci/fetch_secret.sh $AGENT_API_KEY_ORG2 token) || exit $?; export DD_API_KEY
    - !reference [.tag_kmt_ci_job]
  variables:
    AWS_EC2_SSH_KEY_FILE: $CI_PROJECT_DIR/ssh_key

# -- Environment setup
.kmt_setup_env:
  extends:
    - .kitchen_ec2_location_us_east_1
  stage: kernel_matrix_testing_prepare
  image: 486234852809.dkr.ecr.us-east-1.amazonaws.com/ci/test-infra-definitions/runner$TEST_INFRA_DEFINITIONS_BUILDIMAGES_SUFFIX:$TEST_INFRA_DEFINITIONS_BUILDIMAGES
  needs: ["go_deps", "go_tools_deps"]
  tags: ["arch:amd64"]
  variables:
    AWS_REGION: us-east-1
    STACK_DIR: $CI_PROJECT_DIR/stack.dir
    # The ssh key is created by the pulumi scenario, to be used for creating
    # instances in the build-stable account. We reuse this file to ssh into
    # the instances in subsequent jobs.
    AWS_EC2_SSH_KEY_FILE: $CI_PROJECT_DIR/ssh_key
    AWS_EC2_SSH_KEY_NAME: datadog-agent-ci
    INFRA_ENV: "aws/agent-qa"
    PIPELINE_ID: $CI_PIPELINE_ID
    TEAM: "ebpf-platform"
    RESOURCE_TAGS: "instance-type:${INSTANCE_TYPE},arch:${ARCH},test-component:${TEST_COMPONENT},git-branch:${CI_COMMIT_REF_NAME}"
    KUBERNETES_MEMORY_REQUEST: "12Gi"
    KUBERNETES_MEMORY_LIMIT: "16Gi"
    VMCONFIG_FILE: "${CI_PROJECT_DIR}/vmconfig-${CI_PIPELINE_ID}-${ARCH}.json"
    EXTERNAL_LINKS_PATH: external_links_$CI_JOB_ID.json
  before_script:
    - DD_API_KEY=$($CI_PROJECT_DIR/tools/ci/fetch_secret.sh $AGENT_API_KEY_ORG2 token) || exit $?; export DD_API_KEY
    - !reference [.retrieve_linux_go_deps]
    - !reference [.kmt_new_profile]
    - !reference [.write_ssh_key_file]
    - inv -e gitlab.generate-ci-visibility-links --output=$EXTERNAL_LINKS_PATH || true
  script:
    - echo "s3://dd-pulumi-state?region=us-east-1&awssdk=v2&profile=$AWS_PROFILE" > $STACK_DIR
    - pulumi login $(cat $STACK_DIR | tr -d '\n')
      # Each VM gets 12GB of memory. We have set 8GB as base for the system, and upto 4GB can be used
      # by a ram backed filesystem holding the system-probe test packages
    - inv -e kmt.gen-config --ci --arch=$ARCH --output-file=$VMCONFIG_FILE --vmconfig-template=$TEST_COMPONENT --memory=12288
    - inv -e system-probe.start-microvms --provision-instance --provision-microvms --vmconfig=$VMCONFIG_FILE $INSTANCE_TYPE_ARG $AMI_ID_ARG --ssh-key-name=$AWS_EC2_SSH_KEY_NAME --ssh-key-path=$AWS_EC2_SSH_KEY_FILE --infra-env=$INFRA_ENV --stack-name=kernel-matrix-testing-${TEST_COMPONENT}-${ARCH}-${CI_PIPELINE_ID} --run-agent
    - jq "." $CI_PROJECT_DIR/stack.output
    - pulumi logout
  after_script:
    - DD_API_KEY=$($CI_PROJECT_DIR/tools/ci/fetch_secret.sh $AGENT_API_KEY_ORG2 token) || exit $?; export DD_API_KEY
    - export AWS_PROFILE=agent-qa-ci
    - !reference [.shared_filters_and_queries]
    - mkdir -p $CI_PROJECT_DIR/libvirt/log/$ARCH $CI_PROJECT_DIR/libvirt/xml $CI_PROJECT_DIR/libvirt/qemu $CI_PROJECT_DIR/libvirt/dnsmasq
    - !reference [.get_instance_ip_by_type]
    - ssh -o StrictHostKeyChecking=no -i $AWS_EC2_SSH_KEY_FILE "ubuntu@$INSTANCE_IP" "sudo virsh list --name | grep -v -E '^$' | xargs -I '{}' sh -c \"sudo virsh dumpxml '{}' > /tmp/ddvm-xml-'{}'.txt\""
    - ssh -o StrictHostKeyChecking=no -i $AWS_EC2_SSH_KEY_FILE "ubuntu@$INSTANCE_IP" "sudo virsh list --name | xargs -I '{}' sh -c \"sudo cp /var/log/libvirt/qemu/'{}'.log /tmp/qemu-ddvm-'{}'.log && sudo chown 1000:1000 /tmp/qemu-ddvm*\""
    - ssh -o StrictHostKeyChecking=no -i $AWS_EC2_SSH_KEY_FILE "ubuntu@$INSTANCE_IP" "mkdir /tmp/dnsmasq && sudo cp /var/lib/libvirt/dnsmasq/* /tmp/dnsmasq/ && sudo chown 1000:1000 /tmp/dnsmasq/*"
    - scp -o StrictHostKeyChecking=no -i $AWS_EC2_SSH_KEY_FILE "ubuntu@$INSTANCE_IP:/tmp/ddvm-*.log" $CI_PROJECT_DIR/libvirt/log
    - scp -o StrictHostKeyChecking=no -i $AWS_EC2_SSH_KEY_FILE "ubuntu@$INSTANCE_IP:/tmp/ddvm-xml-*" $CI_PROJECT_DIR/libvirt/xml
    - scp -o StrictHostKeyChecking=no -i $AWS_EC2_SSH_KEY_FILE "ubuntu@$INSTANCE_IP:/tmp/qemu-ddvm-*.log" $CI_PROJECT_DIR/libvirt/qemu
    - scp -o StrictHostKeyChecking=no -i $AWS_EC2_SSH_KEY_FILE "ubuntu@$INSTANCE_IP:/tmp/dnsmasq/*" $CI_PROJECT_DIR/libvirt/dnsmasq
    # build vm-metrics collector
    - |
      GO_ARCH=$ARCH
      if [ "${ARCH}" == "x86_64" ]; then
        GO_ARCH=amd64
      fi
    - cd test/new-e2e && GOOS=linux GOARCH="${GO_ARCH}" go build system-probe/vm-metrics/vm-metrics.go
    # The vm-metrics collector is uploaded and executed in the same job because we need to execute it after the datadog-agent
    # is launched in the metal instance, and before the tests are executed. This place naturally satisfies these constraints.
    # upload vm-metrics collector to metal instance
    - scp -o StrictHostKeyChecking=no -i $AWS_EC2_SSH_KEY_FILE $CI_PROJECT_DIR/test/new-e2e/vm-metrics "ubuntu@$INSTANCE_IP:/home/ubuntu/vm-metrics"
    # run vm-metrics collector
    - ssh -o StrictHostKeyChecking=no -i $AWS_EC2_SSH_KEY_FILE "ubuntu@$INSTANCE_IP" "/home/ubuntu/vm-metrics -statsd-host=127.0.0.1 -statsd-port=8125 -libvirt-uri=/var/run/libvirt/libvirt-sock-ro --tag \"arch:${ARCH}\" --tag \"test-component:${TEST_COMPONENT}\" --tag \"ci-pipeline-id:${CI_PIPELINE_ID}\" --daemon -log-file /home/ubuntu/daemon.log"
    - !reference [.tag_kmt_ci_job]
  artifacts:
    when: always
    paths:
      - $CI_PROJECT_DIR/stack.output
      - $CI_PROJECT_DIR/libvirt
      - $VMCONFIG_FILE
    reports:
      annotations:
        - $EXTERNAL_LINKS_PATH

.kmt_cleanup:
  stage: kernel_matrix_testing_cleanup
  image: 486234852809.dkr.ecr.us-east-1.amazonaws.com/ci/test-infra-definitions/runner$TEST_INFRA_DEFINITIONS_BUILDIMAGES_SUFFIX:$TEST_INFRA_DEFINITIONS_BUILDIMAGES
  tags: ["arch:amd64"]
  before_script:
    - GITLAB_TOKEN=$($CI_PROJECT_DIR/tools/ci/fetch_secret.sh $GITLAB_TOKEN read_api) || exit $?; export GITLAB_TOKEN
    - !reference [.kmt_new_profile]
  script:
    - !reference [.shared_filters_and_queries]
    # Gitlab CI ignores the dependency of clean up jobs on the setup job, as the run job already depends on the setup
    # and Gitlab thinks it can simplify the dependency graph. This leads to a problem where, if any other dependency of the run test
    # jobs fail (e.g., building the system probe tests), the run tests will be skipped, and the cleanup job will run even though the
    # setup_env job hasn't finished. This causes instances to be leftover for more time than necessary.
    - inv kmt.wait-for-setup-job --pipeline-id $CI_PIPELINE_ID --arch $ARCH --component $TEST_COMPONENT
    - aws ec2 describe-instances --filters $FILTER_TEAM $FILTER_MANAGED $FILTER_PIPELINE $FILTER_ARCH $FILTER_INSTANCE_TYPE $FILTER_TEST_COMPONENT --output json --query $QUERY_INSTANCE_IDS | tee -a instance.json
    - INSTANCE_ID="$(jq -r '.[0][0]' < instance.json)"
    - echo ${INSTANCE_ID}
    - |
      if [[ "${INSTANCE_ID}" != "" ]] && [[ "${INSTANCE_ID}" != "null" ]]; then
        aws ec2 terminate-instances --instance-ids "${INSTANCE_ID}"
      fi
  after_script:
    - DD_API_KEY=$($CI_PROJECT_DIR/tools/ci/fetch_secret.sh $AGENT_API_KEY_ORG2 token) || exit $?; export DD_API_KEY
    - !reference [.tag_kmt_ci_job]

# Manual cleanup jobs, these will be used to cleanup the instances after the tests
# if the tests are canceled (e.g., by the auto-cancel-prev-pipelines job). The automatic jobs
# will not run if the dependencies are canceled
.kmt_cleanup_manual:
  when: manual
  allow_failure: true # Don't fail the full pipeline, these can fail if the instances are already cleaned up
  needs: []

# -- Test runners
.kmt_run_tests:
  retry:
    max: 2
    exit_codes: 42
    when:
      - job_execution_timeout
      - runner_system_failure
      - stuck_or_timeout_failure
      - unknown_failure
      - api_failure
      - scheduler_failure
      - stale_schedule
      - data_integrity_failure
  variables:
    AWS_EC2_SSH_KEY_FILE: $CI_PROJECT_DIR/ssh_key
    RETRY: 2
    EXTERNAL_LINKS_PATH: external_links_$CI_JOB_ID.json
  before_script:
    - DD_API_KEY=$($CI_PROJECT_DIR/tools/ci/fetch_secret.sh $AGENT_API_KEY_ORG2 token) || exit $?; export DD_API_KEY
    - !reference [.kmt_new_profile]
    - !reference [.write_ssh_key_file]
    - echo "CI_JOB_URL=${CI_JOB_URL}" >> $DD_AGENT_TESTING_DIR/job_env.txt
    - echo "CI_JOB_ID=${CI_JOB_ID}" >> $DD_AGENT_TESTING_DIR/job_env.txt
    - echo "CI_JOB_NAME=${CI_JOB_NAME}" >> $DD_AGENT_TESTING_DIR/job_env.txt
    - echo "CI_JOB_STAGE=${CI_JOB_STAGE}" >> $DD_AGENT_TESTING_DIR/job_env.txt
    - inv -e gitlab.generate-ci-visibility-links --output=$EXTERNAL_LINKS_PATH
    - !reference [.define_if_collect_complexity]
  script:
    - INSTANCE_IP=$(jq --exit-status --arg ARCH $ARCH -r '.[$ARCH].ip' $CI_PROJECT_DIR/stack.output)
    - !reference [.shared_filters_and_queries]
    - RUNNING_INSTANCES=$(aws ec2 describe-instances --filters $FILTER_TEAM $FILTER_MANAGED $FILTER_PIPELINE $FILTER_TEST_COMPONENT "Name=private-ip-address,Values=$INSTANCE_IP" --output text --query $QUERY_INSTANCE_IDS | wc -l )
    - |
      if [ $RUNNING_INSTANCES -eq "0" ]; then
        echo "These jobs do not permit retries. The go tests are retried a user-specified number of times automatically. In order to re-run the tests, you must trigger the pipeline again"
        'false'
      fi
    - MICRO_VM_IP=$(jq --exit-status --arg TAG $TAG --arg ARCH $ARCH --arg TEST_SET $TEST_SET -r '.[$ARCH].microvms | map(select(."vmset-tags"| index($TEST_SET))) | map(select(.tag==$TAG)) | .[].ip' $CI_PROJECT_DIR/stack.output)
    - MICRO_VM_NAME=$(jq --exit-status --arg TAG $TAG --arg ARCH $ARCH --arg TEST_SET $TEST_SET -r '.[$ARCH].microvms | map(select(."vmset-tags"| index($TEST_SET))) | map(select(.tag==$TAG)) | .[].id' $CI_PROJECT_DIR/stack.output)
    - GO_VERSION=$(inv go-version)
    - !reference [.setup_ssh_config]
    # ssh into each micro-vm and run initialization script. This script will also run the tests.
    - scp "$DD_AGENT_TESTING_DIR/job_env.txt" "metal_instance:/home/ubuntu/job_env-${ARCH}-${TAG}-${TEST_SET}.txt"
    - ssh metal_instance "scp /home/ubuntu/job_env-${ARCH}-${TAG}-${TEST_SET}.txt ${MICRO_VM_IP}:/job_env.txt"
    - NESTED_VM_CMD="/home/ubuntu/connector -host ${MICRO_VM_IP} -user root -ssh-file /home/kernel-version-testing/ddvm_rsa -vm-cmd 'CI=true /root/fetch_dependencies.sh ${ARCH} && COLLECT_COMPLEXITY=${COLLECT_COMPLEXITY} /opt/micro-vm-init.sh -test-tools /opt/testing-tools -retry ${RETRY} -test-root /opt/${TEST_COMPONENT}-tests -packages-run-config /opt/${TEST_SET}.json'"
    - $CI_PROJECT_DIR/connector-$ARCH -host $INSTANCE_IP -user ubuntu -ssh-file $AWS_EC2_SSH_KEY_FILE -vm-cmd "${NESTED_VM_CMD}" -send-env-vars DD_API_KEY # Allow DD_API_KEY to be passed to the metal instance, so we can use it to send metrics from the connector.
    - ssh metal_instance "ssh ${MICRO_VM_IP} '/opt/testing-tools/test-json-review -flakes /opt/testing-tools/flakes.yaml -codeowners /opt/testing-tools/CODEOWNERS -test-root /opt/${TEST_COMPONENT}-tests'"
    - '[ ! -f $CI_PROJECT_DIR/daemon-${ARCH}.log ] && scp metal_instance:/home/ubuntu/daemon.log $CI_PROJECT_DIR/vm-metrics-daemon-${ARCH}.log'
  artifacts:
    expire_in: 2 weeks
    when: always
    paths:
      - $DD_AGENT_TESTING_DIR/junit-$ARCH-$TAG-$TEST_SET.tar.gz
      - $DD_AGENT_TESTING_DIR/testjson-$ARCH-$TAG-$TEST_SET.tar.gz
      - $DD_AGENT_TESTING_DIR/verifier-complexity-$ARCH-$TAG-${TEST_COMPONENT}.tar.gz
      - $CI_PROJECT_DIR/logs
      - $CI_PROJECT_DIR/pcaps
      - $CI_PROJECT_DIR/vm-metrics-daemon-${ARCH}.log
    reports:
      annotations:
        - $EXTERNAL_LINKS_PATH

notify_ebpf_complexity_changes:
  stage: notify
  image: 486234852809.dkr.ecr.us-east-1.amazonaws.com/ci/datadog-agent-buildimages/deb_x64$DATADOG_AGENT_BUILDIMAGES_SUFFIX:$DATADOG_AGENT_BUILDIMAGES
  tags: ["arch:amd64"]
  rules:
    - !reference [.except_mergequeue]
    - !reference [.except_main_or_release_branch]
    - !reference [.except_no_tests_no_deploy]
    - !reference [.except_deploy]
    - !reference [.on_system_probe_or_e2e_changes_or_manual]
    - !reference [.on_security_agent_changes_or_manual]
  needs:
    # We need to specify the jobs that generate complexity explicitly, else we hit the limit of "needs"
    # Important: the list of platforms should match the one in .define_if_collect_complexity
    - job: "kmt_run_sysprobe_tests_x64"
      optional: true
      parallel:
        matrix:
          - TAG:
              - amazon_5.4
              - debian_10
              - ubuntu_18.04
              - centos_8
              - opensuse_15.3
              - suse_12.5
              - fedora_38
            TEST_SET: no_usm
    - job: "kmt_run_sysprobe_tests_arm64"
      optional: true
      parallel:
        matrix:
          - TAG:
              - amazon_5.4
              - debian_10
              - ubuntu_18.04
              - centos_8
              - opensuse_15.3
              - suse_12.5
              - fedora_38
            TEST_SET: no_usm
  allow_failure: true
  before_script:
    - python3 -m pip install tabulate # Required for printing the tables
    - python3 -m pip install -r tasks/libs/requirements-github.txt
    - !reference [.setup_agent_github_app]
    - GITLAB_TOKEN=$($CI_PROJECT_DIR/tools/ci/fetch_secret.sh $GITLAB_TOKEN read_api) || exit $?; export GITLAB_TOKEN
  script:
    - inv -e ebpf.generate-complexity-summary-for-pr
