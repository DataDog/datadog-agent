import io
import json
import os
import platform
import re
import tarfile
import tempfile
import xml.etree.ElementTree as ET
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from shutil import which
from subprocess import PIPE, CalledProcessError, Popen

from invoke.exceptions import Exit

from tasks.flavor import AgentFlavor
from tasks.libs.ciproviders.gitlab_api import get_gitlab_repo
from tasks.libs.common.utils import collapsed_section
from tasks.libs.pipeline.notifications import (
    DEFAULT_JIRA_PROJECT,
    DEFAULT_SLACK_CHANNEL,
    GITHUB_JIRA_MAP,
    GITHUB_SLACK_MAP,
)

E2E_INTERNAL_ERROR_STRING = "E2E INTERNAL ERROR"
CODEOWNERS_ORG_PREFIX = "@DataDog/"
REPO_NAME_PREFIX = "github.com/DataDog/datadog-agent/"
if platform.system() == "Windows":
    DATADOG_CI_COMMAND = [r"c:\devtools\datadog-ci\datadog-ci", "junit", "upload"]
else:
    DATADOG_CI_COMMAND = [which("datadog-ci"), "junit", "upload"]
JOB_URL_FILE_NAME = "job_url.txt"
JOB_ENV_FILE_NAME = "job_env.txt"
TAGS_FILE_NAME = "tags.txt"


def enrich_junitxml(xml_path: str, flavor: AgentFlavor):
    """
    Modifies the JUnit XML file:
    1. Adds a flavor field to it, to allow tagging tests by flavor.
    2. Assigns empty classname attributes for each test case given the name
       of the parent test suite (timeouts do not have classnames).
    """
    tree = ET.parse(xml_path)
    root = tree.getroot()

    # 1. Create a new element containing the flavor and append it to the tree
    flavor_element = ET.Element('flavor')
    flavor_element.text = flavor.name
    root.append(flavor_element)

    # 2. Assign empty test cases
    for testsuite in root.findall('.//testsuite'):
        testsuite_name = testsuite.get('name')
        for testcase in testsuite.findall('.//testcase'):
            # The class name cannot be found, set it to the testsuite name
            if testcase.get('classname') == '':
                testcase.set('classname', testsuite_name)

    # Write back to the original file
    tree.write(xml_path)


def junit_upload_from_tgz(junit_tgz, codeowners_path=".github/CODEOWNERS"):
    """
    Upload all JUnit XML files contained in given tgz archive.
    """
    from codeowners import CodeOwners

    with open(codeowners_path) as f:
        codeowners = CodeOwners(f.read())

    flaky_tests = get_flaky_from_test_output()
    junit_tgz = find_tarball(junit_tgz)

    with collapsed_section(f"Uploading JUnit files for {junit_tgz}"), tempfile.TemporaryDirectory() as unpack_dir:
        working_dir = Path(unpack_dir)
        # unpack all files from archive
        with tarfile.open(junit_tgz) as tgz:
            tgz.extractall(path=str(working_dir))

        # Split xml files by codeowners
        generated_xmls = 0
        for xmlfile in list(working_dir.glob("**/*.xml")):  # We need to cast the generator to avoid infinite loop
            if not xmlfile.is_file():
                print(f"[WARN] Matched folder named {xmlfile}")
                continue
            generated_xmls += split_junitxml(xmlfile, codeowners, flaky_tests)
        print(f"Created {generated_xmls} JUnit XML files from {junit_tgz}")
        # *-fast(-v2).tgz contains only tests related to the modified code, they can be empty
        if generated_xmls == 0 and "-fast" not in junit_tgz:
            raise Exit(f"[ERROR] No JUnit XML files for upload found in: {junit_tgz}")

        # Upload junit on a per-team basis
        team_folders = [item for item in working_dir.iterdir() if item.is_dir()]
        with ThreadPoolExecutor() as executor:
            executor.map(upload_junitxmls, team_folders)


def get_flaky_from_test_output():
    """
    Read the test output file generated by gotestsum which contains a list of json for each unit test.
    We catch all tests marked as flaky in source code: they contain a certain message in the output field.
    """
    TEST_OUTPUT_FILE = "module_test_output.json"
    FLAKE_MESSAGE = "flakytest: this is a known flaky test"
    test_output = []

    with open(TEST_OUTPUT_FILE) as f:
        for line in f.readlines():
            test_output.append(json.loads(line))
    flaky_tests = [
        "/".join([test["Package"], test["Test"]]) for test in test_output if FLAKE_MESSAGE in test.get("Output", "")
    ]
    print(f"[INFO] Found {len(flaky_tests)} flaky tests.")
    return flaky_tests


def find_tarball(tarball):
    """
    handle weird kitchen bug where it places the tarball in a subdirectory of the same name
    """
    if os.path.isdir(tarball):
        tmp_tgz = os.path.join(tarball, os.path.basename(tarball))
        if not os.path.isfile(tmp_tgz):
            tmp_tgz = os.path.join(tarball, "junit.tar.gz")
        return tmp_tgz
    return tarball


def read_additional_tags(folder: Path):
    """
    Read tags from a tags.txt file in the given folder
    """
    tags = None
    tagsfile = folder / TAGS_FILE_NAME
    if tagsfile.exists():
        with tagsfile.open() as tf:
            tags = tf.read().split()
    return tags


def split_junitxml(xml_path: Path, codeowners, flaky_tests):
    """
    Split a junit XML into several according to the suite name and the codeowners.
    Returns a list with the owners of the written files.
    """
    tree = ET.parse(xml_path)
    output_xmls = {}

    flem = tree.find("flavor")
    flavor = flem.text if flem else AgentFlavor.base.name

    for suite in tree.iter("testsuite"):
        path = suite.attrib["name"].replace(REPO_NAME_PREFIX, "", 1)

        # Dirs in CODEOWNERS might end with "/", but testsuite names in JUnit XML
        # don't, so for determining ownership we append "/" temporarily.
        owners = codeowners.of(path + "/")
        if not owners:
            # In kitchen testing the test name might not be a file path, so we check the file attribute instead
            filepath = next(tree.iter("testcase")).attrib.get("file", None)
            if filepath:
                if filepath.startswith("./"):  # Leading "./" is not handled by codeowners
                    filepath = filepath[2:]
                owners = codeowners.of(filepath)
                main_owner = owners[0][1][len(CODEOWNERS_ORG_PREFIX) :]
            else:
                main_owner = "none"
        else:
            main_owner = owners[0][1][len(CODEOWNERS_ORG_PREFIX) :]

        if main_owner in output_xmls:
            xml = output_xmls[main_owner]
        else:
            xml = ET.ElementTree(ET.Element("testsuites"))
            output_xmls[main_owner] = xml
        # Flag the test as known flaky if gotestsum already knew it
        for test_case in suite.iter("testcase"):
            test_name = "/".join([test_case.attrib["classname"], test_case.attrib["name"]])
            test_case.attrib["agent_is_known_flaky"] = "true" if test_name in flaky_tests else "false"

        xml.getroot().append(suite)

    # Save the split XMLs in folders with <owner>_<flavor> name (they will be uploaded with the same tags)
    for owner, xml in output_xmls.items():
        write_dir = xml_path.parent / f"{owner}_{flavor}"
        if not write_dir.exists():
            write_dir.mkdir()
        xml.write(write_dir / xml_path.name, encoding="UTF-8", xml_declaration=True)
    return len(output_xmls)


def upload_junitxmls(team_dir: Path):
    """
    Upload all per-team split JUnit XMLs from given directory.
    """
    additional_tags = read_additional_tags(team_dir.parent)
    process_env = _update_environ(team_dir.parent)
    processes = []

    owner, flavor = team_dir.name.split("_")
    # Kitchen/e2e can generate additional tags
    xml_files = group_per_tags(team_dir, additional_tags)
    for flags, files in xml_files.items():
        args = set_tags(owner, flavor, flags, additional_tags, files[0])
        args.extend(files)
        processes.append(Popen(DATADOG_CI_COMMAND + args, bufsize=-1, env=process_env, stdout=PIPE, stderr=PIPE))

    for process in processes:
        _, stderr = process.communicate()
        print(f" Uploaded {len(tuple(team_dir.iterdir()))} files for {team_dir.name}")
        if stderr:
            print(f"Failed uploading junit:\n{stderr}", file=os.sys.stderr)
            raise CalledProcessError(process.returncode, DATADOG_CI_COMMAND)


def group_per_tags(team_dir: Path, additional_tags: list):
    xml_files = defaultdict(list)
    for file in team_dir.iterdir():
        flags = "default"
        if is_e2e_internal_failure(file):
            flags = "e2e"
        if is_kitchen_version(additional_tags):
            flags = "kitchen" if flags == "default" else "kitchen-e2e"
        xml_files[flags].append(str(file))
    return xml_files


def is_e2e_internal_failure(xml_path: Path):
    """
    Check if the given JUnit XML file contains E2E INTERAL ERROR string.
    """
    with xml_path.open(encoding="utf8") as f:
        filecontent = f.read()
    return E2E_INTERNAL_ERROR_STRING in filecontent


def is_kitchen_version(tags):
    """
    Check if we need to add the version from the kitchen file name to the tags
    """
    return tags and "upload_option.os_version_from_name" in tags


def set_tags(owner, flavor, flag: str, additional_tags, file_name):
    codeowner = CODEOWNERS_ORG_PREFIX + owner
    slack_channel = GITHUB_SLACK_MAP.get(codeowner.lower(), DEFAULT_SLACK_CHANNEL)[1:]
    jira_project = GITHUB_JIRA_MAP.get(codeowner.lower(), DEFAULT_JIRA_PROJECT)[0:]
    agent = get_gitlab_repo()
    pipeline = agent.pipelines.get(os.environ["CI_PIPELINE_ID"])
    tags = [
        "--service",
        "datadog-agent",
        "--tags",
        f'test.codeowners:["{codeowner}"]',
        "--tags",
        f"test.flavor:{flavor}",
        "--tags",
        f"slack_channel:{slack_channel}",
        "--tags",
        f"jira_project:{jira_project}",
        "--tags",
        f"gitlab.pipeline_source:{pipeline.source}",
    ]
    if 'e2e' in flag:
        tags.extend(["--tags", "e2e_internal_error:true"])
    if 'kitchen' in flag:
        version_match = re.search(r"kitchen-rspec-([a-zA-Z0-9]+)-?([0-9-]*)-.*\.xml", file_name)
        exact_version = version_match.group(1) + version_match.group(2).replace("-", ".")
        tags.extend(["--tags", f"version:{exact_version}"])
        additional_tags.remove("upload_option.os_version_from_name")
    tags.extend(additional_tags)
    return tags


def _update_environ(unpack_dir: Path):
    """
    Add job_url and job_env to current env if any, for the junit upload command
    """
    process_env = os.environ.copy()
    # read job url (see comment in produce_junit_tar)
    job_url = None
    urlfile = unpack_dir / JOB_URL_FILE_NAME
    if urlfile.exists():
        with urlfile.open() as jf:
            job_url = jf.read()
        process_env["CI_JOB_URL"] = job_url

    job_env = {}
    envfile = unpack_dir / JOB_ENV_FILE_NAME
    if envfile.exists():
        with envfile.open() as jf:
            for line in jf:
                if not line.strip():
                    continue
                key, val = line.strip().split('=', 1)
                job_env[key] = val
        print("\n".join(f"{k}={v}" for k, v in job_env.items()))
        process_env.update(job_env)
    return process_env


def _normalize_architecture(architecture):
    architecture = architecture.lower()
    normalize_table = {"amd64": "x86_64"}
    return normalize_table.get(architecture, architecture)


def produce_junit_tar(files, result_path):
    """
    Produce a tgz file containing all given files JUnit XML files and add a special file
    with additional tags.
    """
    # NOTE: for now, we can't pass CI_JOB_URL through `--tags`, because
    # the parsing logic tags breaks on URLs, as they contain colons.
    # Therefore we pass it through environment variable.
    tags = {
        "os.platform": platform.system().lower(),
        "os.architecture": _normalize_architecture(platform.machine()),
        "ci.job.name": os.environ.get("CI_JOB_NAME", ""),
        # "ci.job.url": os.environ.get("CI_JOB_URL", ""),
    }
    with tarfile.open(result_path, "w:gz") as tgz:
        for f in files:
            tgz.add(f, arcname=f.replace(os.path.sep, "-"))

        tags_file = io.BytesIO()
        for k, v in tags.items():
            tags_file.write(f"--tags {k}:{v} ".encode())
        tags_info = tarfile.TarInfo(TAGS_FILE_NAME)
        tags_info.size = tags_file.getbuffer().nbytes
        tags_file.seek(0)
        tgz.addfile(tags_info, tags_file)

        job_url_file = io.BytesIO()
        job_url_file.write(os.environ.get("CI_JOB_URL", "").encode("UTF-8"))
        job_url_info = tarfile.TarInfo(JOB_URL_FILE_NAME)
        job_url_info.size = job_url_file.getbuffer().nbytes
        job_url_file.seek(0)
        tgz.addfile(job_url_info, job_url_file)


def repack_macos_junit_tar(workflow_conclusion, infile, outfile):
    """
    Repacks JUnit tgz file from macOS Github Action run, so it would
    contain correct job name and job URL.
    """
    if workflow_conclusion == "cancelled" and not os.path.exists(infile):
        print(f"Skipping repacking of JUnit tarball due to {workflow_conclusion} workflow")
        return

    with tarfile.open(infile) as infp, tarfile.open(outfile, "w:gz") as outfp, tempfile.TemporaryDirectory() as tempd:
        infp.extractall(tempd)

        # write the proper job url and job name
        with open(os.path.join(tempd, JOB_URL_FILE_NAME), "w") as fp:
            fp.write(os.environ.get("CI_JOB_URL", ""))
        with open(os.path.join(tempd, TAGS_FILE_NAME)) as fp:
            tags = fp.read()
        job_name = os.environ.get("CI_JOB_NAME", "")
        tags = tags.replace("ci.job.name:", f"ci.job.name:{job_name}")
        with open(os.path.join(tempd, TAGS_FILE_NAME), "w") as fp:
            fp.write(tags)

        # pack all files to a new tarball
        for f in os.listdir(tempd):
            outfp.add(os.path.join(tempd, f), arcname=f)
