stages:
  - source_test
  - binary_build
  - integration_test
  - package_build
  - testkitchen_deploy
  - testkitchen_testing
  - testkitchen_cleanup
  - image_build
  - image_deploy
  - deploy
  - deploy_invalidate
  - e2e

variables:
  SRC_PATH: /src/github.com/DataDog/datadog-agent
  OMNIBUS_BASE_DIR: $CI_PROJECT_DIR/.omnibus/
  OMNIBUS_PACKAGE_DIR: $CI_PROJECT_DIR/.omnibus/pkg/
  # make sure the types of RPM packages are kept separate
  OMNIBUS_BASE_DIR_SUSE: $CI_PROJECT_DIR/.omnibus/suse/
  OMNIBUS_PACKAGE_DIR_SUSE: $CI_PROJECT_DIR/.omnibus/suse/pkg/
  DD_AGENT_TESTING_DIR: $CI_PROJECT_DIR/test/kitchen
  STATIC_BINARIES_DIR: bin/static
  DOGSTATSD_BINARIES_DIR: bin/dogstatsd
  AGENT_BINARIES_DIR: bin/agent
  CLUSTER_AGENT_BINARIES_DIR: bin/datadog-cluster-agent
  DEB_S3_BUCKET: apt.datad0g.com
  RPM_S3_BUCKET: yum.datad0g.com
  WIN_S3_BUCKET: dd-agent-mstesting
  DEB_RPM_BUCKET_BRANCH: nightly  # branch of the DEB_S3_BUCKET and RPM_S3_BUCKET repos to release to, 'nightly' or 'beta'
  DEB_TESTING_S3_BUCKET: apttesting.datad0g.com
  RPM_TESTING_S3_BUCKET: yumtesting.datad0g.com
  WINDOWS_TESTING_S3_BUCKET: $WIN_S3_BUCKET/pipelines/$CI_PIPELINE_ID
  WINDOWS_BUILDS_S3_BUCKET: $WIN_S3_BUCKET/builds
  DEB_RPM_TESTING_BUCKET_BRANCH: testing  # branch of the DEB_TESTING_S3_BUCKET and RPM_TESTING_S3_BUCKET repos to release to, 'testing'
  DD_REPO_BRANCH_NAME: $CI_COMMIT_REF_NAME
  S3_CP_OPTIONS: --only-show-errors --region us-east-1 --sse AES256
  S3_CP_CMD: aws s3 cp $S3_CP_OPTIONS
  S3_ARTEFACTS_URI: s3://dd-ci-artefacts-build-stable/$CI_PROJECT_NAME/$CI_PIPELINE_ID
  S3_OMNIBUS_CACHE_BUCKET: dd-ci-datadog-agent-omnibus-cache-build-stable
  S3_DSD6_URI: s3://dsd6-staging/linux
  RELEASE_VERSION: nightly


# Default before_script for all the jobs. If you create a new job and don't want this to execute
# you NEED to overwrite it.
before_script:
  # We need to install go deps from within the GOPATH, which we set to / on builder images; that's because pointing
  # GOPATH to the project folder would be too complex (we'd need to replicate the `src/github/project` scheme).
  # So we copy the agent sources to / and bootstrap from there the vendor dependencies before running any job.
  - echo running default before_script
  - rsync -azr --delete ./ $SRC_PATH
  - cd $SRC_PATH
  - pip install -U pip
  - inv -e deps

#
# Trigger conditions
#

# run job only when triggered by an external tool (ex: Jenkins). This is used
# for jobs that run both on nightlies and tags
.run_when_triggered: &run_when_triggered
  only:
    - db/kitchentest

# run job only when triggered by an external tool (ex: Jenkins) and when
# RELEASE_VERSION is NOT "nightly". In this setting we are building either a
# new tagged version of the agent (an RC for example). In both cases the
# artifacts should be uploaded to our staging repository.

.run_when_triggered_on_tag: &run_when_triggered_on_tag
  only:
    refs:
      - triggers
  except: # we have to use except since gitlab doens't handle '!=' operator
    variables:
      - $RELEASE_VERSION == "nightly"
      - $RELEASE_VERSION == "" # no  RELEASE_VERSION means a nightly build for omnibus

# run job only when triggered by an external tool (ex: Jenkins) and when
# RELEASE_VERSION is "nightly". In this setting we build from master and update
# the nightly build for windows, linux and docker.

.run_when_triggered_on_nightly: &run_when_triggered_on_nightly
  only:
    refs:
      - triggers
    variables:
      - $RELEASE_VERSION == "nightly"

#

# build Agent package for Windows
build_windows_msi_x64:
  before_script:
    - if exist .omnibus rd /s/q .omnibus
    - if exist \omnibus-ruby rd /s/q \omnibus-ruby
    - if exist \opt\datadog-agent rd /s/q \opt\datadog-agent
    - if exist %GOPATH%\src\github.com\DataDog\datadog-agent rd /s/q %GOPATH%\src\github.com\DataDog\datadog-agent
    - mkdir %GOPATH%\src\github.com\DataDog\datadog-agent
    - xcopy /q/h/e/s * %GOPATH%\src\github.com\DataDog\datadog-agent
    - cd %GOPATH%\src\github.com\DataDog\datadog-agent
    - inv -e deps
  stage: package_build
  variables:
    WINDOWS_BUILDER: 'true'
    CREDENTIALS_FILE_PATH: 'c:\users\gitlab\.aws\config'
  tags: ["runner:windows-agent6"]
  script:
    # remove artifacts from previous pipelines that may come from the cache
    - rm -rf .omnibus/pkg/
    - cd %GOPATH%\src\github.com\DataDog\datadog-agent
    - inv agent.omnibus-build --release-version %RELEASE_VERSION% --omnibus-s3-cache
  artifacts:
    expire_in: 2 weeks
    paths:
      - .omnibus/pkg


# deploy windows packages to our testing bucket
deploy_windows_testing:
  <<: *run_when_triggered
  allow_failure: true
  stage: testkitchen_deploy
  image: 486234852809.dkr.ecr.us-east-1.amazonaws.com/ci/datadog-agent-builders/deploy:latest
  before_script:
    - ls $OMNIBUS_PACKAGE_DIR
  tags: [ "runner:main", "size:large" ]
  script:
    - $S3_CP_CMD --recursive --exclude "*" --include "*.msi" $OMNIBUS_PACKAGE_DIR s3://$WINDOWS_TESTING_S3_BUCKET/ --grants read=uri=http://acs.amazonaws.com/groups/global/AllUsers full=id=3a6e02b08553fd157ae3fb918945dd1eaae5a1aa818940381ef07a430cf25732

# run dd-agent-testing on windows
kitchen_windows:
  stage: testkitchen_testing
  allow_failure: true
  image: 486234852809.dkr.ecr.us-east-1.amazonaws.com/ci/datadog-agent-builders/dd-agent-testing:latest
  only:
    - master
    - tags
    - triggers
    - db/kitchentest
  before_script:
    - rsync -azr --delete ./ $SRC_PATH
  tags: [ "runner:main", "size:large" ]
  script:
    - cd $CI_PROJECT_DIR
    - cd $DD_AGENT_TESTING_DIR
    - mkdir $CI_PROJECT_DIR/kitchen_logs
    - ln -s $CI_PROJECT_DIR/kitchen_logs $DD_AGENT_TESTING_DIR/.kitchen
    - export TEST_PLATFORMS="win2012,MicrosoftWindowsServer:WindowsServer:2012-Datacenter:3.127.20171115"
    - export TEST_PLATFORMS="$TEST_PLATFORMS|win2012r2,MicrosoftWindowsServer:WindowsServer:2012-R2-Datacenter:4.127.20171115"
    - export TEST_PLATFORMS="$TEST_PLATFORMS|win2016,MicrosoftWindowsServer:WindowsServer:2016-Datacenter:2016.127.20171116"
    - bash -l tasks/run-test-kitchen.sh
  artifacts:
    expire_in: 2 weeks
    when: always
    paths:
      - $CI_PROJECT_DIR/kitchen_logs




# deploy windows packages to a public s3 bucket when pushed on master
deploy_windows_master:
  stage: deploy
  image: 486234852809.dkr.ecr.us-east-1.amazonaws.com/ci/datadog-agent-builders/deploy:latest
  before_script:
    - ls $OMNIBUS_PACKAGE_DIR
  <<: *run_when_triggered_on_nightly
  tags: [ "runner:main", "size:large" ]
  script:
    - $S3_CP_CMD --recursive --exclude "*" --include "*.msi" $OMNIBUS_PACKAGE_DIR s3://$WINDOWS_BUILDS_S3_BUCKET/master/ --grants read=uri=http://acs.amazonaws.com/groups/global/AllUsers full=id=3a6e02b08553fd157ae3fb918945dd1eaae5a1aa818940381ef07a430cf25732
    - $S3_CP_CMD $OMNIBUS_PACKAGE_DIR/datadog-agent-*-x86_64.msi "s3://$WINDOWS_BUILDS_S3_BUCKET/master/datadog-agent-6-latest.amd64.msi" --grants read=uri=http://acs.amazonaws.com/groups/global/AllUsers full=id=3a6e02b08553fd157ae3fb918945dd1eaae5a1aa818940381ef07a430cf25732

# deploy windows packages to a public s3 bucket when tagged
deploy_windows_tags:
  stage: deploy
  image: 486234852809.dkr.ecr.us-east-1.amazonaws.com/ci/datadog-agent-builders/deploy:latest
  before_script:
    - ls $OMNIBUS_PACKAGE_DIR
  <<: *run_when_triggered_on_tag
  tags: [ "runner:main", "size:large" ]
  script:
    - $S3_CP_CMD --recursive --exclude "*" --include "*.msi" $OMNIBUS_PACKAGE_DIR s3://$WINDOWS_BUILDS_S3_BUCKET/tagged/ --grants read=uri=http://acs.amazonaws.com/groups/global/AllUsers full=id=3a6e02b08553fd157ae3fb918945dd1eaae5a1aa818940381ef07a430cf25732
    # By default we update the "latest" artifacts on our s3 bucket so the
    # staging box can pick it up. Allow the job to skip this step if needed
    # (when building a custom beta for example).
    - if [ "WINDOWS_DO_NOT_UPDATE_LATEST" != "true" ]; then $S3_CP_CMD $OMNIBUS_PACKAGE_DIR/datadog-agent-*-x86_64.msi s3://$WINDOWS_BUILDS_S3_BUCKET/tagged/datadog-agent-6-latest.amd64.msi --grants read=uri=http://acs.amazonaws.com/groups/global/AllUsers full=id=3a6e02b08553fd157ae3fb918945dd1eaae5a1aa818940381ef07a430cf25732; fi

