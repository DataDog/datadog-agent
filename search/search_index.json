{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Datadog Agent","text":"<p>Welcome to the wonderful world of developing the Datadog Agent. Here we document how we do things, advanced debugging techniques, coding conventions &amp; best practices, the internals of our testing infrastructure, and so much more.</p> <p>If you are intrigued, continue reading. If not, continue all the same </p>"},{"location":"#getting-started","title":"Getting started","text":"<p>First, you'll want to set up the development requirements.</p>"},{"location":"#agent-development-guidelines","title":"Agent development guidelines","text":"<p>To know more about the general design of the Agent and how to add code and features read our section on Components.</p>"},{"location":"#navigation","title":"Navigation","text":"<p>Desktop readers can use keyboard shortcuts to navigate.</p> Keys Action <ul><li>, (comma)</li><li>p</li></ul> Navigate to the \"previous\" page <ul><li>. (period)</li><li>n</li></ul> Navigate to the \"next\" page <ul><li>/</li><li>s</li></ul> Display the search modal"},{"location":"architecture/dogstatsd/internals/","title":"DogStatsD internals","text":"<p>(click to enlarge)</p> <p>Information on DogStatsD, configuration and troubleshooting is available in the Datadog documentation.</p>"},{"location":"architecture/dogstatsd/internals/#packet","title":"Packet","text":"<p>In DogStatsD, a Packet is a bytes array containing one or multiple metrics in the DogStatsD format (separated by a <code>\\n</code> when there are several). Its maximum size is <code>dogstatsd_buffer_size</code>.</p>"},{"location":"architecture/dogstatsd/internals/#packetassembler","title":"PacketAssembler","text":"<ul> <li>Input: a datagram from a UDP socket</li> <li>Output: a Packet containing multiple metrics packed together, separated by a <code>\\n</code></li> </ul> <p>The PacketAssembler gathers multiple datagrams into one Packet of maximum size, <code>dogstatsd_buffer_size</code>, and sends it to the PacketsBuffer which avoids running the whole parsing pipeline with only one metric per packet. The bytes buffer used comes from the PacketPool, which avoids re-allocating the bytes buffer every time.</p> <p>Note</p> <p>The UDS pipeline does not use the PacketAssembler because each UDS packet also contains metadata (origin tags) which are used to enrich the metrics tags, making them impossible to be packed together by the PacketAssembler.</p> <p>The PacketAssembler does not allocate a bytes array every time it has to use one. It retrieves one from a pool containing pre-allocated arrays and this pool never empties. The PacketAssembler allocates a new bytes array when it\u2019s needed. Once fully assembled by the PacketAssembler, the bytes array is sent through the rest of the DogStatsD pipeline and ownership is allocated to each part using it (PacketsBuffer, Worker). Eventually, the Worker takes care of returning it to the pool when the part has processed its content.</p>"},{"location":"architecture/dogstatsd/internals/#packetbuffer","title":"PacketBuffer","text":"<ul> <li>Input: a Packet containing one or several metrics in the DogStatsD format (separated by a <code>\\n</code>)</li> <li>Output: multiple Packets send in a row to the Worker</li> </ul> <p>The PacketsBuffer buffers multiple Packets (in a slice), this way the parsing part of the pipeline is going through several Packets in a row instead of only one each time it is called. This leads to less CPU usage. PacketsBuffer sends the Packets for processing when either:</p> <p>a. The buffer is full (contains <code>dogstatsd_packet_buffer_size, default value: 32</code>)</p> <p>or</p> <p>b. A timer is triggered (i.e. <code>dogstatsd_packer_buffer_flush_timeout, default value: 100ms</code>)</p> <p>The PacketBuffer sends it in a Go buffered channel to the worker / parser, meaning that the channels can buffer the Packets on their own while waiting for the worker to read and process them.</p> <p>In theory, the max memory usage of this Go buffered channel is:</p> <ul> <li>packet buffer size * packet size * channel buffer size</li> <li><code>dogstatsd_packer_buffer_size</code> * <code>dogstatsd_buffer_size</code> * <code>dogstatsd_queue_size</code></li> <li>32 * 8192 * 1024 =  256MB</li> </ul> <p>To this we can add per-listener buffers: <code>dogstatsd_packer_buffer_size</code> * <code>dogstatsd_buffer_size</code> * <code>connections</code>. <code>connections</code> will be 1 for <code>uds</code> and <code>udp</code> and one per client for <code>uds-stream</code>.</p>"},{"location":"architecture/dogstatsd/internals/#worker","title":"Worker","text":"<ul> <li>Input: slice of Packets</li> <li>Output: MetricSample sent</li> </ul> <p>The Worker is the part of the DogStatsD server responsible for parsing the metrics in the bytes array and turning them into MetricSamples.</p> <p>The server spawns multiple workers based on the amount of cores available on the host:</p> <ul> <li>When the server is not running multiple time sampling pipelines: the server creates <code>(number of cores - 2)</code> workers. If this result is less than 2, the server spawns 2 workers.</li> <li>When the server is running multiple time sampling pipelines: the server creates <code>(number of cores / 2)</code> workers.  If this result is less than 2, the server spawns 2 workers.</li> </ul> <p>The Worker is using a system called StringInterner to not allocate memory every time a string is needed. Note that this StringInterner is caching a finite number of strings and when it is full it is emptied to start caching strings again. Its size is configurable with <code>dogstatsd_string_interner_size</code>.</p> <p>The MetricSamples created are not directly sent to the Agent Demultiplexer but first to a part called the Batcher.</p>"},{"location":"architecture/dogstatsd/internals/#batcher","title":"Batcher","text":"<ul> <li>Input: MetricSample from the Worker</li> <li>Output: slices of MetricSample sent to the Agent Demultiplexer (which is distributing them to the TimeSamplerWorkers)</li> </ul> <p>The role of the Batcher is to accumulate multiple MetricSamples before sending them to the Agent Demultiplexer. Every time it accumulates 32 MetricSamples, the Batcher sends them to the Demultiplexer. The Batcher sends 32 MetricSamples in a channel buffering 100 sets. There is one channel per TimeSampler.</p> <p>The size of a MetricSample depends on the size of the host's hostname, its metric name, and its number of tags. An example MetricSample with a 20 character hostname, 40 character metric name, and 200 characters of tags has a size of approximately 264 bytes. A Batcher can use a maximum of 844kb of memory.</p>"},{"location":"architecture/dogstatsd/internals/#timesamplerworker","title":"TimeSamplerWorker","text":"<ul> <li>Input: slice of MetricSamples</li> </ul> <p>The TimeSamplerWorker runs in an infinite loop. It is responsible for the following:</p> <ul> <li>Process slices of MetricSamples sent by the Batcher (through the AgentDemultiplexer). A TimeSampler embedded in the TimeSamplerWorker actually does the sampling.</li> <li>Flush on a signal.</li> <li>Stop on a signal.</li> </ul> <p>The following calculations determine the number of TimeSamplerWorker and TimeSampler instances:</p> <ul> <li>If <code>dogstatsd_pipeline_autoadjust</code> is <code>true</code> then the workers count will be automatically adjusted.</li> <li>If <code>dogstatsd_pipeline_count</code> has a value, the number of TimeSampler pipelines equals that value.</li> <li>If neither condition above is true, one TimeSampler pipeline runs.</li> </ul> <p><code>dogstatsd_pipeline_autoadjust_strategy</code> can be set to the following values:</p> <ul> <li><code>max_throughput</code>: The number of TimeSampler pipelines is adjusted to maximize throughput. There are <code>(number of core/2) - 1</code> instances of TimeSampler.</li> <li><code>per_origin</code>: The number of TimeSampler pipelines is adjusted to improve data locality. The number of dsdWorker instances is equal to half the number of cores.         and the number of TimeSampler pipelines is equal <code>dogstatsd_pipeline_count</code> or twice the number of cores. This strategy will provide a better compression         ratio in shared environments and improve resource allocation fairness within the agent.</li> </ul>"},{"location":"architecture/dogstatsd/internals/#noaggregationstreamworker","title":"NoAggregationStreamWorker","text":"<ul> <li>Input: slice of MetricSamples</li> </ul> <p>The NoAggregationStreamWorker runs an infinite loop in a goroutine. It receives metric samples with timestamps, and it batches them to be sent as quickly as possible to the intake. It performs no aggregation nor extra processing, except from adding tags to the metrics.</p> <p>It runs only when <code>dogstatsd_no_aggregation_pipeline</code> is set to <code>true</code>.</p> <p>The payload being sent to the intake (through the normal <code>Serializer</code>/<code>Forwarder</code> pieces) contains, at maximum, <code>dogstatsd_no_aggregation_pipeline_batch_size</code> metrics. This value defaults to <code>2048</code>.</p>"},{"location":"components/common-patterns/","title":"Common patterns","text":""},{"location":"components/common-patterns/#groups","title":"Groups","text":"<p>Fx groups help you produce and group together values of the same type, even if these values are produced in different parts of the codebase. A component can add any type into a group; this group can then consumed by other components.</p> <p>In the following example, a component add a <code>server.Endpoint</code> type to the <code>server</code> group.</p>  comp/users/users.go <pre><code>type Provides struct {\n    comp     Component\n    Endpoint server.Endpoint `group:\"server\"`\n}\n</code></pre> <p>In the following example, a component requests all the types added to the <code>server</code> group. This takes the form of a slice received at instantiation.</p>  comp/server/server.go <pre><code>type Requires struct {\n    Endpoints []Endpoint `group:\"server\"`\n}\n</code></pre>"},{"location":"components/creating-bundles/","title":"Creating a bundle","text":"<p>A bundle is a grouping of related components. The goal of a bundle is to ease the usage of multiple components working together to constitute a product.</p> <p>One example is <code>DogStatsD</code>, a server to receive metrics locally from customer apps. <code>DogStatsD</code> is composed of 9+ components, but at the binary level we want to include <code>DogStatsD</code> as a whole.</p> <p>For use cases like that of DogStatsD, create a bundle.</p>"},{"location":"components/creating-bundles/#creating-a-bundle_1","title":"Creating a bundle","text":"<p>A bundle eases the aggregation of multiple components and lives in <code>comp/&lt;bundlesName&gt;/</code>.</p>  comp/&lt;bundleName&gt;/bundle.go <pre><code>// Package &lt;bundleName&gt; ...\npackage &lt;bundleName&gt;\n\nimport (\n    \"github.com/DataDog/datadog-agent/pkg/util/fxutil\"\n\n    // We import all the components that we want to aggregate. A bundle must only aggregate components within its\n    // sub-folders.\n    comp1fx \"github.com/DataDog/datadog-agent/comp/&lt;bundleName&gt;/comp1/fx\"\n    comp2fx \"github.com/DataDog/datadog-agent/comp/&lt;bundleName&gt;/comp2/fx\"\n    comp3fx \"github.com/DataDog/datadog-agent/comp/&lt;bundleName&gt;/comp3/fx\"\n    comp4fx \"github.com/DataDog/datadog-agent/comp/&lt;bundleName&gt;/comp4/fx\"\n)\n\n// A single team must own the bundle, even if they don't own all the sub-components\n// team: &lt;the team owning the bundle&gt;\n\n// Bundle defines the fx options for this bundle.\nfunc Bundle() fxutil.BundleOptions {\n    return fxutil.Bundle(\n        comp1fx.Module(),\n        comp2fx.Module(),\n        comp3fx.Module(),\n        comp4fx.Module(),\n    )\n}\n</code></pre> <p>A bundle doesn't need to import all sub components. The idea is to offer a default, easy to use grouping of components. But nothing prevents users from cherry-picking the components they want to use.</p>"},{"location":"components/creating-components/","title":"Creating a Component","text":"<p>This page explains how to create components in detail.</p> <p>This page uses the example of creating a compression component. This component compresses a payload before sending it to the Datadog backend.</p> <p>Since there are multiple ways to compress data, this component provides two implementations of the same interface:</p> <ul> <li>The ZSTD data compression algorithm</li> <li>The ZIP data compression algorithm</li> </ul> <p>A component contains multiple folders and Go packages. Developers split a component into packages to isolate the interface from the implementations and improve code sharing. Declaring the interface in a separate package from the implementation allows you to import the interface without importing all of the implementations.</p>"},{"location":"components/creating-components/#file-hierarchy","title":"File hierarchy","text":"<p>All components are located in the <code>comp</code> folder at the top of the Agent repo.</p> <p>The file hierarchy is as follows:</p> <pre><code>comp /\n  &lt;bundle name&gt; /        &lt;-- Optional\n    &lt;comp name&gt; /\n      def /              &lt;-- The folder containing the component interface and ALL its public types.\n      impl /             &lt;-- The only or primary implementation of the component.\n      impl-&lt;alternate&gt; / &lt;-- An alternate implementation.\n      impl-none /        &lt;-- Optional. A noop implementation.\n      fx /               &lt;-- All fx related logic for the primary implementation, if any.\n      fx-&lt;alternate&gt; /   &lt;-- All fx related logic for a specific implementation.\n      mock /             &lt;-- The mock implementation of the component to ease testing.\n</code></pre> <p>To note:</p> <ul> <li>If your component has only one implementation, it should live in the <code>impl</code> folder.</li> <li>If your component has several implementations instead of a single implementation, you have multiple <code>impl-&lt;version&gt;</code> folders instead of an <code>impl</code> folder. For example, your compression component has <code>impl-zstd</code> and <code>impl-zip</code> folders, but not an <code>impl</code> folder.</li> <li>If your component needs to offer a dummy/empty version, it should live in the <code>impl-none</code> folder.</li> </ul>"},{"location":"components/creating-components/#why-all-those-files","title":"Why all those files ?","text":"<p>This file hierarchy aims to solve a few problems:</p> <ul> <li>Component users should only interact with the <code>def</code> folders and never care about which implementation was loaded in the   main function.</li> <li>Go module support: when you create a Go module, all sub folders are pulled into the module. Thus, you need different folders for each implementation, the definition, and fx. This way, an external repository can pull a specific implementation and definition without having to import everything else.</li> <li>You have one <code>fx</code> folder per implementation, to allow binaries to import/link against a single folder.</li> <li>A main function that imports a component should be able to select a specific implementation without compiling with the others.   For example: the ZSTD library should not be included at compile time when the ZIP version is used.</li> </ul>"},{"location":"components/creating-components/#bootstrapping-components","title":"Bootstrapping components","text":"<p>You can use the command <code>dda inv components.new-component comp/&lt;COMPONENT_NAME&gt;</code> to generate a scaffold for your new component.</p> <p>Every public variable, function, struct, and interface of your component must be documented. Refer to the Documentation section below for details.</p>"},{"location":"components/creating-components/#the-def-folder","title":"The def folder","text":"<p>The <code>def</code> folder contains your interface and ALL public types needed by the users of your component.</p> <p>In the example of a compression component, the def folder looks like this:</p>  comp/compression/def/component.go <pre><code>// Package compression contains all public type and interfaces for the compression component\npackage compression\n\n// team: &lt;your team&gt;\n\n// Component describes the interface implemented by all compression implementations.\ntype Component interface {\n    // Compress compresses the input data.\n    Compress([]byte) ([]byte, error)\n\n    // Decompress decompresses the input data.\n    Decompress([]byte) ([]byte, error)\n}\n</code></pre> <p>All component interfaces must be called <code>Component</code>, so all imports have the form <code>&lt;COMPONENT_NAME&gt;.Component</code>.</p> <p>You can see that the interface only exposes the bare minimum. You should aim at having the smallest possible interface for your component.</p> <p>When defining a component interface, avoid using structs or interfaces from third-party dependencies.</p> <p>Interface using a third-party dependency</p> <pre><code>package telemetry\n\nimport \"github.com/prometheus/client_golang/prometheus\"\n\n// team: agent-runtimes\n\n// Component is the component type.\ntype Component interface {\n    // RegisterCollector Registers a Collector with the prometheus registry\n    RegisterCollector(c prometheus.Collector)\n}\n</code></pre> <p>In the example above, every user of the <code>telemetry</code> component would have to import <code>github.com/prometheus/client_golang/prometheus</code> no matter which implementation they use.</p> <p>In general, be mindful of using external types in the public interface of your component. For example, it would make sense to use Docker types in a <code>docker</code> component, but not in a <code>container</code> component.</p>"},{"location":"components/creating-components/#the-impl-folders","title":"The impl folders","text":"<p>The <code>impl</code> folder is where the component implementation is written. The details of component implementation are up to the developer. The only requirements are that the package name follows the pattern <code>&lt;COMPONENT_NAME&gt;impl</code> for the regular implementation or <code>&lt;IMPL_NAME&gt;impl</code> for the alternative implementation, and that there is a public instantiation function called <code>NewComponent</code>.</p>  comp/compression/impl-zstd/compressor.go <pre><code>package zstdimpl\n\n// NewComponent returns a new ZSTD implementation for the compression component\nfunc NewComponent(reqs Requires) Provides {\n    ....\n}\n</code></pre> <p>To require input arguments to the <code>NewComponent</code> instantiation function, use a special struct named <code>Requires</code>. The instantiation function returns a special stuct named <code>Provides</code>. This internal nomenclature is used to handle the different component dependencies using Fx groups.</p> <p>In this example, the compression component must access the configuration component and the log component. To express this, define a <code>Requires</code> struct with two fields. The name of the fields is irrelevant, but the type must be the concrete type of interface that you require.</p>  comp/compression/impl-zstd/compressor.go <pre><code>package zstdimpl\n\nimport (\n    \"fmt\"\n\n    config \"github.com/DataDog/datadog-agent/comp/core/config/def\"\n    log \"github.com/DataDog/datadog-agent/comp/core/log/def\"\n)\n\n// Here, list all components and other types known by Fx that you need.\n// To be used in `fx` folders, type and field need to be public.\n//\n// In this example, you need config and log components.\ntype Requires struct {\n    Conf config.Component\n    Log  log.Component\n}\n</code></pre> <p>Using other components</p> <p>If you want to use another component within your own, add it to the <code>Requires</code> struct, and <code>Fx</code> will give it to you at initialization. Be careful of circular dependencies.</p> <p>For the output of the component, populate the <code>Provides</code> struct with the return values.</p>  comp/compression/impl-zstd/compressor.go <pre><code>package zstdimpl\n\nimport (\n    // Always import the component def folder, so that you can return a 'compression.Component' type.\n    compression \"github.com/DataDog/datadog-agent/comp/compression/def\"\n)\n\n// Here, list all the types your component is going to return. You can return as many types as you want; all of them are available through Fx in other components.\n// To be used in `fx` folders, type and field need to be public.\n//\n// In this example, only the compression component is returned.\ntype Provides struct {\n    Comp compression.Component\n}\n</code></pre> <p>All together, the component code looks like the following:</p>  comp/compression/impl-zstd/compressor.go <pre><code>package zstdimpl\n\nimport (\n    \"fmt\"\n\n    compression \"github.com/DataDog/datadog-agent/comp/compression/def\"\n    config \"github.com/DataDog/datadog-agent/comp/core/config/def\"\n    log \"github.com/DataDog/datadog-agent/comp/core/log/def\"\n)\n\ntype Requires struct {\n    Conf config.Component\n    Log  log.Component\n}\n\ntype Provides struct {\n    Comp compression.Component\n}\n\n// The actual type implementing the 'Component' interface. This type MUST be private, you need the guarantee that\n// components can only be used through their respective interfaces.\ntype compressor struct {\n    // Keep a ref on the config and log components, so that you can use them in the 'compressor' methods\n    conf config.Component\n    log  log.Component\n\n    // any other field you might need\n}\n\n// NewComponent returns a new ZSTD implementation for the compression component\nfunc NewComponent(reqs Requires) Provides {\n    // Here, do whatever is needed to build a ZSTD compression comp.\n\n    // And create your component\n    comp := &amp;compressor{\n        conf: reqs.Conf,\n        log:  reqs.Log,\n    }\n\n    return Provides{\n        comp: comp,\n    }\n}\n\n//\n// You then need to implement all methods from your 'compression.Component' interface\n//\n\n// Compress compresses the input data using ZSTD\nfunc (c *compressor) Compress(data []byte) ([]byte, error) {\n    c.log.Debug(\"compressing a buffer with ZSTD\")\n\n    // [...]\n    return compressData, nil\n}\n\n// Decompress decompresses the input data using ZSTD.\nfunc (c *compressor) Decompress(data []byte) ([]byte, error) {\n    c.log.Debug(\"decompressing a buffer with ZSTD\")\n\n    // [...]\n    return compressData, nil\n}\n</code></pre> <p>The constructor can return either a <code>Provides</code>, if it is infallible, or <code>(Provides, error)</code>, if it could fail. In the latter case, a non-nil error results in the Agent crashing at startup with a message containing the error.</p> <p>Each implementation follows the same pattern.</p>"},{"location":"components/creating-components/#the-fx-folders","title":"The fx folders","text":"<p>The <code>fx</code> folder must be the only folder importing and referencing Fx. It's meant to be a simple wrapper. Its only goal is to allow dependency injection with Fx for your component.</p> <p>All <code>fx.go</code> files must define a <code>func Module() fxutil.Module</code> function. The helpers contained in <code>fxutil</code> handle all the logic. Most <code>fx/fx.go</code> file should look the same as this:</p>  comp/compression/fx-zstd/fx.go <pre><code>package fx\n\nimport (\n    \"github.com/DataDog/datadog-agent/pkg/util/fxutil\"\n\n    // You must import the implementation you are exposing through FX\n    compressionimpl \"github.com/DataDog/datadog-agent/comp/compression/impl-zstd\"\n)\n\n// Module specifies the compression module.\nfunc Module() fxutil.Module {\n    return fxutil.Component(\n        // ProvideComponentConstructor will automatically detect the 'Requires' and 'Provides' structs\n        // of your constructor function and map them to FX.\n        fxutil.ProvideComponentConstructor(\n            compressionimpl.NewComponent,\n        )\n    )\n}\n</code></pre> <p>Optional dependencies</p> <p>To create an optional wrapper type for your component, you can use the helper function <code>fxutil.ProvideOptional</code>. This generic function requires the type of the component interface, and will automatically make a conversion function <code>optional.Option</code> for that component.</p> <p>More on this in the FAQ.</p> <p>For the ZIP implementation, create the same file in <code>fx-zip</code> folder. In most cases, your component has a single implementation. If so, you have only one <code>impl</code> and <code>fx</code> folder.</p>"},{"location":"components/creating-components/#fx-none","title":"<code>fx-none</code>","text":"<p>Some parts of the codebase might have optional dependencies on your components (see FAQ).</p> <p>If it's the case, you need to provide a fx wrapper called <code>fx-none</code> to avoid duplicating the use of <code>optional.NewNoneOption[def.Component]()</code> in all our binaries</p>  comp/compression/fx-none/fx.go <pre><code>import (\n    compression \"github.com/DataDog/datadog-agent/comp/compression/def\"\n)\n\nfunc Module() fxutil.Module {\n    return fxutil.Component(\n        fx.Provide(func() optional.Option[compression.Component] {\n            return optional.NewNoneOption[compression.Component]()\n        }))\n}\n</code></pre>"},{"location":"components/creating-components/#the-mock-folder","title":"The mock folder","text":"<p>To support testing, components MUST provide a mock implementation (unless your component has no public method in its interface).</p> <p>Your mock must implement the <code>Component</code> interface of the <code>def</code> folder but can expose more methods if needed. All mock constructors must take a <code>*testing.T</code> as parameter.</p> <p>In the following example, your mock has no dependencies and returns the same string every time.</p>  comp/compression/mock/mock.go <pre><code>//go:build test\n\npackage mock\n\nimport (\n    \"testing\"\n\n    compression \"github.com/DataDog/datadog-agent/comp/compression/def\"\n)\n\ntype Provides struct {\n    Comp compression.Component\n}\n\ntype mock struct {}\n\n// New returns a mock compressor\nfunc New(*testing.T) Provides {\n    return Provides{\n        comp: &amp;mock{},\n    }\n}\n\n// Compress compresses the input data using ZSTD\nfunc (c *mock) Compress(data []byte) ([]byte, error) {\n    return []byte(\"compressed\"), nil\n}\n\n// Decompress decompresses the input data using ZSTD.\nfunc (c *compressor) Decompress(data []byte) ([]byte, error) {\n    return []byte(\"decompressed\"), nil\n}\n</code></pre>"},{"location":"components/creating-components/#go-module","title":"Go module","text":"<p>Go modules are not mandatory, but if you want to allow your component to be used outside the <code>datadog-agent</code> repository, create Go modules in the following places:</p> <ul> <li>In the <code>impl</code>/<code>impl-*</code> folder that you want to expose (you can only expose some implementations).</li> <li>In the <code>def</code> folder to expose the interface</li> <li>In the <code>mock</code> folder to expose the mock</li> </ul> <p>Never add a Go module to the component folder (for example,<code>comp/compression</code>) or any <code>fx</code> folders.</p>"},{"location":"components/creating-components/#final-state","title":"Final state","text":"<p>In the end, a classic component folder should look like:</p> <pre><code>comp/&lt;COMPONENT_NAME&gt;/\n\u251c\u2500\u2500 def\n\u2502   \u2514\u2500\u2500 component.go\n\u251c\u2500\u2500 fx\n\u2502   \u2514\u2500\u2500 fx.go\n\u251c\u2500\u2500 impl\n\u2502   \u2514\u2500\u2500 component.go\n\u2514\u2500\u2500 mock\n    \u2514\u2500\u2500 mock.go\n\n4 directories, 4 files\n</code></pre> <p>The example compression component, which has two implementations, looks like:</p> <pre><code>comp/core/compression/\n\u251c\u2500\u2500 def\n\u2502   \u2514\u2500\u2500 component.go\n\u251c\u2500\u2500 fx-zip\n\u2502   \u2514\u2500\u2500 fx.go\n\u251c\u2500\u2500 fx-zstd\n\u2502   \u2514\u2500\u2500 fx.go\n\u251c\u2500\u2500 impl-zip\n\u2502   \u2514\u2500\u2500 component.go\n\u251c\u2500\u2500 impl-zstd\n\u2502   \u2514\u2500\u2500 component.go\n\u2514\u2500\u2500 mock\n    \u2514\u2500\u2500 mock.go\n\n6 directories, 6 files\n</code></pre> <p>This can seem like a lot for a single compression component, but this design answers the exponentially increasing complexity of the Agent ecosystem. Your component needs to behave correctly with many binaries composed of unique and shared components, outside repositories that want to pull only specific features, and everything in between.</p> <p>Important</p> <p>No components know how or where they will be used and MUST, therefore, respect all the rules above. It's a very common pattern for teams to work only on their use cases, thinking their code will not be used anywhere else. Although, customers want common behavior between all Datadog products (Agent, serverless, Agentless, Helm, Operator, etc.).</p> <p>A key idea behind the component is to produce shareable and reusable code.</p>"},{"location":"components/creating-components/#general-consideration-about-designing-components","title":"General consideration about designing components","text":"<p>Your component must:</p> <ul> <li>Be thread safe.</li> <li>Any public methods should be able to be used as soon as your constructor is called. It's OK if some do nothing or   drop data as long as the Agent lifecycle is still in its init phase (see lifecycle section for more | TODO).</li> <li>Be clearly documented (see section below).</li> <li>Be tested.</li> </ul>"},{"location":"components/creating-components/#documentation","title":"Documentation","text":"<p>The documentation (both package-level and method-level) should include everything a user of the component needs to know. In particular, the documentation must address any assumptions that might lead to panic if violated by the user.</p> <p>Detailed documentation of how to avoid bugs in using a component is an indicator of excessive complexity and should be treated as a bug. Simplifying the usage will improve the robustness of the Agent.</p> <p>Documentation should include:</p> <ul> <li>Precise information on when each method may be called. Can methods be called concurrently?</li> <li> <p>Precise information about data ownership of passed values and returned values. Users can assume that any mutable value   returned by a component will not be modified by the user or the component after it is returned. Similarly, any mutable   value passed to a component will not be later modified, whether by the component or the caller. Any deviation from these   defaults should be documented.</p> <p>Note</p> <p>It can be surprisingly hard to avoid mutating data -- for example, <code>append(..)</code> surprisingly mutates its first argument. It is also hard to detect these bugs, as they are often intermittent, cause silent data corruption, or introduce rare data races. Where performance is not an issue, prefer to copy mutable input and outputs to avoid any potential bugs.</p> </li> <li> <p>Precise information about goroutines and blocking. Users can assume that methods do not block indefinitely, so   blocking methods should be documented as such. Methods that invoke callbacks should be clear about how the callback is   invoked, and what it might do. For example, document whether the callback can block, and whether it might be called   concurrently with other code.</p> </li> <li>Precise information about channels. Is the channel buffered? What happens if the channel is not read from quickly   enough, or if reading stops? Can the channel be closed by the sender, and if so, what does that mean?</li> </ul>"},{"location":"components/faq/","title":"FAQ","text":""},{"location":"components/faq/#optional-component","title":"Optional Component","text":"<p>You might need to express the fact that some of your dependencies are optional. This often happens for components that interact with many other components if available (that is, if they were included at compile time). This allows your component to interact with each other without forcing their inclusion in the current binary.</p> <p>The option.Option type answers this need.</p> <p>For examples, consider the metadata components that are included in multiple binaries (<code>core-agent</code>, <code>DogStatsD</code>, etc.). These components use the <code>sysprobeconfig</code> component if it is available. <code>sysprobeconfig</code> is available in the <code>core-agent</code> but not in <code>DogStatsD</code>.</p> <p>To do this in the <code>metadata</code> component:</p> <pre><code>type Requires struct {\n    SysprobeConf optional.Option[sysprobeconfig.Component]\n    [...]\n}\n\nfunc NewMetadata(deps Requires) (metadata.Component) {\n    if sysprobeConf, found := deps.SysprobeConf.Get(); found {\n        // interact with sysprobeconfig\n    }\n}\n</code></pre> <p>The above code produces a generic component, included in both <code>core-agent</code> and <code>DogStatsD</code> binaries, that can interact with <code>sysprobeconfig</code> without forcing the binaries to compile with it.</p> <p>You can use this pattern for every component, since all components provide Fx with a conversion function to convert their <code>Component</code> interfaces to <code>optional.Option[Component]</code> (see creating components).</p>"},{"location":"components/fx/","title":"Fx","text":""},{"location":"components/fx/#overview-of-fx","title":"Overview of Fx","text":"<p>The Agent uses Fx as its application framework. While the linked Fx documentation is thorough, it can be a bit difficult to get started with. This document describes how Fx is used within the Agent in a more approachable style.</p>"},{"location":"components/fx/#what-is-it","title":"What Is It?","text":"<p>Fx's core functionality is to create instances of required types \"automatically,\" also known as dependency injection. Within the agent, these instances are components, so Fx connects components to one another. Fx creates a single instance of each component, on demand.</p> <p>This means that each component declares a few things about itself to Fx, including the other components it depends on. An \"app\" then declares the components it contains to Fx, and instructs Fx to start up the whole assembly.</p>"},{"location":"components/fx/#providing-and-requiring","title":"Providing and Requiring","text":"<p>Fx connects components using types. Within the Agent, these are typically interfaces named <code>Component</code>. For example, <code>scrubber.Component</code> might be an interface defining functionality for scrubbing passwords from data structures:</p>  scrubber/component.go <pre><code>type Component interface {\n    ScrubString(string) string\n}\n</code></pre> <p>Fx needs to know how to provide an instance of this type when needed, and there are a few ways:</p> <ul> <li><code>fx.Provide(NewScrubber)</code> where <code>NewScrubber</code> is a constructor that returns a <code>scrubber.Component</code>. This indicates that if and when a <code>scrubber.Component</code> is required, Fx should call <code>NewScrubber</code>. It will call <code>NewScrubber</code> only once, using the same value everywhere it is required.</li> <li><code>fx.Supply(scrubber)</code> where <code>scrubber</code> implements the <code>scrubber.Component</code> interface. When another component requires a <code>scrubber.Component</code>, this is the instance it will get.</li> </ul> <p>The first form is much more common, as most components have constructors that do interesting things at runtime. A constructor can return multiple arguments, in which case the constructor is called if any of those argument types are required. Constructors can also return <code>error</code> as the final return type. Fx will treat an error as fatal to app startup.</p> <p>Fx also needs to know when an instance is required, and this is where the magic happens. In specific circumstances, it uses reflection to examine the argument list of functions, and creates instances of each argument's type. Those circumstances are:</p> <ul> <li>Constructors used with <code>fx.Provide</code>. Imagine <code>NewScrubber</code> depends on the config module to configure secret matchers:       <pre><code>func NewScrubber(config config.Component) Component {\n    return &amp;scrubber{\n        matchers: makeMatchersFromConfig(config),\n    }\n}\n</code></pre></li> <li>Functions passed to <code>fx.Invoke</code>:     <pre><code>fx.Invoke(func(sc scrubber.Component) {\n    fmt.Printf(\"scrubbed: %s\", sc.ScrubString(somevalue))\n})\n</code></pre>     Like constructors, Invoked functions can take multiple arguments, and can optionally return an error. Invoked functions are called automatically when an app is created.</li> <li> <p>Pointers passed to <code>fx.Populate</code>.    <pre><code>var sc scrubber.Component\n// ...\nfx.Populate(&amp;sc)\n</code></pre>    Populate is useful in tests to fill an existing variable with a provided value. It's equivalent to <code>fx.Invoke(func(tmp scrubber.Component) { *sc = tmp })</code>.</p> <p>Functions can take multple arguments of different types, requiring all of them.</p> </li> </ul>"},{"location":"components/fx/#apps-and-options","title":"Apps and Options","text":"<p>You may have noticed that all of the <code>fx</code> methods defined so far return an <code>fx.Option</code>. They don't actually do anything on their own. Instead, Fx uses the functional options pattern from Rob Pike. The idea is that a function takes a variable number of options, each of which has a different effect on the result.</p> <p>In Fx's case, the function taking the options is <code>fx.New</code>, which creates a new <code>fx.App</code>. It's within the context of an app that requirements are met, constructors are called, and so on.</p> <p>Tying the example above together, a very simple app might look like this:</p> <pre><code>someValue = \"my password is hunter2\"\napp := fx.New(\n    fx.Provide(scrubber.NewScrubber),\n    fx.Invoke(func(sc scrubber.Component) {\n        fmt.Printf(\"scrubbed: %s\", sc.ScrubString(somevalue))\n    }))\napp.Run()\n// Output: scrubbed: my password is *******\n</code></pre> <p>For anything more complex, it's not practical to call <code>fx.Provide</code> for every component in a single source file. Fx has two abstraction mechanisms that allow combining lots of options into one app:</p> <ul> <li><code>fx.Options</code> simply bundles several Option values into a single Option that can be placed in a variable. As the example in the Fx documentation shows, this is useful to gather the options related to a single Go package, which might include un-exported items, into a single value typically named <code>Module</code>.</li> <li><code>fx.Module</code> is very similar, with two additional features. First, it requires a module name which is used in some Fx logging and can help with debugging. Second, it creates a scope for the effects of <code>fx.Decorate</code> and <code>fx.Replace</code>. The second feature is not used in the Agent.</li> </ul> <p>So a slightly more complex version of the example might be:</p>  scrubber/component.go main.go <pre><code>func Module() fxutil.Module {\n    return fx.Module(\"scrubber\",\n    fx.Provide(newScrubber))    // now newScrubber need not be exported\n}\n</code></pre> <pre><code>someValue = \"my password is hunter2\"\napp := fx.New(\n    scrubber.Module(),\n    fx.Invoke(func(sc scrubber.Component) {\n        fmt.Printf(\"scrubbed: %s\", sc.ScrubString(somevalue))\n    }))\napp.Run()\n// Output: scrubbed: my password is *******\n</code></pre>"},{"location":"components/fx/#lifecycle","title":"Lifecycle","text":"<p>Fx provides an <code>fx.Lifecycle</code> component that allows hooking into application start-up and shut-down. Use it in your component's constructor like this:</p> <pre><code>func newScrubber(lc fx.Lifecycle) Component {\n    sc := &amp;scrubber{..}\n    lc.Append(fx.Hook{OnStart: sc.start, OnStop: sc.stop})\n    return sc\n}\n\nfunc (sc *scrubber) start(ctx context.Context) error { .. }\nfunc (sc *scrubber) stop(ctx context.Context) error { .. }\n</code></pre> <p>This separates the application's lifecycle into a few distinct phases:</p> <ul> <li>Initialization - calling constructors to satisfy requirements, and calling invoked functions that require them.</li> <li>Startup - calling components' OnStart hooks (in the same order the components were initialized)</li> <li>Runtime - steady state</li> <li>Shutdown - calling components' OnStop hooks (reverse of the startup order)</li> </ul>"},{"location":"components/fx/#ins-and-outs","title":"Ins and Outs","text":"<p>Fx provides some convenience types to help build constructors that require or provide lots of types: <code>fx.In</code> and <code>fx.Out</code>. Both types are embedded in structs, which can then be used as argument and return types for constructors, respectively. By convention, these are named <code>dependencies</code> and <code>provides</code> in Agent code:</p> <pre><code>type dependencies struct {\n    fx.In\n\n    Config config.Component\n    Log log.Component\n    Status status.Component\n}\n\ntype provides struct {\n    fx.Out\n\n    Component\n    // ... (we'll see why this is useful below)\n}\n\nfunc newScrubber(deps dependencies) (provides, error) { // can return an fx.Out struct and other types, such as error\n    // ..\n    return provides {\n        Component: scrubber,\n        // ..\n    }, nil\n}\n</code></pre> <p>In and Out provide a nice way to summarize and document requirements and provided types, and also allow annotations via Go struct tags. Note that annotations are also possible with <code>fx.Annotate</code>, but it is much less readable and its use is discouraged.</p>"},{"location":"components/fx/#value-groups","title":"Value Groups","text":"<p>Value groups make it easier to produce and consume many values of the same type. A component can add any type into groups which can be consumed by other components.</p> <p>For example:</p> <p>Here, two components add a <code>server.Endpoint</code> type to the <code>server</code> group (note the <code>group</code> label in the <code>fx.Out</code> struct).</p>  todolist/todolist.go users/users.go <pre><code>type provides struct {\n    fx.Out\n    Component\n    Endpoint server.Endpoint `group:\"server\"`\n}\n</code></pre> <pre><code>type provides struct {\n    fx.Out\n    Component\n    Endpoint server.Endpoint `group:\"server\"`\n}\n</code></pre> <p>Here, a component requests all the types added to the <code>server</code> group. This takes the form of a slice received at instantiation (note once again the <code>group</code> label but in <code>fx.In</code> struct).</p>  server/server.go <pre><code>type dependencies struct {\n    fx.In\n    Endpoints []Endpoint `group:\"server\"`\n}\n</code></pre>"},{"location":"components/fx/#day-to-day-usage","title":"Day-to-Day Usage","text":"<p>Day-to-day, the Agent's use of Fx is fairly formulaic. Following the component guidelines, or just copying from other components, should be enough to make things work without a deep understanding of Fx's functionality.</p>"},{"location":"components/jmxfetch/","title":"JMXFetch","text":"<p>JMXFetch is the component of the Agent which is responsible for collecting metrics from Java applications.</p> <p>For more details on JMXFetch or developer documentation, please visit the project documentation on the JMXFetch GitHub repo.</p>"},{"location":"components/jmxfetch/#running-checks-requiring-jmxfetch","title":"Running checks requiring JMXFetch","text":"<p>If your goal is to run a JMX-based check:</p> <ol> <li>Download the <code>-jar-with-dependencies.jar</code> build of the latest version of JMXFetch from    <code>maven</code>, or create a custom build.</li> <li>Copy the jar file and rename it to <code>$GOPATH/src/github.com/DataDog/datadog-agent/dev/dist/jmx/jmxfetch.jar</code>.</li> <li>Run <code>dda inv agent.run</code>.</li> <li>Validate that the JMXFetch section appears in <code>agent status</code>.</li> </ol> <p>If you have a JMX-based integration configured to run, it automatically runs in your local JMXFetch instance.</p>"},{"location":"components/migration/","title":"Integrating with other components","text":"<p>After you create your component, you can link it to other components such as flares. (Others, like status pages or health, will come later).</p> <p>This section documents how to fully integrate your component in the Agent ecosystem.</p>"},{"location":"components/overview/","title":"Overview of components","text":"<p>The Agent is structured as a collection of components working together. Depending on how the binary is built, and how it is invoked, different components may be instantiated.</p>"},{"location":"components/overview/#what-is-a-component","title":"What is a component?","text":"<p>The goal of a component is to encapsulate a particular piece of logic/feature and provide a clear and documented interface. </p> <p>A component must:</p> <ul> <li>Hide the complexity of the implementation from its users.</li> <li>Be reusable no matter the context or the binary in which they're included.</li> <li>Be tested.</li> <li>Expose a mock implementation to help testing if it makes sense.</li> <li>Be owned by a single team which supports and maintains it.</li> </ul> <p>Any change within a component that don't change its interface should not require QA of another component using it.</p> <p>Since each component is an interface to the outside, it can have several implementations.</p>"},{"location":"components/overview/#fx-vs-go-module","title":"Fx vs Go module","text":"<p>Components are designed to be used with a dependency injection framework. In the Agent, we use Fx, a dependency injection framework, for this. All Agent binaries use Fx to load, coordinate, and start the required components.</p> <p>Some components are used outside the <code>datadog-agent</code> repository, where Fx is not available. To support this, the components implementation must not require Fx.  Component implementations can be exported as Go modules. The next section explains in more detail how to create components.</p> <p>The important information here is that it's possible to use components without Fx outside the Agent repository. This comes at the cost of manually doing the work of Fx.</p>"},{"location":"components/overview/#important-note-on-fx","title":"Important note on Fx","text":"<p>The component framework project's core goal is to improve the Agent codebase by decoupling parts of the code, removing global state and init functions, and increasing reusability by separating logical units into components. Fx itself is not intrinsic to the benefits of componentization.</p>"},{"location":"components/overview/#next","title":"Next","text":"<p>Next, see how to create a bundle and a component by using Fx.</p>"},{"location":"components/testing/","title":"Testing components","text":"<p>Testing is an essential part of the software development life cycle. This page covers everything you need to know about testing components.</p> <p>One of the core benefits of using components is that each component isolates its internal logic behind its interface. Focus on asserting that each implementation behaves correctly.</p> <p>To recap from the previous page, a component was created that compresses the payload before sending it to the Datadog backend. The component has two separate implementations.</p> <p>This is the component's interface:</p>  comp/compression/def/component.go <pre><code>type Component interface {\n    // Compress compresses the input data.\n    Compress([]byte) ([]byte, error)\n\n    // Decompress decompresses the input data.\n    Decompress([]byte) ([]byte, error)\n}\n</code></pre> <p>Ensure the <code>Compress</code> and <code>Decompress</code> functions behave correctly.</p> <p>Writing tests for a component implementation follows the same rules as any other test in a Go project. See the testing package documentation for more information.</p> <p>For this example, write a test file for the <code>zstd</code> implementation. Create a new file named <code>component_test.go</code> in the <code>impl-zstd folder</code>. Inside the test file, initialize the component's dependencies, create a new component instance, and test the behavior.</p>"},{"location":"components/testing/#initialize-the-components-dependencies","title":"Initialize the component's dependencies","text":"<p>All components expect a <code>Requires</code> struct with all the necessary dependencies. To ensure a component instance can be created, create a <code>requires</code> instance.</p> <p>The <code>Requires</code> struct declares a dependency on the config component and the log component. The following code snippet shows how to create the <code>Require</code> struct:</p>  comp/compression/impl-zstd/component_test.go <pre><code>package implzstd\n\nimport (\n    \"testing\"\n\n    configmock \"github.com/DataDog/datadog-agent/comp/core/config/mock\"\n    logmock \"github.com/DataDog/datadog-agent/comp/core/log/mock\"\n)\n\nfunc TestCompress(t *testing.T) {\n    logComponent := configmock.New(t)\n    configComponent := logmock.New(t)\n\n    requires := Requires{\n        Conf: configComponent,\n        Log: logComponent,\n    }\n    // [...]\n}\n</code></pre> <p>To create the log and config component, use their respective mocks. The mock package was mentioned previously in the Creating a Component page.</p>"},{"location":"components/testing/#testing-the-components-interface","title":"Testing the component's interface","text":"<p>Now that the <code>Require</code> struct is created, an instance of the component can be created and its functionality tested:</p>  comp/compression/impl-zstd/component_test.go <pre><code>package implzstd\n\nimport (\n    \"testing\"\n\n    configmock \"github.com/DataDog/datadog-agent/comp/core/config/mock\"\n    logmock \"github.com/DataDog/datadog-agent/comp/core/log/mock\"\n)\n\nfunc TestCompress(t *testing.T) {\n    logComponent := configmock.New(t)\n    configComponent := logmock.New(t)\n\n    requires := Requires{\n        Conf: configComponent,\n        Log: logComponent,\n    }\n\n    provides := NewComponent(requires)\n    component := provides.Comp\n\n    result, err := component.Compress([]byte(\"Hello World\"))\n    assert.Nil(t, err)\n\n    assert.Equal(t, ..., result)\n}\n</code></pre>"},{"location":"components/testing/#testing-lifecycle-hooks","title":"Testing lifecycle hooks","text":"<p>Sometimes a component uses Fx lifecycle to add hooks. It is a good practice to test the hooks as well.</p> <p>For this example, imagine a component wants to add some hooks into the app lifecycle. Some code is omitted for simplicity:</p>  comp/somecomponent/impl/component.go <pre><code>package impl\n\nimport (\n    \"context\"\n\n    somecomponent \"github.com/DataDog/datadog-agent/comp/somecomponent/def\"\n    compdef \"github.com/DataDog/datadog-agent/comp/def\"\n)\n\ntype Requires struct {\n    Lc      compdef.Lifecycle\n}\n\ntype Provides struct {\n    Comp somecomponent.Component\n}\n\ntype component struct {\n    started  bool\n    stopped bool\n}\n\nfunc (c *component) start() error {\n    // [...]\n\n    c.started = true\n\n    return nil\n}\n\nfunc (h *healthprobe) stop() error {\n    // [...]\n\n    c.stopped = true\n    c.started = false\n\n    return nil\n}\n\n// NewComponent creates a new healthprobe component\nfunc NewComponent(reqs Requires) (Provides, error) {\n    provides := Provides{}\n    comp := &amp;component{}\n\n    reqs.Lc.Append(compdef.Hook{\n        OnStart: func(ctx context.Context) error {\n            return comp.start()\n        },\n        OnStop: func(ctx context.Context) error {\n            return comp.stop()\n        },\n    })\n\n    provides.Comp = comp\n    return provides, nil\n}\n</code></pre> <p>The goal is to test that the component updates the <code>started</code> and <code>stopped</code> fields.</p> <p>To accomplish this, create a new lifecycle instance, create a <code>Require</code> struct instance, initialize the component, and validate that calling <code>Start</code> on the lifecycle instance calls the component hook and executes the logic.</p> <p>To create a lifecycle instance, use the helper function <code>compdef.NewTestLifecycle(t *testing.T)</code>. The function returns a lifecycle wrapper that can be used to populate the <code>Requires</code> struct. The <code>Start</code> and <code>Stop</code> functions can also be called.</p> <p>Info</p> <p>You can see the <code>NewTestLifecycle</code> function here.</p>  comp/somecomponent/impl/component_test.go <pre><code>package impl\n\nimport (\n    \"context\"\n    \"testing\"\n\n    compdef \"github.com/DataDog/datadog-agent/comp/def\"\n    \"github.com/stretchr/testify/assert\"\n)\n\nfunc TestStartHook(t *testing.T) {\n    lc := compdef.NewTestLifecycle(t)\n\n    requires := Requires{\n        Lc:  lc,\n    }\n\n    provides, err := NewComponent(requires)\n\n    assert.NoError(t, err)\n\n    assert.NotNil(t, provides.Comp)\n    internalComponent := provides.Comp.(*component)\n\n    ctx := context.Background()\n    lc.AssertHooksNumber(1)\n    assert.NoError(t, lc.Start(ctx))\n\n    assert.True(t, internalComponent.started)\n}\n</code></pre> <p>For this example, a type cast operation had to be performed because the <code>started</code> field is private. Depending on the component, this may not be necessary.</p>"},{"location":"components/using-components/","title":"Using components","text":"<p>Using components within other components is covered on the create components page.</p> <p>Now let's explore how to use components in your binaries. One of the core idea behind component design is to be able to create new binaries for customers by aggregating components.</p>"},{"location":"components/using-components/#the-cmd-folder","title":"the <code>cmd</code> folder","text":"<p>All <code>main</code> functions and binary entry points should be in the <code>cmd</code> folder.</p> <p>The <code>cmd</code> folder uses the following hierarchy:</p> <pre><code>cmd /\n    &lt;binary name&gt; /\n        main.go                   &lt;-- The entry points from your binary\n        subcommands /             &lt;-- All subcommand for your binary CLI\n            &lt;subcommand name&gt; /   &lt;-- The code specific to a single subcommand\n                command.go\n                command_test.go\n</code></pre> <p>Say you want to add a <code>test</code> command to the <code>agent</code> CLI.</p> <p>You would create the following file:</p>  cmd/agent/subcommands/test/command.go <pre><code>package test\n\nimport (\n    // [...]\n)\n\n// Commands returns a slice of subcommands for the 'agent' command.\n//\n// The Agent uses \"cobra\" to create its CLI. The command method is your entrypoint. Here, you're going to create a single\n// command.\nfunc Commands(globalParams *command.GlobalParams) []*cobra.Command {\n    cmd := &amp;cobra.Command{\n        Use:   \"test\",\n        Short: \"a test command for the Agent\",\n        Long:  ``,\n        RunE: func(_ *cobra.Command, _ []string) error {\n            return fxutil.OneShot(\n                &lt;callback&gt;,\n                &lt;list of dependencies&gt;.\n            )\n        },\n    }\n\n    return []*cobra.Command{cmd}\n}\n</code></pre> <p>The code above creates a test command that does nothing. As you can see, <code>fxutil.OneShot</code> helpers are being used. These helpers initialize an Fx app with all the wanted dependencies.</p> <p>The next section explains how to request a dependency.</p>"},{"location":"components/using-components/#importing-components","title":"Importing components","text":"<p>The <code>fxutil.OneShot</code> takes a list of components and gives them to Fx. Note that this only tells Fx how to create types when they're needed. This does not do anything else.</p> <p>For a component to be instantiated, it must be one of the following:</p> <ul> <li>Required as a parameter by the <code>callback</code> function</li> <li>Required as a dependency from other components already marked for instantiation</li> <li>Directly asked for by using <code>fx.Invoke</code>. More on this on the Fx page.</li> </ul> <p>Let's require the <code>log</code> components:</p> <pre><code>import (\n    // First let's import the FX wrapper to require it\n    logfx \"github.com/DataDog/datadog-agent/comp/core/log/fx\"\n    // Then the logger interface to use it\n    log \"github.com/DataDog/datadog-agent/comp/core/log/def\"\n)\n\n// [...]\n    return fxutil.OneShot(\n        myTestCallback, // The function to call from fxutil.OneShot\n        logfx.Module(), // This will tell FX how to create the `log.Component`\n    )\n// [...]\n\nfunc myTestCallback(logger log.Component) {\n    logger.Info(\"some message\")\n}\n</code></pre>"},{"location":"components/using-components/#importing-bundles","title":"Importing bundles","text":"<p>Now let's say you want to include the core bundle instead. The core bundle offers many basic features (logger, config, telemetry, flare, ...).</p> <pre><code>import (\n    // We import the core bundle\n    core \"github.com/DataDog/datadog-agent/comp/core\"\n\n    // Then the interfaces we want to use\n    config \"github.com/DataDog/datadog-agent/comp/core/config/def\"\n)\n\n// [...]\n    return fxutil.OneShot(\n        myTestCallback, // The function to call from fxutil.OneShot\n        core.Bundle(core.WithSecrets()),  // This will tell FX how to create the all the components included in the bundle\n    )\n// [...]\n\nfunc myTestCallback(conf config.Component) {\n    api_key := conf.GetString(\"api_key\")\n\n    // [...]\n}\n</code></pre> <p>It's very important to understand that since <code>myTestCallback</code> only uses the <code>config.Component</code>, not all components from the <code>core</code> bundle are instantiated! The <code>core.Bundle</code> instructs Fx how to create components, but only the ones required are created.</p> <p>In our example, the <code>config.Component</code> might have dozens of dependencies instantiated from the core bundle. Fx handles all of this.</p>"},{"location":"components/using-components/#using-plain-data-types-with-fx","title":"Using plain data types with Fx","text":"<p>As your migration to components is not finished, you might need to manually instruct Fx on how to use plain types.</p> <p>You will need to use <code>fx.Supply</code> for this. More details can be found here.</p> <p>But here is a quick example:</p> <pre><code>import (\n    logfx \"github.com/DataDog/datadog-agent/comp/core/log/fx\"\n    log \"github.com/DataDog/datadog-agent/comp/core/log/def\"\n)\n\n// plain custom type\ntype custom struct {}\n\n// [...]\n    return fxutil.OneShot(\n        myTestCallback,\n        logfx.Module(),\n\n        // fx.Supply populates values into Fx.\n        // Any time this is needed, Fx will use it.\n        fx.Supply(custom{})\n    )\n// [...]\n\n// Here our function uses component and non-component type, both provided by Fx.\nfunc myTestCallback(logger log.Component, c custom) {\n    logger.Info(\"Custom type: %v\", c)\n}\n</code></pre> <p>Info</p> <p>This means that components can depend on plain types too (as long as the main entry point populates Fx options with them).</p>"},{"location":"components/shared_features/flares/","title":"Flare","text":"<p>The general idea is to register a callback within your component to be called each time a flare is created. This uses Fx groups under the hood, but helpers are there to abstract all the complexity.</p> <p>Once the callback is created, you will have to migrate the code related to your component from <code>pkg/flare</code> to your component.</p>"},{"location":"components/shared_features/flares/#creating-a-callback","title":"Creating a callback","text":"<p>To add data to a flare, you first need to register a callback, also known as a <code>FlareBuilder</code>.</p> <p>Within your component, create a method with the following signature: <code>func (c *yourComp) fillFlare(fb flaretypes.FlareBuilder) error</code>.</p> <p>This function is called every time the Agent generates a flare\u2014whether from the CLI, RemoteConfig, or from the running Agent. Your callback takes a FlareBuilder as parameter. This object provides all the helpers functions needed to add data to a flare (adding files, copying directories, scrubbing data, and so on).</p> <p>Example:</p> <pre><code>import (\n    yaml \"gopkg.in/yaml.v2\"\n\n    flare \"github.com/DataDog/datadog-agent/comp/core/flare/def\"\n)\n\nfunc (c *myComponent) fillFlare(fb flare.FlareBuilder) error {\n    // Creating a new file\n    fb.AddFile(\n        \"runtime_config_dump.yaml\",\n        []byte(\"content of my file\"),\n    ) //nolint:errcheck\n\n    // Copying a file from the disk into the flare\n    fb.CopyFile(\"/etc/datadog-agent/datadog.yaml\") //nolint:errcheck\n    return nil\n}\n</code></pre> <p>Read the FlareBuilder package documentation for more information on the API.</p> <p>Any errors returned by the <code>FlareBuilder</code> methods are logged into a file shipped within the flare. This means, in most cases, you can ignore errors returned by the <code>FlareBuilder</code> methods. In all cases, ship as much data as possible in a flare instead of stopping at the first error.</p> <p>Returning an error from your callback does not stop the flare from being created or sent. Rather, the error is logged into the flare too.</p> <p>While it's possible to register multiple callbacks from the same component, try to keep all the flare code in a single callback.</p>"},{"location":"components/shared_features/flares/#register-your-callback","title":"Register your callback","text":"<p>Now you need to register your callback to be called each time a flare is created. To do this, your component constructor needs to provide a new Provider. Use NewProvider function for this.</p> <p>Example:</p> <pre><code>import (\n    flare \"github.com/DataDog/datadog-agent/comp/core/flare/def\"\n)\n\ntype Provides struct {\n    // [...]\n\n    // Declare that your component will return a flare provider\n    FlareProvider flare.Provider\n}\n\nfunc newComponent(deps Requires) Provides {\n    // [...]\n\n    return Provides{\n        // [...]\n\n        // NewProvider will wrap your callback in order to be use as a 'Provider'\n        FlareProvider: flare.NewProvider(myComponent.fillFlare),\n    }, nil\n}\n</code></pre>"},{"location":"components/shared_features/flares/#testing","title":"Testing","text":"<p>The flare component offers a FlareBuilder mock to test your callback.</p> <p>Example:</p> <pre><code>import (\n    \"testing\"\n    \"github.com/DataDog/datadog-agent/comp/core/flare/helpers\"\n)\n\nfunc TestFillFlare(t testing.T) {\n    myComp := newComponent(...)\n\n    flareBuilderMock := helpers.NewFlareBuilderMock(t)\n\n    myComp.fillFlare(flareBuilderMock, false)\n\n    flareBuilderMock.AssertFileExists(\"datadog.yaml\")\n    flareBuilderMock.AssertFileContent(\"some_file.txt\", \"my content\")\n    // ...\n}\n</code></pre>"},{"location":"components/shared_features/flares/#migrating-your-code","title":"Migrating your code","text":"<p>Now comes the hard part: migrating the code from <code>pkg/flare</code> related to your component to your new callback.</p> <p>The good news is that the code in <code>pkg/flare</code> already uses the <code>FlareBuilder</code> interface. So you shouldn't need to rewrite any logic. Don't forget to migrate the tests too and expand them (most of the flare features are not tested).</p> <p>Keep in mind that the goal is to delete <code>pkg/flare</code> once the migration to component is done.</p>"},{"location":"components/shared_features/metadata/","title":"Metadata","text":""},{"location":"components/shared_features/remote_config/","title":"Remote Config","text":""},{"location":"components/shared_features/status/","title":"Status","text":"<p>Components can register a status provider. When the status command is executed, we will populate the information displayed using all the status providers.</p>"},{"location":"components/shared_features/status/#status-providers","title":"Status Providers","text":"<p>There are two types of status providers:</p> <ul> <li>Header Providers: these providers are displayed at the top of the status output. This section is reserved for the most important information about the agent, such as agent version, hostname, host info, or metadata.</li> <li>Regular Providers: these providers are rendered after all the header providers.</li> </ul> <p>Each provider has the freedom to configure how they want to display their information for the three types of status output: JSON, Text, and HTML. This flexibility allows you to tailor the output to best suit your component's needs.</p> <p>The JSON and Text outputs are displayed within the status CLI, while the HTML output is used for the Agent GUI.</p> <p>To guarantee consistent output, we order the status providers internally. The ordering mechanism is different depending on the status provider. We order the header providers based on an index using the ascending direction. The regular providers are ordered alphabetically based on their names.</p>"},{"location":"components/shared_features/status/#header-providers-interface","title":"Header Providers Interface","text":"<pre><code>type HeaderProvider interface {\n    // Index is used to choose the order in which the header information is displayed.\n    Index() int\n    // When displaying the Text output the name is render as a header\n    Name() string\n    JSON(verbose bool, stats map[string]interface{}) error\n    Text(verbose bool, buffer io.Writer) error\n    HTML(verbose bool, buffer io.Writer) error\n}\n</code></pre>"},{"location":"components/shared_features/status/#regular-providers-interface","title":"Regular Providers Interface","text":"<pre><code>// Provider interface\ntype Provider interface {\n    // Name is used to sort the status providers alphabetically.\n    Name() string\n    // Section is used to group the status providers.\n    // When displaying the Text output the section is render as a header\n    Section() string\n    JSON(verbose bool, stats map[string]interface{}) error\n    Text(verbose bool, buffer io.Writer) error\n    HTML(verbose bool, buffer io.Writer) error\n}\n</code></pre>"},{"location":"components/shared_features/status/#adding-a-status-provider","title":"Adding a status provider","text":"<p>To add a status provider to your component, you need to declare it in the return value of its <code>NewComponent()</code> function.</p> <p>The status component provides helper functions to create status providers: <code>NewInformationProvider</code> and <code>NewHeaderInformationProvider</code>.</p> <p>Also, the status component has helper functions to render text and HTML output: <code>RenderText</code> and <code>RenderHTML.</code> The signature for both functions is:</p> <pre><code>(templateFS embed.FS, template string, buffer io.Writer, data any)\n</code></pre> <p>The <code>embed.FS</code> variable points to the location of the different status templates. These templates must be inside the component files. The folder must be named <code>status_templates</code>. The name of the templates do not have any rules, but to keep the same consistency across the code, we suggest using <code>\"&lt;component&gt;.tmpl\"</code> for the text template and <code>\"&lt;component&gt;HTML.tmpl\"</code> for the HTML template.</p> <p>Below is an example of adding a status provider to your component.</p>  comp/compression/impl/compressor.go <pre><code>package impl\n\nimport (\n    \"fmt\"\n\n    compression \"github.com/DataDog/datadog-agent/comp/compression/def\"\n    \"github.com/DataDog/datadog-agent/comp/status\"\n)\n\ntype Requires struct {\n}\n\ntype Provides struct {\n    Comp compression.Component\n    Status status.InformationProvider\n}\n\ntype compressor struct {\n}\n\n// NewComponent returns an implementation for the compression component\nfunc NewComponent(reqs Requires) Provides {\n    comp := &amp;compressor{}\n\n    return Provides{\n        Comp: comp,\n        Status: status.NewInformationProvider(&amp;comp)\n    }\n}\n\n//\n// Since we are using the compressor as status provider we need to implement the status interface on our component\n//\n\n//go:embed status_templates\nvar templatesFS embed.FS\n\n// Name renders the name\nfunc (c *compressor) Name() string {\n    return \"Compression\"\n}\n\n// Index renders the index\nfunc (c *compressor) Section() int {\n    return \"Compression\"\n}\n\n// JSON populates the status map\nfunc (c *compressor) JSON(_ bool, stats map[string]interface{}) error {\n    c.populateStatus(stats)\n\n    return nil\n}\n\n// Text renders the text output\nfunc (c *compressor) Text(_ bool, buffer io.Writer) error {\n    return status.RenderText(templatesFS, \"compressor.tmpl\", buffer, c.getStatusInfo())\n}\n\n// HTML renders the html output\nfunc (c *compressor) HTML(_ bool, buffer io.Writer) error {\n    return status.RenderHTML(templatesFS, \"compressorHTML.tmpl\", buffer, c.getStatusInfo())\n}\n\nfunc (c *compressor) populateStatus(stats map[string]interface{}) {\n    // Here we populate whatever informatiohn we want to display for our component\n    stats[\"compressor\"] = ...\n}\n\nfunc (c *compressor) getStatusInfo() map[string]interface{} {\n    stats := make(map[string]interface{})\n\n    c.populateStatus(stats)\n\n    return stats\n}\n</code></pre>"},{"location":"components/shared_features/status/#testing","title":"Testing","text":"<p>A critical part of your component development is ensuring that the status output is displayed as expected is. We highly encourage you to add tests to your components, giving you the confidence that your status output is accurate and reliable. For our example above, testing the status output is as easy as testing the result of calling <code>JSON</code>, <code>Text</code> and <code>HTML</code>.</p>  comp/compression/impl/component_test.go <pre><code>package impl\n\nimport (\n    \"bytes\"\n    \"testing\"\n)\n\nfunc TestText(t *testing.T) {\n    requires := Requires{}\n\n    provides := NewComponent(requires)\n    component := provides.Comp\n    buffer := new(bytes.Buffer)\n\n    result, err := component.Text(false, buffer)\n    assert.Nil(t, err)\n\n    assert.Equal(t, ..., string(result))\n}\n\nfunc TestJSON(t *testing.T) {\n    requires := Requires{}\n\n    provides := NewComponent(requires)\n    component := provides.Comp\n    info := map[string]interface{}\n\n    result, err := component.JSON(false, info)\n    assert.Nil(t, err)\n\n    assert.Equal(t, ..., result[\"compressor\"])\n}\n</code></pre> <p>To complete testing, we encourage adding the new status section output as part of the e2e tests. The CLI status e2e tests are in <code>test/new-e2e/tests/agent-subcommands/status</code> folder.</p>"},{"location":"components/shared_features/workloadmeta/","title":"Workloadmeta","text":""},{"location":"guidelines/contributing/","title":"Contributing to Datadog Agent","text":"<p>First of all, thanks for contributing!</p> <p>This document provides some basic guidelines for contributing to this repository. To propose improvements, feel free to submit a PR.</p>"},{"location":"guidelines/contributing/#submitting-issues","title":"Submitting issues","text":"<ul> <li>If you think you've found an issue, please search the Agent Troubleshooting section to see if it's known.</li> <li>If you\u2019re still unsure about the issue, you may reach out to the Datadog support team with a flare from your Agent.</li> <li>Finally, you can open a Github issue. Add as much information as possible to help us triage the issue.</li> </ul>"},{"location":"guidelines/contributing/#pull-requests","title":"Pull Requests","text":"<p>Have you fixed a bug or written a new check and want to share it? Many thanks!</p> <p>In order to ease/speed up our review, here are some items you can check/improve when submitting your PR:</p> Contributor ChecklistReviewer Checklist <ul> <li> Have a proper commit history (we advise you to rebase if needed) with clear commit messages.</li> <li> Sign your commits.</li> <li> Write tests for the code you wrote.</li> <li> Preferably make sure that all tests pass locally.</li> <li> Summarize your PR with an explanatory title and a message describing your changes, cross-referencing any related bugs/PRs.  Commit titles should be prefixed with general area of pull request's change. You can use conventional commits for the PR title.</li> <li> Use Reno to create a release note.</li> <li> Open your PR as a Draft against the <code>main</code> branch.</li> <li> Sign the Contributor Licence Agreement.</li> <li> Provide adequate QA/testing plan information.</li> <li> When the tests all pass, change your PR from \"Draft\" to \"Ready for review\".</li> </ul> <ul> <li> The added code comes with tests.</li> <li> The CI is green, all tests are passing (required or not).</li> <li> All applicable labels are set on the PR (see PR labels list).</li> <li> If applicable, the config template has been updated.</li> </ul> <p>Note</p> <p>Adding GitHub labels is only possible for contributors with write access.</p> <p>If your PR changes behavior, you must create or update any relevant tests.</p> <p>Your pull request must pass all CI tests before we will merge it. If you're seeing an error and don't think it's your fault, it may not be! Join us on Slack or send us an email, and together we'll get it sorted out.</p>"},{"location":"guidelines/contributing/#merge-protection","title":"Merge protection","text":"<p>We have safety measures to protect our repository. As an external contributor, you are required to:</p> <ol> <li>Sign our Contributor License Agreement (CLA). You will receive a message once your PR is opened to sign the agreement.</li> <li>Provide signed commits before merging. To learn how to sign your commits, follow this procedure from GitHub.</li> </ol>"},{"location":"guidelines/contributing/#keep-it-small-focused","title":"Keep it small, focused","text":"<p>Avoid changing too many things at once. For instance if you're fixing the NTP check and at the same time shipping a dogstatsd improvement, it makes reviewing harder and the time-to-release longer.</p>"},{"location":"guidelines/contributing/#commit-messages","title":"Commit Messages","text":"<p>Please don't be this person: <code>git commit -m \"Fixed stuff\"</code>. Take a moment to write meaningful commit messages.</p> <p>The commit message should describe the reason for the change and give extra details that will allow someone later on to understand in 5 seconds the thing you've been working on for a day.</p> <p>This includes editing the commit message generated by GitHub from:</p> <pre><code>Including new features\n\n* Fix linter\n* WIP\n* Add test for x86\n* Fix licenses\n* Cleanup headers\n</code></pre> <p>to:</p> <pre><code>Including new features\n\nThis feature does this and that. Some tests are excluded on x86 because of ...\n</code></pre>"},{"location":"guidelines/contributing/#pull-request-workflow","title":"Pull request workflow","text":"<p>The goals ordered by priority are:</p> <ul> <li>Make PR reviews (both initial and follow-up reviews) easy for reviewers using GitHub</li> <li>On the <code>main</code> branch, have a meaningful commit history that allows understanding (even years later) what each commit does, and why.</li> </ul> <p>You must open the PR when the code is reviewable or you must set the PR as draft if you want to share code before it's ready for actual reviews.</p>"},{"location":"guidelines/contributing/#write-a-good-pr-description","title":"Write a good PR description","text":"<p>Reviewers and future maintainers only see the PR description, not your individual commit history, so the description should incorporate everything that they will need. The merge commit may include PR descriptions from multiple PRs, so the description should tie back to the changed code in some way. For example <code>run mdformat on iot-agent team docs</code> rather than just <code>mdformat the docs</code>.</p> <ul> <li>A description of what is changed.</li> <li>A reason why the change is made. Pointing to an issue is usually a good reason.</li> <li>When testing had to include work not covered by test suites, a description of how you validated your change.</li> <li>Any relevant benchmarks.</li> <li>Additional notes that make code understanding easier.</li> <li>If this is part of a chain of PRs, point to the predecessors.</li> <li>If there are drawbacks or tradeoffs to consider, raise them here.</li> </ul>"},{"location":"guidelines/contributing/#before-the-first-pr-review","title":"Before the first PR review","text":"<p>Before the first PR review, meaningful commits are best: logically-encapsulated commits help the reviews go quicker and make the job for the reviewer easier. Conflicts with <code>main</code> can be resolved with a <code>git rebase origin/main</code> and a force push if it makes future review(s) easier.</p>"},{"location":"guidelines/contributing/#after-the-first-review","title":"After the first review","text":"<p>After the first review, to make follow-up reviews easier:</p> <ul> <li>Avoid force pushes: rewriting the history that was already reviewed makes follow-up reviews painful as GitHub loses track of each comment. Instead, address reviews with additional commits on the PR branch.</li> <li>Resolve merge conflicts with <code>main</code> using <code>git merge origin/main</code></li> </ul>"},{"location":"guidelines/contributing/#how-to-merge-to-main","title":"How to merge to <code>main</code>","text":"<p>Once reviews are complete, the merge to <code>main</code> should be done with either:</p> <ul> <li>the squash-merge option, to keep the history of <code>main</code> clean (even though some context/details are lost in the squash). The commit message for this squash should always be edited to concisely describe the commit without extraneous \u201caddress review comments\u201d text.</li> <li>the \u201crebase-merge\u201d option, after manually rewriting the PR\u2019s commit history and force-pushing to the branch. When using this option, the branch must have a clean history.</li> </ul>"},{"location":"guidelines/contributing/#reno","title":"Reno","text":"<p>We use <code>Reno</code> to create our CHANGELOG. Reno is a pretty simple tool.</p> <p>Each PR should include a <code>releasenotes</code> file created with <code>reno</code>, unless the PR doesn't have any impact on the behavior of the Agent and therefore shouldn't be mentioned in the CHANGELOG (examples: repository documentation updates, changes in code comments). PRs that don't require a release note file will be labeled <code>changelog/no-changelog</code> by maintainers.</p> <p>To install reno: <code>pip install reno</code></p> <p>Ultra quick <code>Reno</code> HOWTO:</p> <pre><code>$&gt; reno new &lt;topic-of-my-pr&gt; --edit\n[...]\n# Remove unused sections and fill the relevant ones.\n# Reno will create a new file in releasenotes/notes.\n#\n# Each section from every release note are combined when the CHANGELOG.rst is\n# rendered. So the text needs to be worded so that it does not depend on any\n# information only available in another section. This may mean repeating some\n# details, but each section must be readable independently of the other.\n#\n# Each section note must be formatted as reStructuredText.\n[...]\n</code></pre> <p>Then just add and commit the new releasenote (located in <code>releasenotes/notes/</code>) with your PR. If the change is on the <code>trace-agent</code> (folders <code>cmd/trace-agent</code> or <code>pkg/trace</code>) please prefix the release note with \"APM :\" and the  argument with \"apm-\"."},{"location":"guidelines/contributing/#reno-sections","title":"Reno sections","text":"<p>The main thing to keep in mind is that the CHANGELOG is written for the agent's users and not its developers.</p> <ul> <li> <p><code>features</code>: describe shortly what your feature does.</p> <p>example: <pre><code>features:\n  - |\n    Introducing the Datadog Process Agent for Windows.\n</code></pre></p> </li> <li> <p><code>enhancements</code>: describe enhancements here: new behavior that are too small to be considered a new feature.</p> <p>example: <pre><code>enhancements:\n  - |\n    Windows: Add PDH data to flare.\n</code></pre></p> </li> <li> <p><code>issues</code>: describe known issues or limitation of the agent.</p> <p>example: <pre><code>issues:\n  - |\n    Kubernetes 1.3 &amp; OpenShift 3.3 are currently not fully supported: docker\n    and kubelet integrations work OK, but apiserver communication (event\n    collection, `kube_service` tagging) is not implemented\n</code></pre></p> </li> <li> <p><code>upgrade</code>: List actions to take or limitations that could arise upon upgrading the Agent. Notes here must include steps that users can follow to:</p> <ol> <li>know if they're affected, and</li> <li>handle the change gracefully on their end.</li> </ol> <p>example: <pre><code>upgrade:\n  - |\n    If you run a Nomad agent older than 0.6.0, the `nomad_group`\n    tag will be absent until you upgrade your orchestrator.\n</code></pre></p> </li> <li> <p><code>deprecations</code>: List deprecation notes here.</p> <p>example: <pre><code>deprecations:\n- |\n  Changed the attribute name to enable log collection from YAML configuration\n  file from \"log_enabled\" to \"logs_enabled\", \"log_enabled\" is still\n  supported.\n</code></pre></p> </li> <li> <p><code>security</code>: List security fixes, issues, warning or related topics here.</p> <p>example: <pre><code>security:\n  - |\n    The /agent/check-config endpoint has been patched to enforce\n    authentication of the caller via a bearer session token.\n</code></pre></p> </li> <li> <p><code>fixes</code>: List the fixes done in your PR here. Remember to be clear and give a minimum of context so people reading the CHANGELOG understand what the fix is about.</p> <p>example: <pre><code>fixes:\n  - |\n    Fix EC2 tags collection when multiple marketplaces are set.\n</code></pre></p> </li> <li> <p><code>other</code>: Add here every other information you want in the CHANGELOG that don't feat in any other section. This section should rarely be used.</p> <p>example: <pre><code>other:\n  - |\n    Only enable the ``resources`` metadata collector on Linux by default, to match\n    Agent 5's behavior.\n</code></pre></p> </li> </ul>"},{"location":"guidelines/contributing/#pr-labels","title":"PR labels","text":"<p>For internal PRs (from people in the Datadog organization), you have few extra labels that can be use:</p> <ul> <li><code>community/help-wanted</code>: for community PRs where help is needed to finish it.</li> <li><code>community</code>: for community PRs.</li> <li><code>changelog/no-changelog</code>: for PRs that don't require a reno releasenote (useful for PRs only changing documentation or tests).</li> <li> <p><code>qa/done</code> or <code>qa/no-code-change</code>: used to skip the QA week:</p> <ul> <li><code>qa/done</code> label is recommended in case of code changes and manual / automated QA done before merge.</li> <li><code>qa/no-code-change</code> is recommended if there's no code changes in the Agent binary code.</li> </ul> <p>Important</p> <p>Use <code>qa/no-code-change</code> if your PR only changes tests or a module/package that does not end up in the Agent build. All of the following do not require QA:</p> <ul> <li>Changing the CI configuration without impacting the Agent packaging.</li> <li>Changing the documentation.</li> <li>Changing the developer tooling.</li> </ul> </li> <li> <p><code>major_change</code>: to flag the PR as a major change impacting many/all teams working on the agent and will require deeper QA (example: when we change the Python version shipped in the agent).</p> </li> <li><code>need-change/operator</code>, <code>need-change/helm</code>: indicate that the configuration needs to be modified in the operator / helm chart as well.</li> <li><code>k8s/&lt;min-version&gt;</code>: indicate the lowest Kubernetes version compatible with the PR's feature.</li> <li> <p><code>backport/&lt;branch-name&gt;</code>: Add this label to automatically create a PR against the <code>&lt;branch-name&gt;</code> branch with your backported changes. The backport PR creation is triggered:</p> <ul> <li>When a PR with the label is merged</li> <li>When an already-merged PR gets the label</li> </ul> <p>If there is a conflict, the bot prompts you with a list of instructions to follow (example) to manually backport your PR.</p> </li> </ul>"},{"location":"guidelines/contributing/#integrations","title":"Integrations","text":"<p>Also called checks, all officially supported Agent integrations live in the integrations-core repo. Please look there to submit related issues, PRs, or review the latest changes. For new integrations, please open a pull request in the integrations-extras repo.</p>"},{"location":"guidelines/docs/","title":"Writing developer docs","text":"<p>This site is built by MkDocs and uses the Material for MkDocs theme.</p> <p>You can serve documentation locally with the <code>dda run docs serve</code> command.</p>"},{"location":"guidelines/docs/#organization","title":"Organization","text":"<p>The site structure is defined by the <code>nav</code> key in the <code>mkdocs.yml</code> file.</p> <p>We strive to follow the principles of the Di\u00e1taxis documentation framework.</p> <p>When adding new pages, first think about what it is exactly that you are trying to document. For example, if you intend to write about something everyone must follow as a standard practice it would be classified as a guideline whereas a short piece about performing a particular task would be a how-to.</p> <p>After deciding the kind of content, further segment the page under logical groupings for easier navigation.</p>"},{"location":"guidelines/docs/#ordered-lists","title":"Ordered lists","text":"<p>Each item in an ordered list should start with <code>1.</code> and let rendering handle the rest. This is recommended for two reasons:</p> <ol> <li>Changes to the list size do not require re-numbering unmodified items and therefore reduces the diff when reviewing.</li> <li>Rendering will expose improper formatting by having the sequence broken rather than hiding such issues.</li> </ol>"},{"location":"guidelines/docs/#line-continuations","title":"Line continuations","text":"<p>For prose where the rendered content should have no line breaks, always keep the Markdown on the same line. This removes the need for any stylistic enforcement and allows for IDEs to intelligently wrap as usual.</p> <p>Tip</p> <p>When you wish to force a line continuation but stay within the block, indent by 2 spaces from the start of the text and end the block with a new line. For example, the following shows how you would achieve a multi-line ordered list item:</p> <ul> <li> <p>Markdown</p> <pre><code>1. first line\n\n     second line\n\n1. third line\n</code></pre> </li> <li> <p>Rendered</p> <ol> <li> <p>first line</p> <p>second line</p> </li> <li> <p>third line</p> </li> </ol> </li> </ul>"},{"location":"guidelines/docs/#emphasis","title":"Emphasis","text":"<p>When you want to call something out, use admonitions rather than making large chunks of text bold or italicized. The latter is okay for small spans within sentences.</p> <p>Here's an example:</p> <ul> <li> <p>Markdown</p> <pre><code>/// info\nLorem ipsum ...\n///\n</code></pre> </li> <li> <p>Rendered</p> <p>Info</p> <p>Lorem ipsum ...</p> </li> </ul>"},{"location":"guidelines/docs/#links","title":"Links","text":"<p>Always use inline links rather than reference links.</p> <p>The only exception to that rule is links that many pages may need to reference. Such links may be added to this file that all pages are able to reference.</p>"},{"location":"guidelines/docs/#abbreviations","title":"Abbreviations","text":"<p>Abbreviations like DSD may be added to this file which will make it so that a tooltip will be displayed on hover.</p>"},{"location":"guidelines/conventions/logging/","title":"Logging","text":""},{"location":"guidelines/conventions/logging/#writing-a-good-log-message","title":"Writing a good log message","text":"<p>In general, there are a few rules and a few suggestions to follow when it comes to writing good log messages:</p> <ul> <li>Messages must be written in English, preferably in American English.</li> <li>Use proper spelling and grammar when possible. Because not everyone is a native English speaker, this is an ask, not a hard requirement.</li> <li>Identifiers or passages of note should be called out by some means such as wrapping them in   backticks or quotes (single or double). Wrapping with special characters can be helpful in drawing the user's eye to anything of importance.</li> <li>If the message is longer than one or two sentences, it's probably better suited as a single sentence briefly   explaining the event, with a link to external documentation that explains things further.</li> </ul>"},{"location":"guidelines/conventions/logging/#choosing-the-right-log-level","title":"Choosing the right log level","text":"<p>Choosing the right level is also very important. Appropriate log levels make it easy for users to understand what they should pay attention to. They also avoid the performance overhead of excess logging, even if the logs are filtered and never show on the console.</p> <ul> <li> <p>TRACE: Typically contains a high level of detail for deep/rich debugging.</p> <p>Trace logging is typically used when instrumenting algorithms and core pieces of logic. Avoid adding trace logging to tight loops or commonly used codepaths. Even when the logs are disabled, logging an event can incur overheads.</p> </li> <li> <p>DEBUG: Basic information that can be helpful for initially debugging issues.</p> <p>Do not use debug logging for things that happen per-event or that scale with event throughput. You can safely use debug logging for uncommon cases, for example, something that happens every 1000<sup>th</sup> event.</p> </li> <li> <p>INFO: Common information about normal processes.</p> <p>Info logging is appropriate for logical or temporal events. Examples include notifications when components are stopped and started, or other high-level events that do not require operator attention.</p> <p>INFO is primarily used for information that tells an operator that a notable action completed successfully.</p> </li> <li> <p>WARN should be used for potentially problematic but non-critical events where the software can continue operating,     potentially in a degraded state and/or recover from the problem. Do not use WARN for events that require user's immediate attention.</p> </li> <li> <p>ERROR level should be used for events indicating severely problematic issues that require immediate user visibility and remediation.</p> <p>This includes logging related to events that may lead to data loss, unrecoverable states, and any other situation where a required component is faulty, causing the software to be unable to remediate the problem on its own. Error logs should be extremely rare in normally operating software to ensure high signal-to-noise ratio in observability tooling.</p> </li> </ul>"},{"location":"guidelines/deprecated-components-documentation/defining-apps/","title":"Defining Apps and Binaries","text":""},{"location":"guidelines/deprecated-components-documentation/defining-apps/#binaries","title":"Binaries","text":"<p>Each binary is defined as a <code>main</code> package in the <code>cmd/</code> directory, such as <code>cmd/iot-agent</code>. This top-level package contains only a simple <code>main</code> function (or often, one for Windows and one for *nix) which performs any platform-specific initialization and then creates and executes a Cobra command.</p>"},{"location":"guidelines/deprecated-components-documentation/defining-apps/#binary-size","title":"Binary Size","text":"<p>Consider carefully the tree of Go imports that begins with the <code>main</code> package. While the Go linker does some removal of unused symbols, the safest means to ensure a particular package isn't occuping space in the resulting binary is to not include it.</p>"},{"location":"guidelines/deprecated-components-documentation/defining-apps/#simple-binaries","title":"Simple Binaries","text":"<p>A \"simple binary\" here is one that does not have subcommands.</p> <p>The Cobra configuration for the binary is contained in the <code>command</code> subpackage of the main package (<code>cmd/&lt;binary&gt;/command</code>). The <code>main</code> function calls this package to create the command, and then executes it:</p>  cmd/&lt;binary&gt;/main.go <pre><code>func main() {\n    if err := command.MakeCommand().Execute(); err != nil {\n        os.Exit(-1)\n    }\n}\n</code></pre> <p>The <code>command.MakeCommand</code> function creates the <code>*cobra.Command</code> for the binary, with a <code>RunE</code> field that defines an app, as described below.</p>"},{"location":"guidelines/deprecated-components-documentation/defining-apps/#binaries-with-subcommands","title":"Binaries With Subcommands","text":"<p>Many binaries have a collection of subcommands, along with some command-line flags defined at the binary level. For example, the <code>agent</code> binary has subcommands like <code>agent flare</code> or <code>agent diagnose</code> and accepts global <code>--cfgfile</code> and <code>--no-color</code> arguments.</p> <p>As with simple binaries, the top-level Cobra command is defined by a <code>MakeCommand</code> function in <code>cmd/&lt;binary&gt;/command</code>. This <code>command</code> package should also define a <code>GlobalParams</code> struct and a <code>SubcommandFactory</code> type:</p>  cmd/&lt;binary&gt;/command/command.go <pre><code>// GlobalParams contains the values of agent-global Cobra flags.\n//\n// A pointer to this type is passed to SubcommandFactory's, but its contents\n// are not valid until Cobra calls the subcommand's Run or RunE function.\ntype GlobalParams struct {\n    // ConfFilePath holds the path to the folder containing the configuration\n    // file, to allow overrides from the command line\n    ConfFilePath string\n\n    // ...\n}\n\n// SubcommandFactory is a callable that will return a slice of subcommands.\ntype SubcommandFactory func(globalParams *GlobalParams) []*cobra.Command\n</code></pre> <p>Each subcommand is implemented in a subpackage of <code>cmd/&lt;binary&gt;/subcommands</code>, such as <code>cmd/&lt;binary&gt;/subcommands/version</code>. Each such subpackage contains a <code>command.go</code> defining a <code>Commands</code> function that defines the subcommands for that package:</p>  cmd/&lt;binary&gt;/subcommands/&lt;command&gt;/command.go <pre><code>func Commands(globalParams *command.GlobalParams) []*cobra.Command {\n    cmd := &amp;cobra.Command { .. }\n    return []*cobra.Command{cmd}\n}\n</code></pre> <p>While <code>Commands</code> typically returns only one command, it may make sense to return multiple commands when the implementations share substantial amounts of code, such as starting, stopping and restarting a service.</p> <p>The <code>main</code> function supplies a slice of subcommand factories to <code>command.MakeCommand</code>, which calls each one and adds the resulting subcommands to the root command.</p>  cmd/&lt;binary&gt;/main.go <pre><code>subcommandFactories := []command.SubcommandFactory{\n    frobnicate.Commands,\n    ...,\n}\nif err := command.MakeCommand(subcommandFactories).Execute(); err != nil {\n    os.Exit(-1)\n}\n</code></pre> <p>The <code>GlobalParams</code> type supports Cobra arguments that are global to all subcommands. It is passed to each subcommand factory so that the defined <code>RunE</code> callbacks can access these arguments. If the binary has no global command-line arguments, it's OK to omit this type.</p> <pre><code>func MakeCommand(subcommandFactories []SubcommandFactory) *cobra.Command {\n    globalParams := GlobalParams{}\n\n    cmd := &amp;cobra.Command{ ... }\n    cmd.PersistentFlags().StringVarP(\n        &amp;globalParams.ConfFilePath, \"cfgpath\", \"c\", \"\",\n        \"path to directory containing datadog.yaml\")\n\n    for _, sf := range subcommandFactories {\n        subcommands := sf(&amp;globalParams)\n        for _, cmd := range subcommands {\n            agentCmd.AddCommand(cmd)\n        }\n    }\n\n    return cmd\n}\n</code></pre> <p>If the available subcommands depend on build flags, move the creation of the subcommand factories to the <code>subcommands/&lt;command&gt;</code> package and create the slice there using source files with <code>//go:build</code> directives. Your factory can return <code>nil</code> if your command is not compatible with the current build flag. In all cases, the subcommands build logic should be constrained to its package. See <code>cmd/agent/subcommands/jmx/command_nojmx.go</code> for an example.</p>"},{"location":"guidelines/deprecated-components-documentation/defining-apps/#apps","title":"Apps","text":"<p>Apps map directly to <code>fx.App</code> instances, and as such they define a set of provided components and instantiate some of them.</p> <p>The <code>fx.App</code> is always created after Cobra has parsed the command-line, within a <code>cobra.Command#RunE</code> function. This means that the components supplied to an app, and any BundleParams values, are specific to the invoked command or subcommand.</p>"},{"location":"guidelines/deprecated-components-documentation/defining-apps/#one-shot-apps","title":"One-Shot Apps","text":"<p>A one-shot app is one which performs some task and exits, such as <code>agent status</code>. The <code>pkg/util/fxutil.OneShot</code> helper function provides a convenient shorthand to run a function only after all components have started. Use it like this:</p> <pre><code>cmd := cobra.Command{\n    Use: \"foo\", ...,\n    RunE: func(cmd *cobra.Command, args []string) error {\n        return fxutil.OneShot(run,\n            fx.Supply(core.BundleParams{}),\n            core.Bundle(),\n            ..., // any other bundles needed for this app\n        )\n    },\n}\n\nfunc run(log log.Component) error {\n    log.Debug(\"foo invoked!\")\n    ...\n}\n</code></pre> <p>The <code>run</code> function typically also needs some command-line values. To support this, create a (sub)command-specific <code>cliParams</code> type containing the required values, and embedding a pointer to GlobalParams:</p> <pre><code>type cliParams struct {\n    *command.GlobalParams\n    useTLS bool\n    args []string\n}\n</code></pre> <p>Populate this type within <code>Commands</code>, supply it as an Fx value, and require that value in the <code>run</code> function:</p> <pre><code>func Commands(globalParams *command.GlobalParams) []*cobra.Command {\n    cliParams := &amp;cliParams{\n        GlobalParams: globalParams,\n    }\n    var useTLS bool\n    cmd := cobra.Command{\n        Use: \"foo\", ...,\n        RunE: func(cmd *cobra.Command, args []string) error {\n            cliParams.args = args\n            return fxutil.OneShot(run,\n                fx.Supply(cliParams),\n                fx.Supply(core.CreateaBundleParams()),\n                core.Bundle(core.WithSecrets()),\n                ..., // any other bundles needed for this app\n            )\n        },\n    }\n    cmd.PersistentFlags().BoolVarP(&amp;cliParams.useTLS, \"usetls\", \"\", \"\", \"force TLS use\")\n\n    return []*cobra.Command{cmd}\n}\n\nfunc run(cliParams *cliParams, log log.Component) error {\n    if (cliParams.Verbose) {\n        log.Info(\"executing foo\")\n    }\n    ...\n}\n</code></pre> <p>This example includes cli params drawn from GlobalParams (<code>Verbose</code>), from subcommand-specific args (<code>useTLS</code>), and from Cobra (<code>args</code>).</p>"},{"location":"guidelines/deprecated-components-documentation/defining-apps/#daemon-apps","title":"Daemon Apps","text":"<p>A daemon app is one that runs \"forever\", such as <code>agent run</code>. Use the <code>fxutil.Run</code> helper function for this variety of app:</p> <pre><code>cmd := cobra.Command{\n    Use: \"foo\", ...,\n    RunE: func(cmd *cobra.Command, args []string) error {\n        return fxutil.Run(\n            fx.Supply(core.BundleParams{}),\n            core.Bundle(core.WithSecrets()),\n            ..., // any other bundles needed for this app\n            fx.Supply(foo.BundleParams{}),\n            foo.Bundle(), // the bundle implementing this app\n        )\n    },\n}\n</code></pre>"},{"location":"guidelines/deprecated-components-documentation/defining-bundles/","title":"Defining Component Bundles","text":"<p>A bundle is defined in a dedicated package named <code>comp/&lt;bundleName&gt;</code>. The package must have the following defined in <code>bundle.go</code>:</p> <ul> <li>Extensive package-level documentation. This should define:<ul> <li>The purpose of the bundle</li> <li>What components are and are not included in the bundle. Components might be omitted in the interest of binary size, as discussed in the component overview.</li> <li>Which components are automatically instantiated.</li> <li>Which other bundles this bundle depends on. Bundle dependencies are always expressed at a bundle level.</li> </ul> </li> <li>A team-name comment of the form <code>// team: &lt;teamname&gt;</code>. This is used to generate CODEOWNERS information.</li> <li>An optional <code>BundleParams</code> -- the type of the bundle's parameters (see below). This item should have a formulaic doc string like <code>// BundleParams defines the parameters for this bundle.</code></li> <li><code>Bundle</code> -- an <code>fx.Option</code> that can be included in an <code>fx.App</code> to make this bundle's components available. To assist with debugging, use <code>fxutil.Bundle(options...)</code>. Use <code>fx.Invoke(func(componentpkg.Component) {})</code> to instantiate components automatically. This item should have a formulaic doc string like <code>// Module defines the fx options for this component.</code></li> </ul> <p>Typically, a bundle will automatically instantiate the top-level components that represent the bundle's purpose. For example, the trace-agent bundle <code>comp/trace</code> might automatically instantiate <code>comp/trace/agent</code>.</p> <p>You can use the <code>dda inv components.new-bundle comp/&lt;bundleName&gt;</code> command to generate a pre-filled <code>bundle.go</code> file for the given bundle.</p>"},{"location":"guidelines/deprecated-components-documentation/defining-bundles/#bundle-parameters","title":"Bundle Parameters","text":"<p>Apps can provide some intialization-time parameters to bundles. These parameters are limited to two kinds:</p> <ul> <li>Parameters specific to the app, such as whether to start a network server; and</li> <li>Parameters from the environment, such as command-line options.</li> </ul> <p>Anything else is runtime configuration and should be handled vi <code>comp/core/config</code> or another mechanism.</p> <p>Bundle parameters must stored only <code>Params</code> types for sub components. The reason is that each sub component must be usable without <code>BundleParams</code>.</p>  comp/&lt;bundleName&gt;/bundle.go <pre><code>import \".../comp/&lt;bundleName&gt;/foo\"\nimport \".../comp/&lt;bundleName&gt;/bar\"\n// ...\n\n// BundleParams defines the parameters for this bundle.\ntype BundleParams struct {\n    Foo foo.Params\n    Bar bar.Params\n}\n\nvar Bundle = fxutil.Bundle(\n    // You must tell to fx how to get foo.Params from BundleParams.\n    fx.Provide(func(params BundleParams) foo.Params { return params.Foo }),\n    foo.Module(),\n    // You must tell to fx how to get bar.Params from BundleParams.\n    fx.Provide(func(params BundleParams) bar.Params { return params.Bar }),\n    bar.Module(),\n)\n</code></pre>"},{"location":"guidelines/deprecated-components-documentation/defining-bundles/#testing","title":"Testing","text":"<p>A bundle should have a test file, <code>bundle_test.go</code>, to verify the documentation's claim about its dependencies. This simply uses <code>fxutil.TestBundle</code> to check that all dependencies are satisfied when given the full set of required bundles.</p>  bundle_test.go <pre><code>func TestBundleDependencies(t *testing.T) {\n    fxutil.TestBundle(t, Bundle)\n}\n</code></pre>"},{"location":"guidelines/deprecated-components-documentation/purpose/","title":"Purpose of component guidelines","text":"<p>This section describes the mechanics of implementing apps, components, and bundles.</p> <p>The guidelines are quite prescriptive, with the intent of making all components \"look the same\". This reduces cognitive load when using components -- no need to remember one component's peculiarities. It also allows Agent-wide changes, where we make the same formulaic change to each component. If a situation arises that contradicts the guidelines, then we can update the guidelines (and change all affected components).</p>"},{"location":"guidelines/deprecated-components-documentation/registrations/","title":"Component Registrations","text":"<p>Components generally need to talk to one another! In simple cases, that occurs by method calls. But in many cases, a single component needs to communicate with a number of other components that all share some characteristics. For example, the <code>comp/core/health</code> component monitors the health of many other components, and <code>comp/workloadmeta/scheduler</code> provides workload events to an arbitrary number of subscribers.</p> <p>The convention in the Agent codebase is to use value groups to accomplish this. The collecting component requires a slice of some collected type, and the providing components provide values of that type. Consider an example case of an HTTP server component to which endpoints can be attached. The server is the collecting component, requiring a slice of type <code>[]*endpoint</code>, where <code>*endpoint</code> is the collected type. Providing components provide values of type <code>*endpoint</code>.</p> <p>The convention is to \"wrap\" the collected type in a <code>Registration</code> struct type which embeds <code>fx.Out</code> and has tag <code>group:\"pkgname\"</code>, where <code>pkgname</code> is the short package name (Fx requires a group name, and this is as good as any). This helps providing components avoid the common mistake of omitting the tag. Because it is wrapped in an exported <code>Registration</code> type, the collected type can be an unexported type, as in the example below.</p> <p>The collecting component should define the registration type and a constructor for it:</p>  comp/server/component.go <pre><code>// ...\n// Server endpoints are provided by other components, by providing a server.Registration\n// instance.\n// ...\npackage server\n\ntype endpoint struct {  // (the collected type)\n    ...\n}\n\ntype Registration struct {\n    fx.Out\n\n    Endpoint endpoint `group:\"server\"`\n}\n\n// NewRegistration creates a new Registration instance for the given endpoint.\nfunc NewRegistration(route string, handler func()) Registration { ... }\n</code></pre> <p>Its implementation then requires a slice of the collected type (<code>endpoint</code>), again using <code>group:\"server\"</code>:</p>  comp/server/server.go <pre><code>// endpoint defines an endpoint on this server.\ntype endpoint struct { ... }\n\ntype dependencies struct {\n    fx.In\n\n    Registrations []endpoint `group:\"server\"`\n}\n\nfunc newServer(deps dependencies) Component {\n    // ...\n    for _, e := range deps.Registrations {\n        if e.handler == nil {\n            continue\n        }\n        // ...\n    }\n    // ...\n}\n</code></pre> <p>It's good practice to ignore zero values, as that allows providing components to skip the registration if desired.</p> <p>Finally, the providing component (in this case, <code>foo</code>) includes a registration in its output as an additional provided type, beyond its <code>Component</code> type:</p>  comp/foo/foo.go <pre><code>func newFoo(deps dependencies) (Component, server.Registration) {\n    // ...\n    return foo, server.NewRegistration(\"/things/foo\", foo.handler)\n}\n</code></pre> <p>This technique has some caveats to be aware of:</p> <ul> <li>The providing components are instantiated before the collecting component.</li> <li>Fx treats value groups as the collecting component depending on all of the providing components. This means that the providing components cannot depend on the collecting component, as this would represent a dependency cycle.</li> <li>Fx will instantiate all declared providing components before the collecting component, regardless of whether their <code>Component</code> type is required. This may lead to components being instantiated in unexpected circumstances.</li> </ul>"},{"location":"guidelines/deprecated-components-documentation/subscriptions/","title":"Component Subscriptions","text":"<p>Subscriptions are a common form of registration, and have support in the <code>pkg/util/subscriptions</code> package.</p> <p>In defining subscriptions, the component that transmits messages is the collecting component, and the processes receiving components are the providing components. These are matched using the message type, which must be unique across the codebase, and should not be a built-in type like <code>string</code>. Providing components provide a <code>subscriptions.Receiver[coll.Message]</code> which has a <code>Ch</code> channel from which to receive messages. Collecting components require a <code>subscriptions.Transmitter[coll.Message]</code> which has a <code>Notify</code> method to send messages.</p>  announcer/component.go announcer/announcer.go listener/listener.go <pre><code>// ...\n// To subscribe to these announcements, provide a subscriptions.Subscription[announcer.Announcement].\n// ...\npackage announcer\n</code></pre> <pre><code>func newAnnouncer(tx subscriptions.Transmitter[Anouncement]) Component {\n    return &amp;announcer{announcementTx: tx}  // (store the transmitter)\n}\n\n// ... later send messages with\nfunc (ann *announcer) announce(a announcement) {\n    ann.annoucementTx.Notify(a)\n}\n</code></pre> <pre><code>func newListener() (Component, subscriptions.Receiver[announcer.Announcement]) {\n    rx := subscriptions.Receiver[Event]() // create a receiver\n    return &amp;listener{announcementRx: rx}, rx  // capture the receiver _and_ return it\n}\n\n// ... later receive messages (usually in an actor's main loop)\nfunc (l *listener) run() {\n    loop {\n        select {\n        case a := &lt;- l.announcementRx.Ch:\n            ...\n        }\n    }\n}\n</code></pre> <p>Any component receiving messages via a subscription will automatically be instantiated by Fx if it is delcared in the app, regardless of whether its Component type is required by some other component. The workaround for this is to return a zero-valued Receiver when the component does not actually wish to receive messages (such as when the component is disabled by user configuration).</p> <p>If a receiving component does not subscribe (for example, if it is not started), it can return the zero value, <code>subscriptions.Receiver[Event]{}</code>, from its constructor. If a component returns a non-nil subscriber, it must consume messages from the receiver or risk blocking the transmitter.</p> <p>See the <code>pkg/util/subscriptions</code> documentation for more details.</p>"},{"location":"guidelines/deprecated-components-documentation/using-components/","title":"Using Components and Bundles","text":""},{"location":"guidelines/deprecated-components-documentation/using-components/#component-dependencies","title":"Component Dependencies","text":"<p>Component dependencies are automatically determined from the arguments to a component constructor. Most components have a few dependencies, and use a struct named <code>dependencies</code> to represent them:</p> <pre><code>type dependencies struct {\n    fx.In\n\n    Lc fx.Lifecycle\n    Params internal.BundleParams\n    Config config.Module\n    Log log.Module\n    // ...\n}\n\nfunc newThing(deps dependencies) Component {\n    t := &amp;thing{\n        log: deps.Log,\n        ...\n    }\n    deps.Lc.Append(fx.Hook{OnStart: t.start})\n    return t\n}\n</code></pre>"},{"location":"guidelines/deprecated-components-documentation/using-components/#testing","title":"Testing","text":"<p>Testing for a component should use <code>fxtest</code> to create the component. This focuses testing on the API surface of the component against which other components will be built. Per-function unit tests are, of course, also great where appropriate!</p> <p>Here's an example testing a component with a mocked dependency on <code>other</code>:</p> <pre><code>func TestMyComponent(t *testing.T) {\n    var comp Component\n    var other other.Component\n    app := fxtest.New(t,\n        Module,              // use the real version of this component\n        other.MockModule(),  // use the mock version of other\n        fx.Populate(&amp;comp),  // get the instance of this component\n        fx.Populate(&amp;other), // get the (mock) instance of the other component\n    )\n\n    // start and, at completion of the test, stop the components\n    defer app.RequireStart().RequireStop()\n\n    // cast `other` to its mock interface to call mock-specific methods on it\n    other.(other.Mock).SetSomeValue(10)                      // Arrange\n    comp.DoTheThing()                                        // Act\n    require.Equal(t, 20, other.(other.Mock).GetSomeResult()) // Assert\n}\n</code></pre> <p>If the component has a mock implementation, it is a good idea to test that mock implementation as well.</p>"},{"location":"guidelines/languages/RUST/","title":"Rust in the Datadog Agent","text":"<p>This document describes how Rust components are built and integrated in the Datadog Agent repository.</p>"},{"location":"guidelines/languages/RUST/#overview","title":"Overview","text":"<p>The Datadog Agent uses rules_rust for building Rust code with Bazel and rules_rs to manage Cargo crates. This enables seamless integration with the existing Go and Python codebase, consistent toolchain management, and reproducible builds across the repository.</p> <p>Important: We strongly encourage using Bazel directly for all Rust operations (building, testing, except Cargo.toml management) rather than Cargo. While Cargo may work for some local development tasks, Bazel is the source of truth for builds and ensures consistency with CI. All instructions in this document use Bazel commands.</p>"},{"location":"guidelines/languages/RUST/#toolchain-configuration","title":"Toolchain Configuration","text":""},{"location":"guidelines/languages/RUST/#bazel-module-configuration","title":"Bazel Module Configuration","text":"<p>The Rust toolchain is configured in MODULE.bazel:</p> <pre><code>bazel_dep(name = \"rules_rust\", version = \"0.68.1\")\n\nrust = use_extension(\"@rules_rust//rust:extensions.bzl\", \"rust\")\nrust.toolchain(\n    edition = \"2024\",\n    versions = [\"1.92.0\"],\n)\nuse_repo(rust, \"rust_toolchains\")\n\nregister_toolchains(\"@rust_toolchains//:all\")\n</code></pre> <p>This configuration: - Uses Rust 2024 edition as the default - Pins to Rust 1.92.0 for reproducible builds - Registers toolchains for all supported platforms</p> <p>Important: This is a global toolchain configuration that is used across the entire codebase of <code>datadog-agent</code>. The configuration in MODULE.bazel should not be changed without proper testing to ensure that all <code>rust</code> components are still working.</p>"},{"location":"guidelines/languages/RUST/#crate-management","title":"Crate Management","text":"<p>All external Rust crates are managed centrally through a single Cargo workspace defined in the root Cargo.toml. Individual components must not declare their own dependency versions \u2014 all versions live in the root <code>[workspace.dependencies]</code> section, and component <code>Cargo.toml</code> files reference them with <code>.workspace = true</code>.</p> <p>Important: Do not add crate versions directly in a component's <code>Cargo.toml</code>. Every external dependency must be declared in the root Cargo.toml under <code>[workspace.dependencies]</code>. This ensures consistent versions across all Rust components, a single <code>Cargo.lock</code>, and a single source of truth for Bazel crate resolution.</p>"},{"location":"guidelines/languages/RUST/#how-it-works","title":"How It Works","text":"<p>The root Cargo.toml defines three things:</p> <ol> <li><code>[workspace]</code> \u2014 lists all Rust component directories as <code>members</code></li> <li><code>[workspace.dependencies]</code> \u2014 the single place where all external crate versions are pinned</li> <li><code>[workspace.package]</code> \u2014 shared metadata (<code>edition</code>, <code>license</code>, <code>rust-version</code>) inherited by all members</li> </ol> <p>A component's <code>Cargo.toml</code> then references workspace dependencies rather than specifying versions:</p> <pre><code># Component Cargo.toml \u2014 NO version numbers here\n[package]\nname = \"my_component\"\nversion = \"0.1.0\"\nedition.workspace = true\nlicense.workspace = true\nrust-version.workspace = true\n\n[dependencies]\nanyhow.workspace = true\nserde.workspace = true\n# When a component needs specific features, add them on top of the workspace version:\ntokio = { workspace = true, features = [\"macros\", \"rt-multi-thread\", \"signal\"] }\n</code></pre> <p>This produces a single <code>Cargo.lock</code> at the repository root \u2014 all components share the same resolved dependency graph.</p>"},{"location":"guidelines/languages/RUST/#bazel-integration","title":"Bazel Integration","text":"<p>The workspace is registered once in deps/crates.MODULE.bazel, pointing to the root <code>Cargo.toml</code> and <code>Cargo.lock</code>:</p> <pre><code>crate = use_extension(\"@rules_rs//rs:extensions.bzl\", \"crate\")\ncrate.from_cargo(\n    name = \"crates\",\n    cargo_lock = \"//:Cargo.lock\",\n    cargo_toml = \"//:Cargo.toml\",\n    platform_triples = [\n        \"aarch64-unknown-linux-gnu\",\n        \"x86_64-unknown-linux-gnu\",\n    ],\n    validate_lockfile = True,\n)\nuse_repo(crate, \"crates\")\n</code></pre> <p>All components reference crates from this single repository: <code>@crates//:&lt;crate_name&gt;</code>. There is intentionally only one <code>crate.from_cargo</code> entry \u2014 do not add per-component entries.</p>"},{"location":"guidelines/languages/RUST/#adding-dependencies-to-an-existing-component","title":"Adding Dependencies to an Existing Component","text":"<ol> <li> <p>Add the dependency version to the root Cargo.toml under <code>[workspace.dependencies]</code> (skip if the crate is already listed):    <pre><code>[workspace.dependencies]\nserde = { version = \"1.0\", features = [\"derive\"] }\n</code></pre></p> </li> <li> <p>Reference it in your component's <code>Cargo.toml</code> using <code>.workspace = true</code> (never a version number):    <pre><code>[dependencies]\nserde.workspace = true\n</code></pre></p> </li> <li> <p>Add the dependency to your <code>BUILD.bazel</code>: <pre><code>rust_library(\n    name = \"my_lib\",\n    # ...\n    deps = [\n        \"@crates//:serde\",\n    ],\n)\n</code></pre></p> </li> <li> <p>Regenerate <code>Cargo.lock</code>: <pre><code>cargo generate-lockfile\n</code></pre></p> </li> <li> <p>Commit both the root <code>Cargo.toml</code> and <code>Cargo.lock</code></p> </li> </ol>"},{"location":"guidelines/languages/RUST/#adding-a-new-rust-component","title":"Adding a New Rust Component","text":"<p>Follow these steps to add a new Rust component to the repository.</p>"},{"location":"guidelines/languages/RUST/#step-1-create-the-directory-structure","title":"Step 1: Create the Directory Structure","text":"<pre><code>&lt;path_to_your_component&gt;\n\u251c\u2500\u2500 BUILD.bazel\n\u251c\u2500\u2500 Cargo.toml\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 lib.rs\n\u2502   \u2514\u2500\u2500 main.rs  # if building a binary\n\u2514\u2500\u2500 tests/       # optional integration tests\n</code></pre> <p>Note: The component directory does not contain a <code>Cargo.lock</code> \u2014 the single lock file lives at the repository root.</p>"},{"location":"guidelines/languages/RUST/#step-2-add-your-component-to-the-cargo-workspace","title":"Step 2: Add Your Component to the Cargo Workspace","text":"<p>Edit the root Cargo.toml:</p> <ol> <li> <p>Register your component as a workspace member: <pre><code>[workspace]\nmembers = [\n    \"pkg/discovery/module/rust\",\n    \"pkg/procmgr/rust\",\n    \"pkg/your/component/rust\",  # Add your component here\n]\n</code></pre></p> </li> <li> <p>Add any new crate versions to <code>[workspace.dependencies]</code> (all external crate versions must be declared here):    <pre><code>[workspace.dependencies]\n# ... existing deps ...\nmy_new_dep = \"1.0\"\n</code></pre></p> </li> </ol>"},{"location":"guidelines/languages/RUST/#step-3-create-your-components-cargotoml","title":"Step 3: Create Your Component's <code>Cargo.toml</code>","text":"<p>The component <code>Cargo.toml</code> must not contain any dependency version numbers. Use <code>.workspace = true</code> to inherit versions from the root:</p> <pre><code>[package]\nname = \"my_component\"\nversion = \"0.1.0\"\nedition.workspace = true\nlicense.workspace = true\nrust-version.workspace = true\n\n[lib]\nname = \"my_component\"\ncrate-type = [\"rlib\"]  # Add \"cdylib\" if you need FFI\n\n[[bin]]\nname = \"my_binary\"\npath = \"src/main.rs\"\n\n[dependencies]\nanyhow.workspace = true\nserde.workspace = true\n# When you need specific features on top of the workspace-declared version:\ntokio = { workspace = true, features = [\"macros\", \"rt-multi-thread\"] }\n\n[dev-dependencies]\ntempfile.workspace = true\n</code></pre> <p>Do not add <code>version = \"...\"</code> to dependencies in component <code>Cargo.toml</code> files. If the crate you need is not yet in the root <code>[workspace.dependencies]</code>, add it there first.</p>"},{"location":"guidelines/languages/RUST/#step-4-regenerate-the-lock-file","title":"Step 4: Regenerate the Lock File","text":"<pre><code>cargo generate-lockfile\n</code></pre> <p>Note: You must run <code>cargo generate-lockfile</code> (or <code>cargo build</code>) whenever you change any <code>Cargo.toml</code>. If <code>Cargo.lock</code> is out of sync, Bazel will report an error: <pre><code>ERROR: Cargo.lock out of sync: sd-agent requires clap ^4.5.58 but Cargo.lock has 4.5.51.\n</code></pre></p>"},{"location":"guidelines/languages/RUST/#step-5-create-buildbazel","title":"Step 5: Create BUILD.bazel","text":"<p>All components share the single <code>@crates</code> repository for external dependencies:</p> <pre><code>load(\"@rules_rust//rust:defs.bzl\", \"rust_binary\", \"rust_library\", \"rust_test\")\n\nrust_library(\n    name = \"my_component\",\n    srcs = glob([\"src/**/*.rs\"], exclude = [\"src/main.rs\"]),\n    crate_name = \"my_component\",\n    edition = \"2024\",\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \"@crates//:anyhow\",\n        \"@crates//:serde\",\n    ],\n)\n\nrust_binary(\n    name = \"my_binary\",\n    srcs = [\"src/main.rs\"],\n    edition = \"2024\",\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":my_component\",\n        \"@crates//:anyhow\",\n    ],\n)\n\nrust_test(\n    name = \"my_component_test\",\n    crate = \":my_component\",\n    edition = \"2024\",\n    deps = [\n        \"@crates//:tempfile\",\n    ],\n)\n</code></pre>"},{"location":"guidelines/languages/RUST/#step-6-build-and-test","title":"Step 6: Build and Test","text":"<pre><code># Build\nbazel build //pkg/your/component/rust:my_component\nbazel build //pkg/your/component/rust:my_binary\n\n# Test\nbazel test //pkg/your/component/rust:my_component_test\n</code></pre>"},{"location":"guidelines/languages/RUST/#build-target-types","title":"Build Target Types","text":""},{"location":"guidelines/languages/RUST/#rust_library","title":"rust_library","text":"<p>For Rust libraries (produces <code>.rlib</code>):</p> <pre><code>rust_library(\n    name = \"my_lib\",\n    srcs = glob([\"src/**/*.rs\"]),\n    crate_name = \"my_lib\",\n    edition = \"2024\",\n    deps = [\"@crates//:serde\"],\n)\n</code></pre>"},{"location":"guidelines/languages/RUST/#rust_shared_library","title":"rust_shared_library","text":"<p>For C-compatible shared libraries (produces <code>.so</code>/<code>.dylib</code>), useful for FFI with Go via cgo:</p> <pre><code>rust_shared_library(\n    name = \"libmy_lib\",\n    srcs = glob([\"src/**/*.rs\"]),\n    crate_name = \"my_lib\",\n    crate_root = \"src/lib.rs\",\n    edition = \"2024\",\n    deps = [\"@crates//:serde\"],\n)\n</code></pre>"},{"location":"guidelines/languages/RUST/#rust_binary","title":"rust_binary","text":"<p>For executable binaries:</p> <pre><code>rust_binary(\n    name = \"my_tool\",\n    srcs = [\"src/main.rs\"],\n    edition = \"2024\",\n    deps = [\":my_lib\"],\n)\n</code></pre>"},{"location":"guidelines/languages/RUST/#rust_test","title":"rust_test","text":"<p>For unit and integration tests:</p> <pre><code># Unit tests (embedded in library)\nrust_test(\n    name = \"my_lib_test\",\n    crate = \":my_lib\",\n    edition = \"2024\",\n    deps = [\"@crates//:tempfile\"],  # dev-dependencies\n)\n\n# Integration tests (standalone test files)\nrust_test(\n    name = \"integration_test\",\n    srcs = [\"tests/integration_test.rs\"],\n    edition = \"2024\",\n    data = [\":my_tool\"],  # Binary needed at runtime\n    rustc_env = {\n        \"CARGO_BIN_EXE_my_tool\": \"$(rootpath :my_tool)\",\n    },\n    deps = [\n        \"@crates//:tempfile\",\n    ],\n)\n</code></pre>"},{"location":"guidelines/languages/RUST/#platform-restrictions","title":"Platform Restrictions","text":"<p>To restrict targets to specific platforms, use <code>target_compatible_with</code>:</p> <pre><code>rust_library(\n    name = \"linux_only_lib\",\n    # ...\n    target_compatible_with = [\n        \"@platforms//os:linux\",\n    ],\n)\n</code></pre>"},{"location":"guidelines/languages/RUST/#release-builds","title":"Release Builds","text":"<p>For optimized release builds with size optimization, use the <code>sd-agent-release</code> config:</p> <pre><code>bazel build --config=sd-agent-release //pkg/your/component/rust:my_binary\n</code></pre> <p>This enables: - Fat LTO (Link-Time Optimization) - Size optimization (<code>opt-level=z</code>) - Symbol stripping - Panic abort (no stack unwinding)</p> <p>For custom release profiles, add to <code>bazel/configs/</code> and import in <code>.bazelrc</code>.</p> <p>Note: right now we only have a release configuration that is sd-agent specific. However, if we identify that future components want to utilize the same configuration it can be promoted to the global <code>datadog-agent-release</code> configuration. For now, please, introduce your own <code>my_component.bazelrc</code> in bazel/configs/ and add <code>import %workspace%/bazel/configs/my_component.bazelrc</code> to .bazelrc under <code>Project configs</code> section.</p>"},{"location":"guidelines/languages/RUST/#ci-integration","title":"CI Integration","text":"<p>TODO: Describe how to add rust build to CI.</p> <p>In CI (with <code>--config=ci</code>), Rust builds automatically run:</p> <ul> <li>Clippy checks via <code>rust_clippy_aspect</code></li> <li>Rustfmt checks via <code>rustfmt_aspect</code></li> </ul> <p>Configuration from <code>.bazelrc</code>: <pre><code>build:ci --aspects=@rules_rust//rust:defs.bzl%rust_clippy_aspect\nbuild:ci --output_groups=+clippy_checks\nbuild:ci --aspects=@rules_rust//rust:defs.bzl%rustfmt_aspect\nbuild:ci --output_groups=+rustfmt_checks\n</code></pre></p> <p>Note: you can also enable these checks for your local development (for instance, to format code automatically). To do so create <code>user.bazelrc</code> file in the root of the project and just add the same flags without configuration name. This enforces unconditional usage of the flags for arbitrary <code>build</code> invocation: <pre><code>build --aspects=@rules_rust//rust:defs.bzl%rust_clippy_aspect\nbuild --output_groups=+clippy_checks\nbuild --aspects=@rules_rust//rust:defs.bzl%rustfmt_aspect\nbuild --output_groups=+rustfmt_checks\n</code></pre></p>"},{"location":"guidelines/languages/RUST/#local-development-tips","title":"Local Development Tips","text":""},{"location":"guidelines/languages/RUST/#common-bazel-commands","title":"Common Bazel Commands","text":"<pre><code># Build a target\nbazel build //pkg/your/component/rust:my_component\n\n# Run tests\nbazel test //pkg/your/component/rust:my_component_test\n\n# Build with verbose output\nbazel build --verbose_failures //pkg/your/component/rust:...\n\n# Query dependencies\nbazel query \"deps(//pkg/your/component/rust:my_lib)\"\n\n# Check crate resolution\nbazel query \"@crates//...\"\n</code></pre>"},{"location":"guidelines/languages/RUST/#updating-dependencies","title":"Updating Dependencies","text":"<p>After modifying any <code>Cargo.toml</code>, regenerate the lock file from the repository root:</p> <pre><code># Alternatively, you can use cargo build command to do the same\ncargo generate-lockfile\n</code></pre> <p>Bazel will fail if <code>Cargo.toml</code> and <code>Cargo.lock</code> are out of sync: <pre><code>ERROR: Cargo.lock out of sync: sd-agent requires clap ^4.5.58 but Cargo.lock has 4.5.51.\n</code></pre></p>"},{"location":"guidelines/languages/RUST/#further-reading","title":"Further Reading","text":"<ul> <li>rules_rust documentation - Rust toolchain and build rules</li> <li>rules_rs documentation - Crate management</li> </ul>"},{"location":"guidelines/languages/go/","title":"Go guidelines","text":""},{"location":"guidelines/languages/go/#imports","title":"Imports","text":"<p>The imports defined in the <code>imports ( ... )</code> block of each Go file should be separated into the following sections, in order.</p> <ol> <li>Standard library packages (e.g. <code>fmt</code>, <code>net/http</code>)</li> <li>External packages (e.g. <code>github.com/stretchr/testify/assert</code>, <code>github.com/DataDog/datadog-agent/pkg/util/log</code>)</li> <li>Internal packages (e.g. <code>github.com/DataDog/datadog-agent/&lt;parent&gt;/internal</code>)</li> </ol> <p>This is not verified by our static analysis during CI. Instead, we suggest configuring your editor to keep imports properly sorted.</p> Editor setup <p>The goimports tool supports a \"local packages\" section. Use the flag <code>-local github.com/DataDog/datadog-agent</code>.</p> VS Code / CursorVim <p>See the wiki for more details.</p> <pre><code>{\n  \"gopls\": {\n    \"formatting.local\": \"github.com/DataDog/datadog-agent\"\n  }\n}\n</code></pre> <p>Configure vim-go as follows.</p> <pre><code>let g:go_fmt_options = {\n\\ 'goimports': '-local github.com/DataDog/datadog-agent',\n\\ }\n</code></pre>"},{"location":"guidelines/languages/go/#public-apis","title":"Public APIs","text":"<p>Go supports the use of private <code>internal</code> packages to control the public API of a Go module. This prevents packages from being used outside of the parent module, which is useful for decoupling different parts of our codebase.</p> <p>When adding new code, carefully consider what API is exported by both taking care of what symbols are uppercase and by making judicious use of <code>internal</code> directories.</p> <ol> <li>When in doubt, prefer hiding public APIs as it's much easier to refactor code to expose something that was private than doing it the other way around.</li> <li>When refactoring a struct into its own package, try moving it into its own <code>internal</code> directory if possible (see example below).</li> <li>If the directory you are editing already has <code>internal</code> directories, try to move code to these <code>internal</code> directories instead of creating new ones.</li> <li>When creating new <code>internal</code> directories, try to use the deepest possible <code>internal</code> directory to limit the packages that can import yours. For example, if making <code>a/b/c/d</code> internal, consider moving it to <code>a/b/c/internal/d</code> instead of <code>a/internal/b/c/d</code>. With the first path, code from <code>a/b</code> won't be able to access the <code>d</code> package, while it could with the second path.</li> </ol> Example <p>Sometimes one wants to hide private fields of a struct from other code in the same package to enforce a particular code invariant. In this case, the struct should be moved to a different folder within the same package making this folder <code>internal</code>.</p> <p>Consider a module named <code>example</code> where you want to move <code>exampleStruct</code> from the <code>a</code> package to a subfolder to hide its private fields from <code>a</code>'s code.</p> <p>Before the refactor, the code will look like this:</p>  example/a/code.go <pre><code>package a\n\ntype exampleStruct struct {\n    // Public\n    Foo string\n    // Private\n    bar string\n}\n\nfunc doSomethingWithExampleStruct(e exampleStruct) {\n    // some code goes here ...\n}\n</code></pre> <p>After the refactor, you should move <code>exampleStruct</code> to an <code>a/internal/b</code> directory:</p>  example/a/internal/b/examplestruct.go <pre><code>package b\n\ntype ExampleStruct struct {\n    // Public\n    Foo string\n    // Private\n    bar string\n}\n</code></pre> <p>and import this package from <code>a/code.go</code>:</p>  example/a/code.go <pre><code>package a\n\nimport (\n    \"example/a/internal/b\"\n)\n\nfunc doSomethingWithExampleStruct(e b.ExampleStruct) {\n    // some code goes here\n}\n</code></pre> <p>In this way, no new public API is exposed on the <code>a</code> folder: <code>ExampleStruct</code> remains private to <code>a</code>, while we have improved encapsulation as we wanted.</p>"},{"location":"guidelines/languages/go/#atomics","title":"Atomics","text":"Avoid atomics! <p>Atomics are a very low-level concept and full of subtle gotchas and dramatic performance differences across platforms. If you can use something higher-level, such as something from the standard library's <code>sync</code> package, prefer to do so. Otherwise, if you are in search of performance, be sure to benchmark your work carefully and on multiple platforms.</p> <p>It's tempting to use an atomic to avoid complaints from the race detector, but this is almost always a mistake -- it is merely hiding your race from the race detector. Consider carefully why the implementation is racing, and try to address that behavior directly.</p> <p>The exception to this is tests, where atomics can be useful for sensing some value that you would like to assert on that is manipulated in another goroutine. Even here, be wary of race conditions, such as assuming that a background goroutine has executed before the test goroutine makes its assertions.</p> <p>Always ensure that you:</p> <ol> <li> <p>Use <code>go.uber.org/atomic</code> instead of <code>sync/atomic</code>.</p> Why? <p>There are two main issues with the standard library's <code>sync/atomic</code> package.</p> <ol> <li>It has an alignment bug requiring users to manually ensure alignment. This is frequently forgotten, and only causes issues on less-common platforms, leading to undetected bugs.</li> <li>It is very easy to access a raw integer variable using a mix of atomic and non-atomic operations. This mix may be enough to satisfy the race detector, but not sufficient to actually prevent undefined behavior.</li> </ol> </li> <li> <p>Declare atomic types using a pointer to ensure proper alignment.</p> </li> </ol> <pre><code>// global variable\nvar maxFooCount = atomic.NewUint64(42)\n\n// in a struct\ntype FooTracker struct {\n    maxCount *atomic.Uint64\n}\n\nfunc NewFooTracker() *FooTracker {\n    return &amp;FooTracker {\n        maxCount: atomic.NewUint64(42),\n    }\n}\n</code></pre> <p>Use the <code>atomic.Uint64</code> methods to perform atomic operations on the value. These include some conveniences not available in <code>sync/atomic</code>, such as <code>Inc</code>/<code>Dec</code> and <code>atomic.Bool</code>.</p> <p>If the additional pointer allocation poses an undue performance burden, do both of the following.</p> <ol> <li>Include the value as the first element in the struct (to ensure alignment).</li> <li>Add a comment indicating that it must remain in that position and why a pointer was not suitable.</li> </ol> <p>Pointers to atomic types marshal correctly to JSON as their enclosed value. Unmarshaling does the reverse, except that missing values are represented as <code>nil</code>, rather than an atomic type with zero value.</p> <p>Types such as <code>expvar.Int</code> are simple wrappers around an integer, and are accessed using <code>sync/atomic</code>. Go will properly align variables (whether global or local) but not struct fields, so any expvar types embedded in a struct must use a pointer.</p> <ul> <li> <p> Good</p> <pre><code>type Example struct {\n    field *expvar.Int{}\n}\n</code></pre> </li> <li> <p> Bad</p> <pre><code>type Example struct {\n    field expvar.Int{}\n}\n</code></pre> </li> </ul>"},{"location":"guidelines/languages/go/#testing","title":"Testing","text":""},{"location":"guidelines/languages/go/#failing-fast","title":"Failing fast","text":"<p>The functions in <code>github.com/stretchr/testify/require</code> automatically abort the test when an assertion fails, whereas <code>github.com/stretchr/testify/assert</code> does not.</p> <p>For example, given an error, <code>assert.NoError(t, err)</code> causes the test to be marked as a failure, but continues to the next statement, possibly leading to a <code>nil</code> dereference or other such failure. In contrast, <code>require.NoError(t, err)</code> aborts the test when an error is encountered.</p> <p>Where a test makes a sequence of independent assertions, <code>assert</code> is a good choice. When each assertion depends on the previous having been successful, use <code>require</code>.</p>"},{"location":"guidelines/languages/go/#time","title":"Time","text":"<p>Tests based on time are a major source of flakes. If you find yourself thinking something like \"the ticker should run three times in 500ms\", you will be disappointed at how often that is not true in CI. Even if that test is not flaky, it will take at least 500ms to run. Summing such delays over thousands of tests means very long test runs and slower work for everyone.</p> <p>When the code you are testing requires time, the first strategy is to remove that requirement. For example, if you are testing the functionality of a poller, factor the code such that the tests can call the <code>poll()</code> method directly, instead of waiting for a Ticker to do so.</p> <p>Where this is not possible, refactor the code to use a Clock from <code>github.com/benbjohnson/clock</code>. In production, create a <code>clock.Clock</code>, and in tests, inject a <code>clock.Mock</code>. When time should pass in your test execution, call <code>clock.Add(..)</code> to deterministically advance the clock.</p> <p>A common pattern for objects that embed a timer is as follows:</p> <pre><code>func NewThing(arg1, arg2) *Thing {\n    return newThingWithClock(arg1, arg2, clock.New())\n}\n\nfunc newThingWithClock(arg1, arg2, clock clock.Clock) *Thing {\n    return &amp;Thing{\n        ...,\n        clock: clock,\n    }\n}\n\nfunc TestThingFunctionality(t *testing.T) {\n    clk := clock.NewMock()\n    thing := newThingWithClock(..., clk)\n\n    // ...\n\n    clk.Add(100 * time.Millisecond)\n\n    // ...\n}\n</code></pre>"},{"location":"guidelines/languages/go/#logging","title":"Logging","text":"<p>Logging utilizes the <code>log/slog</code> package as its underlying framework. You can access logging through <code>pkg/util/log</code> and the <code>comp/core/log</code> component wrappers. Using the component wrapper is recommended, as it adheres to component best practices.</p>"},{"location":"guidelines/testing/test-categories/","title":"Test Categories","text":"<p>The Datadog Agent employs a comprehensive testing strategy with four distinct categories of tests, each serving a specific purpose in ensuring code quality, functionality, and reliability. This document serves as both a reference guide and decision-making framework for choosing the appropriate test type.</p>"},{"location":"guidelines/testing/test-categories/#unit-tests","title":"Unit Tests","text":"<p>Purpose: Validate individual functions, methods, or small code units in isolation.</p> <p>When to Use Unit Tests:</p> <ul> <li>Testing pure functions with predictable inputs/outputs.</li> <li>Validating business logic without external dependencies.</li> <li>Testing error handling and edge cases.</li> <li>Verifying data transformations and calculations.</li> <li>When you can mock all external dependencies effectively.</li> </ul> <p>Criteria:</p> <ul> <li>Speed: Must execute in milliseconds (&lt; 100ms per test).</li> <li>Isolation: No network calls, file system access, or external services.</li> <li>Deterministic: Same input always produces same output.</li> <li>Independent: Can run in any order without affecting other tests.</li> </ul> <p>Examples: <pre><code>// Testing metric aggregation logic\nfunc TestMetricAggregator_Sum(t *testing.T) {\n    aggregator := NewMetricAggregator()\n    aggregator.Add(\"cpu.usage\", 10.0)\n    aggregator.Add(\"cpu.usage\", 15.0)\n    assert.Equal(t, 25.0, aggregator.Sum(\"cpu.usage\"))\n}\n\n// Testing configuration parsing\nfunc TestConfig_ParseYAML(t *testing.T) {\n    yamlData := `api_key: test123`\n    config, err := ParseYAML([]byte(yamlData))\n    assert.NoError(t, err)\n    assert.Equal(t, \"test123\", config.APIKey)\n}\n</code></pre></p> <p>Common Patterns:</p> <ul> <li>Mock external dependencies using interfaces.</li> <li>Test both happy path and error conditions.</li> <li>Use table-driven tests for multiple input scenarios.</li> <li>Focus on business logic rather than implementation details.</li> </ul>"},{"location":"guidelines/testing/test-categories/#integration-tests","title":"Integration Tests","text":"<p>Purpose: Test the interaction between multiple components or validate functionality that requires external services.</p> <p>When to Use Integration Tests:</p> <ul> <li>Testing database interactions and queries.</li> <li>Validating API client implementations.</li> <li>Testing component interactions within the same service.</li> <li>Verifying configuration loading from actual files.</li> <li>When external services are required but controllable.</li> </ul> <p>Criteria:</p> <ul> <li>Moderate Speed: Should complete within seconds (&lt; 30s per test).</li> <li>Controlled Dependencies: Use real services but in test environments.</li> <li>CI Compatible: Must work in both local and CI environments.</li> <li>Platform Aware: Should work on all supported platforms or skip gracefully.</li> <li>Cleanup: Must clean up resources after execution.</li> </ul> <p>Examples: <pre><code>// Testing database integration\nfunc TestMetricStore_SaveAndRetrieve(t *testing.T) {\n    db := setupTestDB(t) // Creates isolated test database\n    defer cleanupTestDB(t, db)\n\n    store := NewMetricStore(db)\n    metric := &amp;Metric{Name: \"test.metric\", Value: 42.0}\n\n    err := store.Save(metric)\n    assert.NoError(t, err)\n\n    retrieved, err := store.Get(\"test.metric\")\n    assert.NoError(t, err)\n    assert.Equal(t, metric.Value, retrieved.Value)\n}\n\n// Testing HTTP client integration\nfunc TestDatadogClient_SubmitMetrics(t *testing.T) {\n    server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        assert.Equal(t, \"POST\", r.Method)\n        assert.Contains(t, r.Header.Get(\"Content-Type\"), \"application/json\")\n        w.WriteHeader(http.StatusOK)\n    }))\n    defer server.Close()\n\n    client := NewDatadogClient(server.URL, \"test-key\")\n    err := client.SubmitMetrics([]Metric{{Name: \"test\", Value: 1.0}})\n    assert.NoError(t, err)\n}\n</code></pre></p> <p>Requirements:</p> <ul> <li>Use test containers or in-memory databases when possible.</li> <li>Implement proper setup/teardown procedures.</li> <li>Handle network timeouts and retries gracefully.</li> <li>Skip tests when required services are unavailable.</li> </ul>"},{"location":"guidelines/testing/test-categories/#system-tests","title":"System Tests","text":"<p>Purpose: Validate how multiple Agent components work together as a cohesive system, testing inter-component communication and workflows.</p> <p>When to Use System Tests:</p> <ul> <li>Testing complete data pipelines (collection \u2192 processing \u2192 forwarding).</li> <li>Validating component startup and shutdown sequences.</li> <li>Testing configuration changes and reloads.</li> <li>Verifying metric collection from actual system resources.</li> <li>Testing cross-component communication (e.g., DogStatsD \u2192 Forwarder).</li> </ul> <p>Criteria:</p> <ul> <li>Realistic Environment: Uses actual Agent binaries and configurations.</li> <li>Component Integration: Tests multiple Agent components together.</li> <li>Moderate Isolation: May use real system resources but in controlled manner.</li> <li>Execution Time: Should complete within minutes (&lt; 5 minutes per test).</li> <li>Environment Specific: May require specific OS features or permissions.</li> </ul> <p>Examples: <pre><code>// Testing DogStatsD metric forwarding pipeline\nfunc TestDogStatsDMetricPipeline(t *testing.T) {\n    // Start Agent with test configuration\n    agent := startTestAgent(t, &amp;Config{\n        DogStatsDPort: 8125,\n        APIKey: \"test-key\",\n        FlushInterval: time.Second,\n    })\n    defer agent.Stop()\n\n    // Send metric via DogStatsD\n    conn, err := net.Dial(\"udp\", \"localhost:8125\")\n    require.NoError(t, err)\n    defer conn.Close()\n\n    _, err = conn.Write([]byte(\"test.metric:42|g\"))\n    require.NoError(t, err)\n\n    // Verify metric was processed and forwarded\n    assert.Eventually(t, func() bool {\n        return agent.GetMetricCount(\"test.metric\") &gt; 0\n    }, 5*time.Second, 100*time.Millisecond)\n}\n\n// Testing configuration reload\nfunc TestAgentConfigReload(t *testing.T) {\n    configFile := writeTestConfig(t, &amp;Config{LogLevel: \"info\"})\n    agent := startTestAgent(t, configFile)\n    defer agent.Stop()\n\n    // Update configuration\n    updateTestConfig(t, configFile, &amp;Config{LogLevel: \"debug\"})\n\n    // Trigger reload\n    err := agent.ReloadConfig()\n    require.NoError(t, err)\n\n    // Verify new configuration is active\n    assert.Equal(t, \"debug\", agent.GetLogLevel())\n}\n</code></pre></p> <p>Characteristics:</p> <ul> <li>May spawn actual Agent processes.</li> <li>Tests real configuration files and command-line arguments.</li> <li>Validates inter-process communication.</li> <li>Can test Python integration and bindings.</li> </ul>"},{"location":"guidelines/testing/test-categories/#e2e-end-to-end-tests","title":"E2E (End-to-End) Tests","text":"<p>Purpose: Validate complete user workflows in production-like environments with real infrastructure managed by Pulumi, and external services.</p> <p>When to Use E2E Tests:</p> <ul> <li>Testing complete deployment scenarios.</li> <li>Validating Agent behavior in different operating systems.</li> <li>Testing Kubernetes integration and autodiscovery.</li> <li>Verifying cloud provider integrations (AWS, Azure, GCP).</li> <li>Testing upgrade and rollback procedures.</li> <li>Validating security configurations and compliance.</li> </ul> <p>Criteria:</p> <ul> <li>Production-Like Environment: Uses real infrastructure (VMs, containers, cloud services).</li> <li>Complete Workflows: Tests entire user journeys from installation to data delivery.</li> <li>Extended Duration: May run for hours to test long-running scenarios.</li> <li>Infrastructure Dependencies: Requires provisioned environments (Pulumi, Terraform).</li> <li>Comprehensive Coverage: Tests all supported platforms and configurations.</li> </ul> <p>Example:</p> <p>More examples can be found in the examples directory. <pre><code>package examples\n\nimport (\n    \"testing\"\n\n    \"github.com/DataDog/datadog-agent/test/e2e-framework/testing/e2e\"\n    \"github.com/DataDog/datadog-agent/test/e2e-framework/testing/environments\"\n    awshost \"github.com/DataDog/datadog-agent/test/e2e-framework/testing/provisioners/aws/host\"\n)\n\ntype vmSuite struct {\n    e2e.BaseSuite[environments.Host]\n}\n\n// TestVMSuite runs tests for the VM interface to ensure its implementation is correct.\nfunc TestVMSuite(t *testing.T) {\n    suiteParams := []e2e.SuiteOption{e2e.WithProvisioner(awshost.ProvisionerNoAgentNoFakeIntake())}\n\n    e2e.Run(t, &amp;vmSuite{}, suiteParams...)\n}\n\nfunc (v *vmSuite) TestExecute() {\n    vm := v.Env().RemoteHost\n\n    out, err := vm.Execute(\"whoami\")\n    v.Require().NoError(err)\n    v.Require().NotEmpty(out)\n}\n</code></pre></p> <p>Infrastructure Examples:</p> <ul> <li>Container Orchestration: Testing in Kubernetes, Docker Swarm, ECS.</li> <li>Operating Systems: Validating on Ubuntu, CentOS, Windows Server, macOS.</li> <li>Cloud Integrations: Testing AWS EC2, Azure VMs, GCP Compute Engine.</li> <li>Network Configurations: Testing with firewalls, load balancers, service meshes.</li> </ul> <p>Test Scenarios:</p> <ul> <li>Fresh installation and first-time setup.</li> <li>Agent updates and version migrations.</li> <li>High-availability and failover scenarios.</li> <li>Performance under load and resource constraints.</li> <li>Security configurations and compliance validation.</li> </ul>"},{"location":"guidelines/testing/test-categories/#choosing-the-right-test-type","title":"Choosing the Right Test Type","text":"<p>Use this decision tree to determine the appropriate test category:</p> <ol> <li>Can you test it with mocked dependencies? \u2192 Unit Test</li> <li>Does it require external services but limited scope? \u2192 Integration Test</li> <li>Does it test multiple Agent components together? \u2192 System Test</li> <li>Does it require production-like infrastructure? \u2192 E2E Test</li> </ol> <p>Speed vs Coverage Trade-off:</p> <ul> <li>Unit Tests: Maximum speed, narrow coverage.</li> <li>Integration Tests: Fast execution, moderate coverage.</li> <li>System Tests: Moderate speed, broad coverage.</li> <li>E2E Tests: Slower execution, maximum coverage.</li> </ul> <p>Execution Context:</p> <ul> <li>Unit + Integration: Run on every commit (CI/CD pipeline).</li> <li>System: Run on pull requests and nightly builds.</li> <li>E2E: Run on releases and scheduled intervals.</li> </ul> <p>Remember: A well-tested codebase uses all four categories strategically. Start with unit tests for core logic, add integration tests for external dependencies, use system tests for component interactions, and implement E2E tests for critical user workflows.</p>"},{"location":"hostname/hostname_force_config_as_canonical/","title":"Config-provided hostname starting with <code>ip-</code> or <code>domu</code>","text":""},{"location":"hostname/hostname_force_config_as_canonical/#description-of-the-issue","title":"Description of the issue","text":"<p>In v6 and v7 Agents, if <code>hostname</code> is set in <code>datadog.yaml</code> (or through the <code>DD_HOSTNAME</code> env var) and its value starts with <code>ip-</code> or <code>domu</code>, the hostname is not used in-app as the canonical hostname, even if it is a valid hostname. More information about what a canonical hostname is can be found at How does Datadog determine the Agent hostname?.</p> <p>To know if your Agents are affected, starting with v6.16.0 and v7.16.0, the Agent logs the following warning if it detects a situation where the config-provided hostname is a valid hostname but will not be accepted as the canonical hostname in-app: <code>Hostname '&lt;HOSTNAME&gt;' defined in configuration are not used as the in-app hostname. For more information: https://dtdg.co/agent-hostname-force-config-as-canonical</code></p> <p>If this warning is logged, you have the following options:</p> <ul> <li>If you are satisfied with the in-app hostname: unset the configured <code>hostname</code> from <code>datadog.yaml</code> (or the <code>DD_HOSTNAME</code> env var) and restart the Agent; or</li> <li>If you are not satisfied with the in-app hostname, and want the configured hostname to appear as the in-app hostname, follow the instructions below</li> </ul>"},{"location":"hostname/hostname_force_config_as_canonical/#allowing-agent-in-app-hostnames-to-start-with-ip-or-domu","title":"Allowing Agent in-app hostnames to start with <code>ip-</code> or <code>domu</code>","text":"<p>Starting with Agent v6.16.0 and v7.16.0, the Agent supports the config option <code>hostname_force_config_as_canonical</code> (default: <code>false</code>). When set to <code>true</code>, a configuration-provided hostname starting with <code>ip-</code> or <code>domu</code> is accepted as the canonical hostname in-app:</p> <ul> <li>For new hosts, enabling this option works immediately.</li> <li>For hosts that already report to Datadog, after enabling this option, contact Datadog support at support@datadoghq.com so that the in-app hostname can be changed to your configuration-provided hostname.</li> </ul>"},{"location":"how-to/build/distributions/","title":"How to build agent distribution packages","text":""},{"location":"how-to/build/distributions/#intro","title":"Intro","text":""},{"location":"how-to/build/distributions/#what-are-distribution-packages","title":"What are distribution packages ?","text":"<p>By \"distribution package\", we mean an artifact used by end users to install the Agent on their system. The format is OS-specific:</p> OS Package Format Linux <code>.deb</code> (Debian-based) / <code>.rpm</code> (RHEL-based) Windows <code>.msi</code> macOS <code>.dmg</code> <p>These distribution packages contain a binary of the Agent, along with any supporting libraries needed for the agent to function properly.</p>"},{"location":"how-to/build/distributions/#omnibus","title":"Omnibus","text":"<p>Agent packages for all the supported platforms are built using Omnibus.</p> <p>Info</p> <p>There is an ongoing effort to migrate our build system to Bazel, so this may change in the near-to-mid future.</p> <p>Warning</p> <p>Omnibus creates a package for the operating system it runs on, so you'll get a <code>.deb</code> package on Debian-based distros, an <code>msi</code> installer on Windows etc.</p> <p>There is currently no way to \"cross-build\" packages for a platform different than the host's.</p> <p>Omnibus is best called indirectly, via dda commands. The main entrypoint is the <code>omnibus.build</code> invoke task, which you can run like this: <pre><code>dda inv omnibus.build\n</code></pre></p> <p>This will probably not work out of the box though - see instructions below for more information.</p>"},{"location":"how-to/build/distributions/#building-for-linux-deb","title":"Building for Linux: <code>deb</code>","text":""},{"location":"how-to/build/distributions/#using-the-dev-env","title":"Using the <code>dev env</code>","text":"<p>The developer environments we provide contain all the dependencies required for building Omnibus packages, along with extra tools and features that make local development easier. We recommend that you use them for all your development needs.</p> <p>To build distribution packages in a developer environment:</p> <ol> <li>Follow the <code>dev env</code> tutorial to install the required tools and familiarize yourself with the dev environments.</li> <li>Make sure Docker is running on your system.</li> <li>Start a developer environment, which will automatically pull the latest version of the container image: <code>dda env dev start</code></li> <li> <p>Either:</p> <ul> <li>Connect to a shell inside the dev container: <code>dda env dev shell</code></li> <li>Open an IDE window inside the container: <code>dda env dev code</code></li> <li>Run a single command inside the container: prefix the command in the next step with <code>dda env dev run</code></li> </ul> </li> <li> <p>Run the following command: <pre><code>dda inv -- -e omnibus.build\n</code></pre></p> </li> </ol> <p>Once the process completes, the built artifacts will be available in the container under <code>/omnibus/pkg</code>.</p> Moving the artifacts back to the host <p>The <code>datadog-agent</code> local repo clone is bind-mounted into the dev env container. You can use this to access your artifacts from the host, by copying them from <code>/omnibus/pkg</code> to <code>/root/repos/datadog-agent</code>: <pre><code>mv /omnibus/pkg /root/repos/datadog-agent/bin\n</code></pre></p>"},{"location":"how-to/build/distributions/#using-the-build-image","title":"Using the build image","text":"<p>We provide a Docker image containing all the build dependencies required for building <code>deb</code> packages via Omnibus: <code>datadog/agent-buildimages-linux</code>.This image is the one used by CI, and as such it is quite bare-bones. The developer environments mentioned in the section above are based on this image.</p> Building the image locally <p>The Dockerfile for this image is available in the datadog-agent-buildimages repository. To build it from scratch, you can run the following command from the root of that repo:</p> <pre><code>docker build -t datadog-agent-buildimages:linux -f linux/Dockerfile .\n</code></pre> <ol> <li>Make sure Docker is running on your machine.</li> <li>Navigate to the root folder of a clone of the <code>datadog-agent</code> repo.</li> <li>Run the following command, which will create a container for the previously-mentioned image, and run the <code>omnibus.build</code> task inside. <pre><code>docker run\n    -v \"$PWD:/go/src/github.com/DataDog/datadog-agent\"\n    -v \"/tmp/omnibus:/omnibus\"\n    -v \"/tmp/opt/datadog-agent:/opt/datadog-agent\"\n    -v\"/tmp/gems:/gems\"\n    --workdir=/go/src/github.com/DataDog/datadog-agent\n    datadog/agent-buildimages-linux\n    dda inv -- -e omnibus.build --base-dir=/omnibus --gem-path=/gems\n</code></pre></li> </ol> <p>Info</p> <p>The container will bind-mount 3 volumes on the host to avoid starting from scratch at each Omnibus run:</p> <ul> <li><code>/tmp/omnibus</code>, containing the Omnibus base directory</li> <li><code>/tmp/opt/datadog-agent</code>, containing the Omnibus installation directory</li> <li><code>/tmp/gems</code>, containing all the ruby gems installed with Bundler</li> </ul> <p>Once the process completes, the built artifacts will be available on your host under <code>/tmp/omnibus/pkg</code>.</p>"},{"location":"how-to/build/distributions/#building-on-the-host-discouraged","title":"Building on the host (discouraged)","text":"<p>Danger</p> <p>Building on the host is not recommended, and this section of the guide will be maintained on a best-effort basis.</p> <p>Running Omnibus builds locally may affect the global state of your machine, and in particular the installation of the Agent already present on your laptop.</p> <p>Please use one of the containerized build options instead.</p> <p>Running an Omnibus build will both create and install an Agent distribution package.</p> <ul> <li>The project will be built locally into a <code>.tar.xz</code> archive under <code>omnibus/pkg</code>.</li> <li>The project will be installed under <code>/opt/datadog-agent</code>. This is the same path where the Agent is installed on customer machines.</li> </ul> <p>Warning</p> <p>If you already have a Datadog Agent installed, you will need to move it to a different location before operating Omnibus - otherwise it will get overwritten by the build.</p> <p>As a Datadog employee, an Agent is installed on your machine during IT's onboarding session.</p> Linux-specific requirements <ul> <li>On Linux, you will need root privileges, as you need permission to write into <code>/opt</code></li> <li>On Linux, some configuration files will also be dropped under <code>/etc</code>.</li> </ul> <ol> <li>Follow the general local setup instructions</li> <li>Make <code>/opt</code> world-readable</li> <li>Run the following command: <pre><code>dda inv -- omnibus.build --base-dir=$HOME/.omnibus\n</code></pre></li> </ol> <p>The path you pass with the <code>--base-dir</code> option will be used as a working directory for the Omnibus build. Once the build completes, it will contain:</p> Directory Contents <code>src</code> The sources downloaded by Omnibus <code>cache</code> The binaries cached after building those sources <code>pkg</code> The final <code>deb</code>/<code>rpm</code>/<code>dmg</code> artifacts Make sure to pass a <code>--base-dir</code> ! <p>It is strongly advised to pass a <code>--base-dir</code>, and point it to a directory outside of the Agent repo.</p> <p>By default Omnibus stores packages in the project folder itself: running the task multiple times would recursively add those artifacts to the source files for the <code>datadog-agent</code> software definition.</p> <p>Tip</p> <p>You can fine tune an Omnibus run by passing more options, see <code>dda inv -- omnibus.build --help</code> for the list of all the available options.</p> <p>You can chose to generate an installable package in the form of a <code>deb</code>/<code>rpm</code> artifact by providing a <code>OMNIBUS_FORCE_PACKAGES</code> environment variable during the build.</p> <p>On macOS, a <code>dmg</code> artifact will always be generated.</p>"},{"location":"how-to/build/distributions/#building-for-linux-rpm","title":"Building for Linux: <code>rpm</code>","text":"<p>Some extra dependencies are required for building <code>rpm</code> packages that are not yet included in the main <code>datadog/agent-buildimages-linux</code> build image.</p> <p>A separate docker image containing these special dependencies is also available. This image, contrary to the main <code>datadog/agent-buildimages-linux</code> image, is not multi-arch - thus there are two flavors depending on the CPU architecture of the host machine:</p> <ul> <li>For <code>x86_64</code>/<code>amd64</code>: <code>datadog/agent-buildimages-rpm_x64</code></li> <li>For <code>arm64</code>/<code>aarch64</code>: <code>datadog/agent-buildimages-rpm_arm64</code></li> </ul> <p>To build using these images, follow the same instructions as for <code>deb</code> packages, but replace <code>datadog/agent-buildimages-linux</code> with the appropriate flavor of the <code>rpm</code> image for your CPU platform. You can also attempt a host-based build, although this is heavily discouraged.</p>"},{"location":"how-to/build/distributions/#building-for-macos","title":"Building for MacOS","text":"<p>We do not currently support MacOS development environments or any container build image. You will therefore need to follow the host-based build instructions.</p> <p>When running the build command, you might want to skip the signing step by adding the <code>--skip-sign</code> flag.</p>"},{"location":"how-to/build/distributions/#building-for-windows","title":"Building for Windows","text":"<p>Warning</p> <p>This can only be done in a containerized environment. Please see the relevant folder in <code>datadog-agent-buildimages</code> for more details on the images to use.</p> Image naming scheme <p>As of the writing of this doc, the relevant images follow this naming pattern: <code>registry.ddbuild.io/datadog-agent-buildimages/windows_ltsc{$YEAR}_${ARCH}${SUFFIX}:${TAG}</code></p> <ul> <li><code>YEAR</code> is either <code>2022</code> or <code>2025</code></li> <li><code>ARCH</code> can only be <code>x64</code> at this time</li> <li><code>SUFFIX</code> can be either empty or <code>_test_only</code>, which refers to images used by CI in PR builds.</li> <li>The <code>TAG</code> follows the usual convention, i.e. <code>v{gitlab pipeline id}-{short commit sha}</code></li> </ul> <p>Example</p> <p><code>registry.ddbuild.io/ci/datadog-agent-buildimages/windows_ltsc2025_x64:v77240728-510448c3</code></p> <p>First, mount / clone a checkout of the <code>datadog-agent</code> repo inside the container.</p> <p>The recommended way to do this while developing manually is to bind-mount your host's checkout of the repo into the container.On the host, while inside the <code>datadog-agent</code> repo: <pre><code>docker run -v \"$(Get-Location):c:\\mnt\" &lt;image&gt;\n</code></pre></p> <p>You can then invoke one of the windows build scripts, available in <code>tasks/winbuildscripts</code>:</p> <ul> <li><code>Build-AgentPackages.ps1</code> is used for building the \"main\" Agent msi package</li> <li><code>Build-OmnibusTarge.ps1</code> is used for building all other Agent packages via Omnibus</li> <li><code>Build-InstallerPackages.ps1</code> is used for building the <code>.exe</code> installer for the Agent.</li> </ul> <p>These scripts read a few environment variables, notably (non-exhaustive !):</p> <ul> <li><code>OMNIBUS_TARGET</code> - usually set to <code>main</code></li> <li><code>TARGET_ARCH</code> - only <code>x64</code> is supported at the moment</li> </ul> <p>Example</p> <pre><code>docker run -v \"$(Get-Location):c:\\mnt\" -e OMNIBUS_TARGET=main -e TARGET_ARCH=x64 registry.ddbuild.io/ci/datadog-agent-buildimages/windows_ltsc2025_x64:v77240728-510448c3 powershell -C \"c:\\mnt\\tasks\\winbuildscripts\\Build-AgentPackages.ps1 -BuildOutOfSource 1 -InstallDeps 1 -CheckGoVersion 1\"\n</code></pre> <p>If the build succeeds, the build artifacts can be found under <code>omnibus\\pkg</code> in the repo.</p>"},{"location":"how-to/build/standalone/","title":"How to build standalone agent binaries","text":""},{"location":"how-to/build/standalone/#building-agent-binaries","title":"Building agent binaries","text":"<p>The Core Agent is built using the <code>dda inv agent.build</code> command.</p> <pre><code>dda inv agent.build --build-exclude=systemd\n</code></pre> <p>Running this command will:</p> <ul> <li>Discard any changes done in <code>bin/agent/dist</code>.</li> <li>Build the Agent and write the binary to <code>bin/agent/agent</code>, with a <code>.exe</code> extension on Windows.</li> <li>Copy files from <code>dev/dist</code> to <code>bin/agent/dist</code>.</li> </ul> <p>Caveat</p> <p>If you built an older version of the Agent and are encountering the error <code>make: *** No targets specified and no makefile found</code>, remove the <code>rtloader/CMakeCache.txt</code> file.</p> <p>Other Agent binaries</p> <p>Other agent binaries are built using <code>dda inv &lt;target&gt;.build</code> commands. Some examples are:</p> <pre><code>dda inv dogstatsd.build\ndda inv otel-agent.build\ndda inv system-probe.build\ndda inv trace-agent.build\n</code></pre> <p>You can find the full list of buildable agent-related binaries here.</p>"},{"location":"how-to/build/standalone/#including-or-excluding-agent-features","title":"Including or excluding Agent features","text":"<p>Different features of the Agent can be included / excluded at build time, by leveraging Go build constraints. This can be done by passing the <code>--build-include</code> or <code>--build-exclude</code> flags to the build commands. A (non-exhaustive) list of available features can be found here.</p> <p>The set of features enabled by default (i.e. with no flag) depends on the build context: which binary you are trying to build, which flavor of the agent, which platform you are building on etc.</p> <p>Info</p> <p>If you want to replicate the same configuration of the Agent as the one distributed in system packages, you need to use this default set of features - so no flag needs to be passed.</p> Determining the default set of features <p>The default set of features is determined by the <code>get_default_build_tags</code> method.</p> <p>There is a command you can use to print out the default build tags for your build context: <pre><code>dda inv print-default-build-tags\n</code></pre></p> <p>You can give more info about your build context using the <code>-b</code>, <code>-f</code> and <code>-p</code> flags: <pre><code>dda inv print-default-build-tags -b otel-agent -p windows\n&gt; otlp,zlib,zstd\ndda inv print-default-build-tags -f fips\n&gt; bundle_installer,consul,datadog.no_waf,ec2,etcd,fargateprocess,goexperiment.systemcrypto,grpcnotrace,jmx,kubeapiserver,kubelet,ncm,oracle,orchestrator,otlp,python,requirefips,trivy_no_javadb,zk,zlib,zstd\n</code></pre> Run <code>dda inv print-default-build-tags --help</code> for more details.</p> <p>Example</p> <p>To include the <code>zstd</code>, <code>etcd</code> and <code>python</code> features: <pre><code>dda inv &lt;target&gt;.build --build-include=zstd,etcd,python\n</code></pre></p> <p>To exclude some features that would otherwise be enabled: <pre><code>dda inv &lt;target&gt;.build --build-exclude=systemd,python\n</code></pre></p>"},{"location":"how-to/build/standalone/#running-agents","title":"Running agents","text":"<p>You can run the Core Agent directly in the foreground with the following command.</p> <pre><code>./bin/agent/agent run -c bin/agent/dist/datadog.yaml\n</code></pre> <p>Note</p> <p>The file <code>bin/agent/dist/datadog.yaml</code> is copied from <code>dev/dist/datadog.yaml</code> by <code>dda inv agent.build</code> and must contain a valid API key. If this did not already exist, you can create a file at any path and reference that with the <code>-c</code> flag instead.</p>"},{"location":"how-to/build/standalone/#agent-bundles","title":"Agent Bundles","text":"<p>As an option, the Agent can combine functionality from multiple binaries into a single one to reduce the space used on disk. We call this a \"bundled agent\".</p>"},{"location":"how-to/build/standalone/#building-an-agent-bundle","title":"Building an agent bundle","text":"<p>To build a bundled agent, simply use the <code>--bundle</code> flag with the <code>dda inv agent.build</code> to include the features from other binaries alongside the main <code>agent</code> into your final artifacts.</p> <p>Example</p> <p>To create a binary that contains the features from the main <code>agent</code>, as well as the features from <code>process-agent</code> and <code>security-agent</code>, use: <pre><code>dda inv agent.build --bundle process-agent --bundle security-agent\n</code></pre></p> Under the hood <p>Making a bundle - combining functionality from multiple binaries - just corresponds to building an agent binary including the source code from the others.</p> <p>Like other features, this is accomplished through Go build constraints. Under the hood, building with a <code>--bundle</code> argument simply corresponds to including a special agent \"feature\".</p> <p>Those special features are named in a predictable pattern: <code>bundle_&lt;binary name&gt;</code>, ex: <code>bundle_process_agent</code>.</p> <p>Thus, the two following commands are equivalent: <pre><code>dda inv agent.build --bundle process-agent --bundle security-agent\ndda inv agent.build --build-include=bundle_process_agent,bundle_security_agent\n</code></pre></p>"},{"location":"how-to/build/standalone/#using-an-agent-bundle","title":"Using an agent bundle","text":"<p>The bundled agent binary, when executed, will dynamically determine which binary to act as. This is determined according to:</p> <ol> <li>The value of the <code>DD_BUNDLED_AGENT</code> environment variable.</li> <li>If it is not set, the process name is used instead.</li> <li>As a fallback, the executable will behave as the 'main' Agent.</li> </ol> <p>Example</p> <pre><code># Build the agent bundle\ndda inv agent.build --bundle process-agent\n# -- The built artifact is available in bin/agent/agent\n\n# This behaves as the main agent\n./bin/agent/agent\n\n# This behaves as the process-agent\nDD_BUNDLED_AGENT=process-agent ./bin/agent/agent\n</code></pre>"},{"location":"how-to/debug-agents/","title":"Tools to troubleshoot a running Agent","text":"<p>This page attempts to list useful tools and resources to troubleshoot and profile a running Agent.</p>"},{"location":"how-to/debug-agents/#pprof","title":"pprof","text":"<p>The Agent exposes pprof's HTTP server on port <code>5000</code> by default. Through the pprof port you can get profiles (CPU, memory, etc) on the go runtime, along with some general information on the state of the runtime.</p> <p>In particular/additionally, the following commands can come in handy:</p> <ul> <li>List all goroutines: <pre><code>curl http://localhost:5000/debug/pprof/goroutine?debug=2\n</code></pre></li> <li>Profile the go heap: <pre><code>go tool pprof http://localhost:5000/debug/pprof/heap\n</code></pre></li> </ul>"},{"location":"how-to/debug-agents/#expvar","title":"expvar","text":"<p>The Agent also exposes expvar variables through an HTTP server on port <code>5000</code> by default, in JSON format.</p> <p>General documentation: https://golang.org/pkg/expvar/</p> <p>Most components of the Agent expose variables (under their respective key). By default expvar also exposes general memory stats from <code>runtime.MemStats</code>. In particular, the <code>Sys</code>, <code>HeapSys</code> and <code>HeapInuse</code> variables can be interesting.</p> <p>Using the <code>jq</code> command-line tool, it's rather easy to explore and find relevant variables, for example: <pre><code># Find total bytes of memory obtained from the OS by the go runtime\ncurl -s http://localhost:5000/debug/vars | jq '.memstats.Sys'\n# Get names of checks that the collector's check runner has run\ncurl -s http://localhost:5000/debug/vars | jq '.runner.Checks | keys'\n</code></pre></p>"},{"location":"how-to/debug-agents/#delve","title":"delve","text":"<p>A debugger for Go.</p> <p>Example usage: <pre><code>$ sudo dlv attach `pgrep -f '/opt/datadog-agent/bin/agent/agent run'`\n(dlv) help # help on all commands\n(dlv) goroutines # list goroutines\n(dlv) threads # list threads\n(dlv) goroutine &lt;number&gt; # switch to goroutine\n</code></pre></p> <p>Using external/split debug symbols</p> <p>If you're running a stripped binary of the agent, you can <code>attach</code> and point delve at the debug symbols.</p> <p>Configure delve to search for debug symbols in the path you installed debug symbols to.</p> <p>Eg, on ubuntu/debian, <code>apt install datadog-agent-dbg</code> installs to <code>/opt/datadog-agent/.debug</code>, so modify your delve config file to search this directory:</p> <pre><code># delve config file is at $HOME/.config/dlv/config.yml\ndebug-info-directories: [\"/usr/lib/debug/.build-id\", \"/opt/datadog-agent/.debug/\" ]\n</code></pre> <p>Running delve as root</p> <p>If you use <code>sudo</code> to run <code>dlv attach</code>, <code>$HOME</code> will be set to <code>/root</code>. You may want to symlink <code>/root/.config/dlv/config.yml</code> to point to your user delve config file.</p>"},{"location":"how-to/debug-agents/#gdb","title":"gdb","text":"<p>GDB can in some rare cases be useful to troubleshoot the embedded python interpreter. See https://wiki.python.org/moin/DebuggingWithGdb</p> <p>Example usage (using the legacy <code>pystack</code> macro): <pre><code>sudo ./gdb --pid &lt;pid&gt;\ninfo threads\nthread &lt;number&gt; # switch to thread\npystack # python stacktrace of current thread\n</code></pre></p> <p>To debug a core dump generated with the <code>c_core_dump</code> Agent option, refer to the GDB docker image that includes the Agent symbols.</p> <p>For simple debugging cases, you can simply use the python-provided <code>pdb</code> to jump into a debugging shell by adding to the python code that's run: <pre><code>import pdb\npdb.set_trace()\n</code></pre> and running the agent in the foreground.</p>"},{"location":"how-to/debug-agents/linux/","title":"Linux Troubleshooting","text":""},{"location":"how-to/debug-agents/linux/#generating-and-using-core-dumps-on-linux","title":"Generating and using core dumps on Linux","text":""},{"location":"how-to/debug-agents/linux/#generating-core-dumps-on-agent-crashes","title":"Generating core dumps on Agent crashes","text":"<p>There are two ways to generate core dumps when the Agent crashes on Linux.</p> <p>Starting on version 7.27 the Datadog Agent includes the <code>go_core_dump</code> option that, when enabled, makes any Agent process generate a core dump when it crashes. This is the simplest option and it can generate core dumps so long as the crash happens after the internal packages (configuration, logging...) finish initializing.</p> <p>Core dump drop locations</p> <p>Where core dumps end up depends on the pattern set in <code>/proc/sys/kernel/core_pattern</code>:</p> <ul> <li>If you are in an OS that uses <code>systemd</code>, the core dump will be sent to <code>coredumpctl</code> .</li> <li>Otherwise, you may need to set the <code>/proc/sys/kernel/core_pattern</code> to a folder that can be written to by the user that will run the Agent.</li> </ul> <p>Example</p> <p>To use the <code>/var/crash/</code> folder, set the pattern to <code>/var/crash/core-%e-%p-%t</code>.</p> <p>For previous versions of the Agent and for crashes that happen before initialization (e.g. during Go runtime initialization or during configuration initialization), you need to set the crashing setting manually. To do this follow these steps:</p> <ol> <li>Set the user limit for core dump maximum size limit to a high-enough value. For example, you can set it to be arbitrarily big by running <code>ulimit -c unlimited</code>.</li> <li>Run any of the Datadog Agents debug packages manually, setting the <code>GOTRACEBACK</code> environment variable to <code>crash</code>. This will send a <code>SIGABRT</code> signal to the Agent process and trigger the creation of a core dump.</li> </ol>"},{"location":"how-to/debug-agents/linux/#inspecting-a-core-dump","title":"Inspecting a core dump","text":"<p>Use <code>dlv core DUMPFILE EXEFILE</code> to debug against a dump file.</p> <p>You need to use the debug binaries in the debug package to as the <code>EXEFILE</code>.</p> <p>See the delve section for more information on using delve.</p>"},{"location":"how-to/debug-agents/windows/","title":"Windows Troubleshooting","text":"<p>Prior to 7.23, Agent binaries (Datadog Agent, Process Agent, Trace Agent, etc.) on Windows contain symbol information.</p> <p>Starting from 7.23, Agent binaries on Windows have debugging information stripped. The original files are packed in a file called debug package.</p>"},{"location":"how-to/debug-agents/windows/#prerequisite","title":"Prerequisite","text":"<p>To debug Agent process, Golang Runtime, Git and Golang Delve must be installed.</p> <p>Download the matching debug package. If the MSI file is <code>datadog-agent-7.23.0-x86_64.msi</code>, the debug package should be <code>datadog-agent-7.23.0-x86_64.debug.zip</code>.</p>"},{"location":"how-to/debug-agents/windows/#live-debugging","title":"Live Debugging","text":"<p>Delve debugger on Windows cannot attach to the service process. The corresponding Windows service must be stopped and disabled.</p> <p>For pre 7.23, start the Agent executable in the interactive session.</p> <p>For 7.23 or later version, find the file in the debug package. For <code>agent.exe</code>, the file in debug package is under <code>\\src\\datadog-agent\\src\\github.com\\DataDog\\datadog-agent\\bin\\agent\\agent.exe.debug</code>. You might find the same file under <code>\\omnibus-ruby\\src\\cf-root\\bin</code>. Use either one is fine. Copy the file to replace the executable file you want to debug, start the agent executable in the interactive session.</p> <p>Use <code>dlv attach PID</code> to attach to the running process and start debugging.</p>"},{"location":"how-to/debug-agents/windows/#non-live-debugging","title":"Non-live Debugging","text":"<p>Use <code>dlv core DUMPFILE EXEFILE</code> to debug against a dump file.</p> <p>For 7.23 or newer, the EXEFILE is the .debug file in the debug package.</p>"},{"location":"how-to/go/config/","title":"Agent Configuration","text":"<p>This doc describes how to define new configuration parameters for the Agent.</p> <ol> <li>Define your config.</li> <li>Add it to the config template (optional).</li> <li>Use your config in your code.</li> <li>Request a review from the Agent configuration team (team/agent-configuration)</li> </ol> <p>If you have any questions, head over to #agent-configuration and ask (datadog internal).</p>"},{"location":"how-to/go/config/#1-define-your-config","title":"1. Define Your Config","text":"<p>A config must be declared before it can be used. If you don't do this, the Agent will log warnings about missing config at runtime that look like:</p> <pre><code>WARN | config keyconfig key \"config.subsystem.bananas\" is unknown\n</code></pre> <p>There are multiple places a config can be defined:</p> <ul> <li> <p>Above all else, prefer consistency. If there's existing similar config,   put the new config item alongside that existing config.</p> </li> <li> <p>If you want your config to be defined by the user in <code>system-probe.yml</code> then   your declaration belongs in [<code>system_probe.go</code>].</p> </li> <li> <p>Otherwise it lives in the default <code>datadog-agent.yaml</code> file and goes in   [<code>config.go</code>].</p> </li> </ul>"},{"location":"how-to/go/config/#2-add-to-template","title":"2. Add to Template","text":"<p>By default newly declared configs are not added to the sample config file a user sees.</p> <p>If you want your config to appear in the sample config file, add it to the config template.</p>"},{"location":"how-to/go/config/#3-use-your-config","title":"3. Use Your Config","text":"<p>You can access your configured value (or the declared default) using the config <code>model.Reader</code>. For example:</p> <pre><code>if cfg.GetBool(\"config.subsystem.bananas\") {\n    // Go bananas\n}\n</code></pre> <p>See the package documentation for available methods.</p>"},{"location":"how-to/go/config/#4-request-a-review","title":"4. Request a Review!","text":"<p>Please add this label to your PRs: <code>team/agent-configuration</code></p> <p>This will summon a config wizard who can review your changes and suggest any changes.</p>"},{"location":"how-to/go/modules/","title":"Creating go modules in the Agent project","text":"<p>The Datadog Agent is not meant to be imported as a Go library. However, certain parts of it can be exposed as a library by making use of nested Go modules.</p> <p>This allows for exposing parts of the codebase without needing to extract the code to a different repository, and helps avoid <code>replace</code> directive clashes.</p> <p>Warning</p> <p>At present the Go modules offer no public stability guarantees and are intended for internal consumption by Datadog only.</p>"},{"location":"how-to/go/modules/#creating-a-new-module","title":"Creating a new module","text":"<ol> <li> <p>Determine the packages that you want to expose and their dependencies.</p> <p>Info</p> <p>You might need to refactor the code to have an exportable package, because the <code>replace</code> directives that we use might be incompatible with your project.</p> </li> <li> <p>Create a directory for the module:</p> <pre><code>cd ~/my_path_to/datadog-agent &amp;&amp; mkdir mymodule\n</code></pre> </li> <li> <p>Initialize a new Go module:</p> <p><pre><code>cd path/to/mymodule &amp;&amp; go mod init &amp;&amp; go mod tidy\n</code></pre> This will create the <code>go.mod</code> and <code>go.sum</code> files in the module's root folder. Ensure the <code>go version</code> line matches the version in the main <code>go.mod</code> file.</p> </li> <li> <p>Create a package file named <code>doc.go</code> in your new module based on this template:</p> <pre><code>/// doc.go\n\n// Unless explicitly stated otherwise all files in this repository are licensed\n// under the Apache License Version 2.0.\n// This product includes software developed at Datadog (https://www.datadoghq.com/).\n// Copyright 2025-present Datadog, Inc.\npackage mymodule\n</code></pre> </li> <li> <p>Update the <code>modules.yml</code> file at the root of the repository, adding a section for your new module.</p> <p>See <code>modules.yml</code> for more details. Here are a couple of example configurations:</p> Example <pre><code>my/module:\n    condition: is_linux\n    used_by_otel: true\n</code></pre> <pre><code>my/module:\n    independent: false\n    lint_targets:\n    - ./pkg\n    - ./cmd\n    - ./comp\n    targets:\n    - ./pkg\n    - ./cmd\n    - ./comp\n</code></pre> </li> <li> <p>Update dependent modules. For each module depending on your new module, add:</p> <ul> <li> <p>A <code>require</code> directive in its <code>go.mod</code> containing the new module's path with version <code>v0.0.0</code>:</p> <pre><code>// Other module's go.mod file\nrequire (\n    // ...\n    github.com/DataDog/datadog-agent/path/to/module v0.0.0\n    // ...\n)\n</code></pre> <p>Note</p> <p>Make sure to also include any dependencies !</p> <p>Tip</p> <p>You can do this by running <code>go get github.com/DataDog/datadog-agent/path/to/mymodule</code> from the root folder of the other module.</p> <p>This will also add <code>require</code> directives for all required dependencies and compute the <code>go.sum</code> changes.</p> </li> <li> <p>A <code>replace</code> directive in the main <code>go.mod</code> file to replace the module with the local path:</p> <pre><code>// main go.mod file\nreplace (\n    github.com/DataDog/datadog-agent/path/to/module =&gt; ./path/to/module\n)\n</code></pre> </li> </ul> <p>Example</p> <p>See this example PR: #17350</p> </li> <li> <p>Cleanup and tidy. Run the following commands to generate the update <code>go.work</code> and <code>go.sum</code> files:</p> <pre><code>dda inv modules.go-work\ngo mod tidy\n</code></pre> </li> </ol>"},{"location":"how-to/go/modules/#nested-go-module-tagging-and-versioning","title":"Nested go module tagging and versioning","text":"<p>A few invoke tasks are available that help with automatically updating module versions and tags:</p> <ul> <li> <p><code>dda inv release.tag-modules</code></p> <p>Creates tags for Go nested modules for a given Datadog Agent version.</p> <p>Info</p> <p>For Agent version <code>7.X.Y</code> the module will have version <code>v0.X.Y</code>.</p> </li> <li> <p><code>dda inv release.update-modules</code></p> <p>Updates the internal dependencies between the different Agent nested go modules.</p> </li> </ul> <p>Info</p> <p>The <code>release.update-modules</code> task is also called automatically by the invoke tasks used as part of the release process:</p> <ul> <li><code>dda inv release.create-rc</code></li> <li><code>dda inv release.finish</code></li> </ul> <p>The <code>release.tag-modules</code> task is also called by the <code>release.tag-version</code> invoke task, using the same commit as the main module, with a tag of the form <code>path/to/module/v0.X.Y</code>.</p>"},{"location":"how-to/go/modules/#the-modulesyml-file","title":"The <code>modules.yml</code> file","text":"<p>The <code>modules.yml</code> file gathers all go module configurations. Each module is listed even if this module has default attributes or is ignored.</p> <p>For each module, you can specify:</p> <ul> <li><code>default</code> - for modules with default attribute values</li> <li><code>ignored</code> - for ignored modules.</li> </ul> <p>To create a special configuration, the attributes of the <code>GoModule</code> class can be overriden - see the definition here for the list of attributes and their details.</p> <p>Tip</p> <p>This file can be linted and checked by using <code>dda inv modules.validate [--fix-format]</code>.</p> <p>Example</p> <pre><code>modules:\n  .:\n    independent: false\n    lint_targets:\n    - ./pkg\n    - ./cmd\n    - ./comp\n    test_targets:\n    - ./pkg\n    - ./cmd\n    - ./comp\n  comp/api/api/def:\n    used_by_otel: true\n  comp/api/authtoken: default\n  test/integration/serverless/src: ignored\n  tools/retry_file_dump:\n    should_test_condition: never\n    independent: false\n    should_tag: false\n</code></pre>"},{"location":"how-to/memory-profiling/c%2B%2B/","title":"C/C++ tracking and troubleshooting","text":"<p>Allocations in the Datadog cgo and RTLoader code have been wrapped by a set of helper functions that help keep accounting with regard to the number of allocations made and freed, as well as their respective addresses and bytes reserved. The RTLoader is not particularly intensive, and thus the overhead for the accounting is fairly negligible, allowing us to keep the feature on at all times on production machines. That said, there is a configuration flag in datadog.yaml you can use to enable/disable the feature:</p> <pre><code>memtrack_enabled: true\n</code></pre> <p>Raw malloc and free calls are deprecated in the RTLoader project. Compiler warnings will occur if anyone attempts to reserve memory without using the accounting wrappers.</p> <p>The way these wrappers work is by registering a Go-callback via cgo, by which we can then call back into Go territory and track the allocations as well as update the relevant go expvars. These expvars can be queried at any point in time and paint a snapshot of the memory usage within the RTLoader.</p> <p>Because these counters are exposed as expvars the most useful way to understand the evolution of the RTLoader/cgo memory usage is by means of the go-expvar check, enabling it, and setting the following configuration:</p> <pre><code>init_config:\n\ninstances:\n  - expvar_url: http://localhost:5000/debug/vars\n    namespace: datadog.agent\n    metrics:\n      # other expvar metrics\n\n      # datadog-agent rtloader monitoring\n      - path: rtloader/AllocatedBytes\n        type: monotonic_counter\n      - path: rtloader/FreedBytes\n        type: monotonic_counter\n      - path: rtloader/Allocations\n        type: monotonic_counter\n      - path: rtloader/Frees\n        type: monotonic_counter\n      - path: rtloader/InuseBytes\n        type: gauge\n      - path: rtloader/UntrackedFrees\n        type: monotonic_counter\n</code></pre> <p>This will show timeseries in the <code>datadog.agent</code> namespace: - datadog.agent.rtloader.allocatedbytes - datadog.agent.rtloader.freedbytes - datadog.agent.rtloader.allocations - datadog.agent.rtloader.frees - datadog.agent.rtloader.inusebytes - datadog.agent.rtloader.untrackedfrees</p> <p>Note:<code>UntrackedFrees</code> is increased when trying to free up code that was not accounted for somewhere in the RTLoader or cgo code. It helps identify developer issues with the RTLoader accounting.</p> <p>The metrics provided can be used to help identify leaks and other memory issues in the C/C++ memory space.</p> <p>Should you want to avoid configuring the expvar check, or if its not viable for you, you can still easily query the expvars with curl. For instance:</p> <pre><code>curl http://localhost:5000/debug/vars | jq .rtloader\n</code></pre> <p>As a developer, please be mindful of compiler messages, and make sure you use the provided wrappers to reserve memory: - <code>void *_malloc(size_t sz);</code> - <code>void _free(void *ptr);</code></p>"},{"location":"how-to/memory-profiling/go/","title":"Go tracking and troubleshooting","text":"<p>To investigate the Go portion of the process memory, you can use the usual and expected tooling available to any Go binary. If you encounter a leak in the Agent process as seen in the process RSS, review the Go memory profile. If everything is okay, the leak may be elsewhere.</p> <p>The usual way to profile go binary memory usage is via the <code>pprof</code> facilities:</p> <ul> <li>Run <code>go tool pprof  http://localhost:5000/debug/pprof/heap</code> to jump into the <code>pprof</code> interpreter and load the heap profile.</li> <li>Run <code>curl localhost:5000/debug/pprof/heap &gt; myheap.profile</code> to save a heap profile to disk. Note: You may have to do this on a box without the <code>Go</code> toolchain.</li> <li>Use <code>go tool pprof</code> to analyze the profile.</li> </ul> <p>Note: You have multiple other profiles on other parts of the Go runtime you can dump: <code>goroutine</code>, <code>heap</code>, <code>threadcreate</code>, <code>block</code>, <code>mutex</code>, <code>profile</code> and <code>trace</code>. This doc only covers <code>heap</code> profiling.</p> <p>You can normally jump into <code>pprof</code> in interactive mode easily and load the profile: <pre><code>go tool pprof myheap.profile\n</code></pre></p> <p>There are several tools available to explore the heap profile, most notably the <code>top</code> tool. Use the <code>top</code> tool to list the top memory hungry elements, including cumulative and sum statistics to produce an input similar to below:</p> <pre><code>(pprof) top\nShowing nodes accounting for 4848.62kB, 100% of 4848.62kB total\nShowing top 10 nodes out of 31\n      flat  flat%   sum%        cum   cum%\n 1805.17kB 37.23% 37.23%  1805.17kB 37.23%  compress/flate.NewWriter\n  858.34kB 17.70% 54.93%   858.34kB 17.70%  github.com/DataDog/datadog-agent/vendor/github.com/modern-go/reflect2.loadGo17Types\n  583.01kB 12.02% 66.96%  2388.18kB 49.25%  github.com/DataDog/datadog-agent/pkg/serializer/jsonstream.(*PayloadBuilder).Build\n  553.04kB 11.41% 78.36%   553.04kB 11.41%  github.com/DataDog/datadog-agent/vendor/github.com/gogo/protobuf/proto.RegisterType\n  536.37kB 11.06% 89.43%   536.37kB 11.06%  github.com/DataDog/datadog-agent/vendor/k8s.io/apimachinery/pkg/api/meta.init.ializers\n  512.69kB 10.57%   100%   512.69kB 10.57%  crypto/x509.parseCertificate\n         0     0%   100%  1805.17kB 37.23%  compress/flate.NewWriterDict\n         0     0%   100%  1805.17kB 37.23%  compress/zlib.(*Writer).Write\n         0     0%   100%  1805.17kB 37.23%  compress/zlib.(*Writer).writeHeader\n         0     0%   100%   512.69kB 10.57%  crypto/tls.(*Conn).Handshake\n</code></pre> <p>or <code>tree</code>:</p> <pre><code>(pprof) tree\nShowing nodes accounting for 4848.62kB, 100% of 4848.62kB total\n----------------------------------------------------------+-------------\n      flat  flat%   sum%        cum   cum%   calls calls% + context\n----------------------------------------------------------+-------------\n                                         1805.17kB   100% |   compress/flate.NewWriterDict\n 1805.17kB 37.23% 37.23%  1805.17kB 37.23%                | compress/flate.NewWriter\n----------------------------------------------------------+-------------\n                                          858.34kB   100% |   github.com/DataDog/datadog-agent/vendor/github.com/modern-go/reflect2.init.0\n  858.34kB 17.70% 54.93%   858.34kB 17.70%                | github.com/DataDog/datadog-agent/vendor/github.com/modern-go/reflect2.loadGo17Types\n----------------------------------------------------------+-------------\n                                         2388.18kB   100% |   github.com/DataDog/datadog-agent/pkg/serializer.Serializer.serializeStreamablePayload\n  583.01kB 12.02% 66.96%  2388.18kB 49.25%                | github.com/DataDog/datadog-agent/pkg/serializer/jsonstream.(*PayloadBuilder).Build\n                                         1805.17kB 75.59% |   github.com/DataDog/datadog-agent/pkg/serializer/jsonstream.newCompressor\n----------------------------------------------------------+-------------\n                                          553.04kB   100% |   github.com/DataDog/datadog-agent/vendor/github.com/gogo/googleapis/google/rpc.init.2\n  553.04kB 11.41% 78.36%   553.04kB 11.41%                | github.com/DataDog/datadog-agent/vendor/github.com/gogo/protobuf/proto.RegisterType\n----------------------------------------------------------+-------------\n                                          536.37kB   100% |   runtime.main\n  536.37kB 11.06% 89.43%   536.37kB 11.06%                | github.com/DataDog/datadog-agent/vendor/k8s.io/apimachinery/pkg/api/meta.init.ializers\n----------------------------------------------------------+-------------\n                                          512.69kB   100% |   crypto/x509.ParseCertificate\n  512.69kB 10.57%   100%   512.69kB 10.57%                | crypto/x509.parseCertificate\n----------------------------------------------------------+-------------\n...\n</code></pre> <p>There are several facets to inspect your profiles: - <code>inuse_space</code>:      Display in-use memory size - <code>inuse_objects</code>:    Display in-use object counts - <code>alloc_space</code>:      Display allocated memory size - <code>alloc_objects</code>:    Display allocated object counts</p> <p>In interactive mode, select and change modes by entering the moce and hitting <code>enter</code>.</p> <p>Another useful feature is the allocation graph, or what the <code>top</code> and <code>tree</code> commands show in text mode graphically. Open the graph directly in your browser using the <code>web</code> command, or if you'd like to export it to a file, use the <code>svg</code> command or another graph exporting commands.</p> <p>Another useful profile you can use if RSS is growing and you cannot resolve the issue is the <code>goroutines</code> profile. It is useful for identifying Go routine leaks, which is another common issue in Go development:</p> <pre><code>go tool pprof  http://localhost:5000/debug/pprof/goroutine\n</code></pre> <p>Load into <code>pprof</code> and explore in the same way as noted above.</p> <p>This section will help you get started, but there is more information available in the links below.</p>"},{"location":"how-to/memory-profiling/go/#further-reading","title":"Further Reading","text":"<ul> <li>Julia Evans: go profiling</li> <li>Detectify: memory leak investigation</li> </ul>"},{"location":"how-to/memory-profiling/overview/","title":"Overview","text":"<p>Troubleshooting Agent Memory Usage</p> <p>The Agent process presents unusual challenges when it comes to memory profiling and investigation. Multiple memory spaces, with various heaps coming from multiple different runtimes, can make identifying memory issues tricky.</p> <p>The Agent has three distinct memory spaces, each handled independently:</p> <ul> <li>Go</li> <li>C/C++</li> <li>Python</li> </ul> <p>There is tooling to dive deeper into each of these environments, but having logic flow through the boundaries defined by these runtimes and their memory management often confuses this tooling, or yields inaccurate results. A good example of a tool that becomes difficult to use in this environment is Valgrind. The problem is Valgrind will account for all allocations in the Go and CPython spaces, and these being garbage collected can make the reports a little hard to understand. You can also try to use a supression file to supress some of the allocations in Python or Go, but it is difficult to find a supression file.</p> <p>This guide covers Go and Python have facilities for tracking and troubleshooting. Datadog also offers some C/C++ facilities to help you track allocations.</p>"},{"location":"how-to/memory-profiling/python/","title":"Python tracking and troubleshooting","text":"<p>Python, another runtime in the Agent process, is also garbage collected. Datadog offers two tools with the Agent that can help you identify memory issues:</p> <ul> <li>Python memory telemetry (Python 3 only)</li> <li>Tracemalloc</li> <li>Pympler</li> </ul>"},{"location":"how-to/memory-profiling/python/#python-memory-telemetry","title":"Python memory telemetry","text":"<p>Python memory telemetry hooks into low-level allocator routines, to provide a coarse view of the total memory allocated by the Python memory manager.</p> <p>Python memory telemetry is only available when using Python 3 (Python 2 lacks the hooks necessary to implement this).</p> <p>Python memory telemetry is part of the Agent internal telemetry and is enabled by default. Set <code>telemetry.python_memory: false</code> to disable.</p> Internal name Default metric name Description <code>pymem__alloc</code> <code>datadog.agent.pymem.alloc</code> Total number of bytes allocated since the start of the Agent. <code>pymem__inuse</code> <code>datadog.agent.pymem.inuse</code> Number of bytes currently allocated by the Python interpreter. <p>The Python memory manager internally maintains a small reserve of unused memory, so the numbers provided by this tool may be slightly larger than the memory actually used by the Python code.</p> <p>This telemetry represents memory allocated by pymalloc and the raw allocator (See Memory management in the Python manual). It does not include memory allocated by native extensions and libraries directly via libc.</p>"},{"location":"how-to/memory-profiling/python/#tracemalloc","title":"Tracemalloc","text":"<p>Tracemalloc is part of the CPython interpreter, and tracks allocations and frees. It's implemented efficiently and runs with relatively low overhead. It also allows the user to compare memory in different points in time to help identify issues.</p> <p>Tracemalloc is disabled by default, and only requires the user to enable a flag in the agent config: <pre><code>tracemalloc_debug: true\n</code></pre></p> <p>Note:One important caveat with regard to enabling the Tracemalloc feature is that it will reduce the number of check runners to 1. This is enforced by the Agent because otherwise the allocations of multiple checks begin to overlap in time making debugging the Tracemalloc output difficult. Imposing a single runner ensures Python checks are executed sequentially producing a more sensible output for debugging purposes.</p> <p>Once this feature is enabled, the metric<code>datadog.agent.profile.memory.check_run_alloc</code> will begin populating in Datadog. The metric is basic and only reflects the memory allocated by a check over time, in each check run, but it is still helpful for identifying regressions and leaks. The metric itself has two tags associated with it:</p> <ul> <li><code>check_name</code></li> <li><code>check_version</code></li> </ul> <p>The two should help identify the sources of leaks and memory usage regressions as well as what version they were introduced in.</p> <p>For a more granular control of how tracemalloc runs, there are an additional set of flags you may want to apply to your check's config on a check by check basis via their respective config files, by using the following directives in the <code>init_config</code> section:</p> <ul> <li><code>frames</code>: the number of stack frames to consider. Please note that this is the total number of frames considered, not the depth of the call-tree. Therefore, in some cases, you may need to set this value to a considerably high value to get a good enough understanding of how your agent is behaving. Default: 100.</li> <li><code>gc</code>: whether or not to run the garbage collector before each snapshot to remove noise. Garbage collections will not run by default (?) while tracemalloc is in action. That is to allow us to more easily identify sources of allocations without the interference of the GC. Note that the GC is not permanently disabled, this is only enforced during the check run while tracemalloc is tracking allocations. Default: disabled.</li> <li><code>combine</code>: whether or not to aggregate over all traceback frames. useful only to tell which particular usage of a function triggered areas of interest.</li> <li><code>sort</code>: what to group results by between: <code>lineno</code> | <code>filename</code> | <code>traceback</code>. Default: <code>lineno</code>.</li> <li><code>limit</code>: the maximum number of sorted results to show. Default: 30.</li> <li><code>diff</code>: how to order diff results between:<ul> <li><code>absolute</code>: absolute value of the difference between consecutive snapshots. Default.</li> <li><code>positive</code>: same as absolute, but memory increases will be shown first.</li> </ul> </li> <li><code>filters</code>: comma-separated list of file path glob patterns to filter by.</li> <li><code>unit</code>: the binary unit to represent memory usage (kib, mb, etc.). Default: dynamic.</li> <li><code>verbose</code>: whether or not to include potentially noisy sources. Default: false.</li> </ul> <p>You may also want to run tracemalloc and take a look at the actual debug information generated by the feature for a particular check, beyond just metrics. To do this you can resort to the check command and its optional <code>-m</code> flag. Running a check as follows will produce detailed memory allocation output for the check: <pre><code>sudo -u dd-agent -- datadog-agent check &lt;foo_check&gt; -m\n</code></pre></p> <p>That will print out some memory information to screen, for instance: <pre><code>#1: python3.7/abc.py:143: 10.69 KiB\n    return _abc_subclasscheck(cls, subclass)\n\n#2: simplejson/decoder.py:400: 6.84 KiB\n    return self.scan_once(s, idx=_w(s, idx).end())\n\n#3: go_expvar/go_expvar.py:142: 4.85 KiB\n    metric_tags = list(metric.get(TAGS, []))\n\n#4: go_expvar/go_expvar.py:241: 4.45 KiB\n    results.extend(self.deep_get(new_content, keys[1:], traversed_path + [str(new_key)]))\n\n    ...\n</code></pre></p> <p>But will also store the profiling information for futher inspection if necessary.</p> <p>There are additional hidden flags available when performing the memory profiling. Those flags map directly to the configuration options described above and will define and override the tracemalloc behavior. Because these flags are hidden and not meant for the end-user they will not be listed when issuing a <code>datadog-agent check --help</code> command. The command flags are:</p> <ul> <li><code>-m-frames</code></li> <li><code>-m-gc</code></li> <li><code>-m-combine</code></li> <li><code>-m-sort</code></li> <li><code>-m-limit</code></li> <li><code>-m-diff</code></li> <li><code>-m-filters</code></li> <li><code>-m-unit</code></li> <li><code>-m-verbose</code></li> </ul> <p>Additionally there's other command switch: - <code>-m-dir</code>: an existing directory in which to store memory profiling data, ignoring clean-up.</p> <p>The directory above must be writable by the user running the agent, typically the <code>dd-agent</code> user. Once the check command completes, you will be able to find the memory profile files created in the corresponding directory for your delight and careful inspection :)</p>"},{"location":"how-to/test/e2e/","title":"Running E2E tests","text":"<p>End-to-End (E2E) tests validate complete user workflows in production-like environments with real infrastructure and external services. The Datadog Agent uses the test-infra-definitions framework to provision and manage test environments. Tests are stored in the test/new-e2e folder.</p>"},{"location":"how-to/test/e2e/#prerequisites","title":"Prerequisites","text":"<p>Datadog Employees Only</p> <p>E2E testing requires access to Datadog's internal cloud infrastructure and is currently limited to Datadog employees. This limitation is temporary and may be expanded in the future.</p>"},{"location":"how-to/test/e2e/#software-requirements","title":"Software Requirements","text":"<p>Before running E2E tests, ensure you have the following installed:</p> <ul> <li>Go 1.22 or later</li> <li>Python 3.9+</li> <li>dda tooling - Install by following the development requirements</li> </ul>"},{"location":"how-to/test/e2e/#cloud-provider-setup","title":"Cloud Provider Setup","text":""},{"location":"how-to/test/e2e/#aws-configuration","title":"AWS Configuration","text":"<p>E2E tests require access to the <code>agent-sandbox</code> AWS account:</p> <ol> <li>Role Access: Ensure you have the <code>account-admin</code> role on the <code>agent-sandbox</code> account</li> <li>AWS Keypair: Set up an existing AWS keypair for your account</li> <li>Authentication: Validate your connection with login using aws-vault:    <pre><code>aws-vault login sso-agent-sandbox-account-admin\n</code></pre></li> </ol>"},{"location":"how-to/test/e2e/#gcp-configuration-for-gke-tests","title":"GCP Configuration (for GKE tests)","text":"<p>If you plan to run tests on Google Kubernetes Engine:</p> <ol> <li>Install GKE Plugin: Install the GKE authentication plugin</li> <li>PATH Configuration: Add the plugin to your system PATH</li> <li>Authentication: Validate your connection with GCP authentication:    <pre><code>gcloud auth application-default login\n</code></pre></li> </ol>"},{"location":"how-to/test/e2e/#environment-configuration","title":"Environment Configuration","text":"<p>Set up the required environment variables:</p> <ol> <li>Pulumi Configuration: Set <code>PULUMI_CONFIG_PASSPHRASE</code> in your terminal rc file (<code>.bashrc</code>, <code>.zshrc</code>, etc.)    <pre><code>export PULUMI_CONFIG_PASSPHRASE=\"your-random-password\"\n</code></pre></li> </ol> <p>Tip</p> <p>Generate a secure random password using 1Password or your preferred password manager.    You can use <code>security</code> on macOS to safely get this password.</p>"},{"location":"how-to/test/e2e/#setting-up-the-development-environment","title":"Setting Up the Development Environment","text":"<p>E2E tests should be run within a developer environment to ensure consistency and proper isolation.</p>"},{"location":"how-to/test/e2e/#start-a-developer-environment","title":"Start a Developer Environment","text":"<ol> <li> <p>Clone the repository (if using local checkout):    <pre><code>git clone https://github.com/DataDog/datadog-agent.git\ncd datadog-agent\n</code></pre></p> </li> <li> <p>Start the environment The following example is for an amd64 environment:    <pre><code>dda env dev start --id devenv-amd --arch amd64\n</code></pre></p> </li> </ol> <p>Or use a remote clone for better isolation:    <pre><code>dda env dev start --clone\n</code></pre></p> <ol> <li>Enter the environment shell:    <pre><code>dda env dev shell --id devenv-amd\n</code></pre></li> </ol> <p>For detailed information about developer environments, see the Using developer environments tutorial.</p>"},{"location":"how-to/test/e2e/#running-e2e-tests_1","title":"Running E2E Tests","text":""},{"location":"how-to/test/e2e/#basic-test-execution","title":"Basic Test Execution","text":"<p>E2E tests are located in the <code>test/new-e2e/</code> directory. To run a basic test use the invoke task <code>new-e2e-tests.run</code>, specifying a target folder relative to <code>test/new-e2e/</code>:</p> <pre><code># Run a simple VM test\ndda inv new-e2e-tests.run --targets=./examples --run=^TestVMSuite$\n</code></pre> <p>Replace ./examples with your subfolder. This also supports the golang testing flag --run and --skip to target specific tests using go test syntax. See go help testflag for details.</p> <pre><code>inv new-e2e-tests.run --targets=./examples --run=TestMyLocalKindSuite/TestClusterAgentInstalled\n</code></pre> <p>You can also run it with go test, from test/new-e2e <pre><code>cd test/new-e2e &amp;&amp; go test ./examples -timeout 0 -run=^TestVMSuite$\n</code></pre></p> <p>While developing a test you might want to keep the remote instance alive to iterate faster. You can skip the resources deletion using dev mode with the environment variable <code>E2E_DEV_MODE</code>. You can force this in the terminal <pre><code>E2E_DEV_MODE=true inv -e new-e2e-tests.run --targets ./examples --run=^TestVMSuite$\n</code></pre> or for instance add it in the <code>go.testEnvVars</code> if you are using a VSCode-based IDE <pre><code>\"go.testEnvVars\": {\n  \"E2E_DEV_MODE\": \"true\",\n}, \n</code></pre></p>"},{"location":"how-to/test/e2e/#test-with-local-agent-packages","title":"Test with Local Agent Packages","text":"<p>Limitations</p> <p>type: warning</p> <p>Local packaging is curently limited to DEB packages, only for Linux and Macos computers. This method relies on updating an existing agent package with the local Go binaries. As a consequence, this is incompatible with tests related to the agent packaging or the python integration.</p> <p>From a developer environment (see Using developer environments), you can create the agent package with your local code using: <pre><code>dda inv omnibus.build-repackaged-agent\n</code></pre></p> <p>You can then execute your E2E tests with the associated command: <pre><code># Run tests with a specific agent version\ndda inv new-e2e-tests.run --targets ./examples --run TestVMSuiteEx5 --local-package $(pwd)/omnibus\n</code></pre></p> <p>Make sure to replace <code>examples</code> with the package you want to test and to target the test you want to run with <code>--run</code>.</p>"},{"location":"how-to/test/e2e/#test-with-local-agent-image","title":"Test with Local Agent Image","text":"<p>Limitations</p> <p>type: warning</p> <p>This method relies on updating an existing Agent image with the local Go binaries. It only works for Docker images and must be considered as a solution for testing only.</p> <p>Build the Agent binary and the Docker image, using this command: <pre><code>dda inv [--core-opts] agent.hacky-dev-image-build [--base-image=STRING --push --signed-pull --target-image=STRING]\n</code></pre></p> <p>The command uses <code>dda inv agent.build</code> to generate the Go binaries. The generated image embeds this binary, a debugger and auto-completion for the agent commands. By default, the image is names <code>agent</code> unless you override it with the <code>--target-image</code> option.</p> <p>Then push the image to a registry: <pre><code># Login to ECR\naws-vault exec sso-agent-sandbox-account-admin -- \\\naws ecr get-login-password --region us-east-1 | \\\ndocker login --username AWS --password-stdin 376334461865.dkr.ecr.us-east-1.amazonaws.com\n# Push the image\ndocker push 376334461865.dkr.ecr.us-east-1.amazonaws.com/agent-e2e-tests:$USER\n</code></pre></p> <p>And finally, execute your E2E tests with the associated command: <pre><code># Run Ubuntu tests\ninv -e new-e2e-tests.run --targets ./tests/containers \\\n  --run TestDockerSuite/TestDSDWithUDP \\\n  --agent-image 376334461865.dkr.ecr.us-east-1.amazonaws.com/agent-e2e-tests:$USER\n</code></pre></p>"},{"location":"how-to/test/e2e/#test-framework-usage","title":"Test Framework Usage","text":""},{"location":"how-to/test/e2e/#environment-provisioning","title":"Environment Provisioning","text":"<p>E2E tests use Pulumi-based provisioning to create real infrastructure:</p> <pre><code>package examples\n\nimport (\n    \"testing\"\n\n    \"github.com/DataDog/datadog-agent/test/e2e-framework/testing/e2e\"\n    \"github.com/DataDog/datadog-agent/test/e2e-framework/testing/environments\"\n    awshost \"github.com/DataDog/datadog-agent/test/e2e-framework/testing/provisioners/aws/host\"\n)\n\ntype vmSuite struct {\n    e2e.BaseSuite[environments.Host]\n}\n\nfunc TestVMSuite(t *testing.T) {\n    suiteParams := []e2e.SuiteOption{\n        e2e.WithProvisioner(awshost.ProvisionerNoAgentNoFakeIntake()),\n    }\n\n    e2e.Run(t, &amp;vmSuite{}, suiteParams...)\n}\n</code></pre>"},{"location":"how-to/test/e2e/#available-provisioners","title":"Available Provisioners","text":"<p>The framework provides several provisioners for different scenarios:</p> <ul> <li>AWS Host: <code>awshost.Provisioner*()</code> - Provision EC2 instances</li> <li>Kubernetes: <code>eks.Provisioner*()</code> - Provision EKS clusters</li> <li>Docker: Container-based provisioning</li> <li>Multi-platform: Cross-platform test scenarios</li> </ul>"},{"location":"how-to/test/e2e/#test-validation","title":"Test Validation","text":"<p>E2E tests should validate complete workflows:</p> <pre><code>func (v *vmSuite) TestAgentInstallation() {\n    vm := v.Env().RemoteHost\n\n    // Install agent\n    _, err := vm.Execute(\"sudo apt-get install datadog-agent\")\n    v.Require().NoError(err)\n\n    // Verify agent is running\n    out, err := vm.Execute(\"sudo systemctl status datadog-agent\")\n    v.Require().NoError(err)\n    v.Require().Contains(out, \"active (running)\")\n\n    // Validate metric submission\n    v.Eventually(func() bool {\n        return v.Env().FakeIntake.GetMetricCount() &gt; 0\n    }, 30*time.Second, 1*time.Second)\n}\n</code></pre>"},{"location":"how-to/test/e2e/#test-categories-and-scenarios","title":"Test Categories and Scenarios","text":""},{"location":"how-to/test/e2e/#installation-and-deployment-tests","title":"Installation and Deployment Tests","text":"<ul> <li>Fresh installation on clean systems</li> <li>Package manager installations (APT, YUM, MSI)</li> <li>Container deployment validation</li> <li>Kubernetes operator deployment</li> </ul>"},{"location":"how-to/test/e2e/#upgrade-and-migration-tests","title":"Upgrade and Migration Tests","text":"<ul> <li>Agent version upgrades</li> <li>Configuration migration</li> <li>Rollback scenarios</li> <li>Zero-downtime upgrades</li> </ul>"},{"location":"how-to/test/e2e/#platform-integration-tests","title":"Platform Integration Tests","text":"<ul> <li>Cloud provider integrations (AWS, Azure, GCP)</li> <li>Container runtime compatibility (Docker, containerd, CRI-O)</li> <li>Kubernetes version compatibility</li> <li>Operating system support validation</li> </ul>"},{"location":"how-to/test/e2e/#performance-and-scale-tests","title":"Performance and Scale Tests","text":"<ul> <li>High-throughput metric collection</li> <li>Resource consumption validation</li> <li>Memory leak detection</li> <li>Long-running stability tests</li> </ul>"},{"location":"how-to/test/e2e/#security-and-compliance-tests","title":"Security and Compliance Tests","text":"<ul> <li>Security configuration validation</li> <li>Compliance framework testing</li> <li>Permission and access control verification</li> <li>Secure communication validation</li> </ul>"},{"location":"how-to/test/e2e/#best-practices","title":"Best Practices","text":""},{"location":"how-to/test/e2e/#test-design","title":"Test Design","text":"<ul> <li>Single Responsibility: Each test should validate one specific workflow</li> <li>Clear Assertions: Use descriptive assertion messages</li> <li>Proper Timeouts: Set appropriate timeouts for operations</li> <li>Resource Management: Always clean up created resources</li> </ul>"},{"location":"how-to/test/e2e/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Parallel Execution: Design tests to run in parallel when possible</li> <li>Resource Efficiency: Reuse infrastructure when appropriate</li> <li>Test Duration: Keep individual tests under 10 minutes when possible</li> </ul>"},{"location":"how-to/test/e2e/#maintenance","title":"Maintenance","text":"<ul> <li>Regular Updates: Keep test environments updated with latest agent versions</li> <li>Documentation: Document test scenarios and expected outcomes</li> <li>Monitoring: Monitor test execution times and failure rates</li> <li>Version Compatibility: Test against supported platform versions</li> </ul>"},{"location":"how-to/test/e2e/#see-also","title":"See Also","text":"<ul> <li>Test Categories - Understanding different test types</li> <li>Unit Testing - Running unit tests</li> <li>Using Developer Environments - Setting up development environments</li> <li>test-infra-definitions - Infrastructure provisioning framework</li> </ul>"},{"location":"how-to/test/static-analysis/","title":"How to perform static analysis","text":"<p>CI enforces static analysis checks for code, configuration, documentation, and more.</p>"},{"location":"how-to/test/static-analysis/#go","title":"Go","text":"<p>Go code can be analyzed with the <code>dda inv linter.go</code> command. This uses golangci-lint which is an aggregator for several linters.</p> <p>The configuration is defined in the .golangci.yml file. The <code>linters</code> key defines the list of linters we enable.</p> <p>Tip</p> <p>You can ignore linter issues on specific lines of code with the nolint directive.</p>"},{"location":"how-to/test/static-analysis/#python","title":"Python","text":"<p>The <code>dda inv linter.python</code> command performs analysis on Python code. This uses Ruff for linting and formatting, and also (for now) Vulture to find unused code.</p> <p>Tip</p> <p>Ruff supports several ways to suppress errors in your code.</p>"},{"location":"how-to/test/static-analysis/#other","title":"Other","text":"<p>All analysis tasks are prefixed by <code>linter.</code> and may be shown by running <code>dda inv --list</code>.</p>"},{"location":"how-to/test/unit/","title":"How to run unit tests","text":"<p>The <code>dda inv test</code> command runs Go tests and is a thin wrapper around gotestsum.</p>"},{"location":"how-to/test/unit/#test-selection","title":"Test selection","text":"<p>The Go module to test may be selected with the <code>-m</code>/<code>--module</code> flag using a relative path, defaulting to <code>.</code>.</p> <p>The <code>-t</code>/<code>--targets</code> flag is used to select the targets to test using a comma-separated list of relative paths within the given module. For example, the following command runs tests for the <code>pkg/collector/check</code> and <code>pkg/aggregator</code> root packages.</p> <pre><code>dda inv test --targets=pkg/collector/check,pkg/aggregator\n</code></pre> <p>Note</p> <p>If no module nor targets are set then the tests for all modules and targets are executed, which may be time-consuming.</p>"},{"location":"how-to/test/unit/#race-detection","title":"Race detection","text":"<p>The <code>-r</code>/<code>--race</code> flag enables Go's built-in data race detector.</p>"},{"location":"reference/builds/components/","title":"Agent components","text":""},{"location":"reference/builds/components/#agent-binaries","title":"Agent binaries","text":"<p>The \"Agent\" is not distributed as a single binary. Instead, running an Agent on a given host will usually involve multiple processes communicating with each other, spawned from different binaries<sup>1</sup>.</p> <p>These binaries have a good amount of code shared between them, but are all buildable individually. Here is the exhaustive list:</p> <ul> <li><code>agent</code></li> <li><code>process-agent</code></li> <li><code>trace-agent</code></li> <li><code>cluster-agent</code></li> <li><code>security-agent</code></li> <li><code>system-probe</code></li> <li><code>jmxfetch</code></li> </ul> <p>Info</p> <p>Every binary is built from the same codebase. By leveraging Go build constraints, we end up compiling different parts of the source code for each binary.</p>"},{"location":"reference/builds/components/#agent-features","title":"Agent \"features\"","text":"<p>The Agent codebase makes heavy use of Go build constraints to dynamically include or exclude some parts of the source code during the build process.</p> <p>Here is a list of usable \"tags\" that you can pass during the build process to customize your build. This list is not exhaustive !</p> <ul> <li><code>apm</code>: make the APM agent execution available. (1)</li> <li><code>consul</code>: enable consul as a configuration store.</li> <li><code>python</code>: embed the Python interpreter.</li> <li><code>docker</code>: add Docker support (required by AutoDiscovery).</li> <li><code>ec2</code>: enable EC2 hostname detection and metadata collection.</li> <li><code>etcd</code>: enable Etcd as a configuration store.</li> <li><code>gce</code>: enable GCE hostname detection and metadata collection.</li> <li><code>jmx</code>: enable the JMX-fetch bridge.</li> <li><code>kubelet</code>: enable kubelet tag collection.</li> <li><code>log</code>: enable the log agent.</li> <li><code>process</code>: enable the process agent.</li> <li><code>zk</code>: enable Zookeeper as a configuration store.</li> <li><code>zstd</code>: use Zstandard instead of Zlib.</li> <li><code>systemd</code>: enable systemd journal log collection.</li> <li><code>netcgo</code>: force the use of the CGO resolver. This will also have the effect of making the binary non-static.</li> <li><code>secrets</code>: enable secrets support in configuration files (see documentation here).</li> <li><code>clusterchecks</code>: enable cluster-level checks.</li> <li><code>cri</code> : add support for the CRI integration.</li> <li><code>containerd</code>: add support for the containerd integration.</li> <li><code>kubeapiserver</code>: enable interaction with Kubernetes API server (required by the cluster Agent).</li> </ul> <ol> <li>Note that the trace agent needs to be built separately. For more information on the trace agent, see the official docs.</li> </ol> <p>Note</p> <p>You might need to provide some extra dependencies in your dev environment to build with certain features (see manual setup).</p> <ol> <li> <p>This is not always the case: the Agent can, as an option, combine multiple binaries into a single one to reduce disk space usage. See here for more info.\u00a0\u21a9</p> </li> </ol>"},{"location":"reference/images/builders/","title":"Build image reference","text":"<p>All non-macOS release artifacts are produced within container images that are hosted on the Datadog Docker Hub.</p> <p>Note</p> <p>These images are not meant to be used directly outside of CI scenarios. Instead, use the developer environments to build and test the Agent.</p>"},{"location":"reference/images/builders/#linux","title":"Linux","text":"<p>The Linux builder, available as datadog/agent-buildimages-linux, is used to build all Linux release artifacts except for the RPM distribution (temporary limitation). It ships a custom toolchain for cross-compiling the Agent.</p> <p>The image is based on Ubuntu 24.04 and supports both the <code>amd64</code> and <code>arm64</code> host architectures.</p>"},{"location":"reference/images/builders/#windows","title":"Windows","text":"<p>The Windows builder, available as datadog/agent-buildimages-windows_x64, is used to build all Windows release artifacts.</p> <p>The image is based on Windows Server 2022 and only supports the <code>amd64</code> host architecture.</p>"},{"location":"reference/images/builders/#macos","title":"macOS","text":"<p>Due to licensing restrictions, we are unable to maintain a dedicated macOS container image. macOS release artifacts are built on a custom AMI that is not publicly available. Contributors must manually set up their local environment in order to build macOS-specific release artifacts.</p>"},{"location":"reference/images/dev/","title":"Developer environment reference","text":"<p>The developer environments are container images that use the builders as a base image. These images are only intended for use by the <code>dda env dev</code> command group.</p> <p>There is currently only one image available:</p> <ul> <li><code>datadog/agent-dev-env-linux</code>, based on the Linux builder</li> </ul>"},{"location":"reference/images/dev/#editors","title":"Editors","text":"<p>Images come with the following editors ready for remote development via SSH.</p> <ul> <li>Visual Studio Code</li> <li>Cursor</li> </ul> Pre-installed extensions VS Code/Cursor <ul> <li>bazelbuild.vscode-bazel</li> <li>charliermarsh.ruff</li> <li>dbaeumer.vscode-eslint</li> <li>github.copilot</li> <li>github.vscode-github-actions</li> <li>golang.go</li> <li>ms-azuretools.vscode-docker</li> <li>ms-python.python</li> <li>ms-vscode.cmake-tools</li> <li>redhat.vscode-xml</li> <li>redhat.vscode-yaml</li> <li>rust-lang.rust-analyzer</li> <li>timonwong.shellcheck</li> <li>yzhang.markdown-all-in-one</li> <li>zxh404.vscode-proto3</li> </ul>"},{"location":"reference/images/dev/#tools","title":"Tools","text":"<p>The following is a non-exhaustive list of available tools.</p> <ul> <li><code>ambr</code> - find and replace like <code>sed</code> but with interactivity</li> <li><code>bat</code> - show file contents like <code>cat</code></li> <li><code>bazel</code> - Bazelisk, ensuring the correct version of the Bazel build system is used</li> <li><code>btm</code> - system monitor like <code>top</code></li> <li><code>dda</code> - Datadog Agent development CLI</li> <li><code>docker</code> - Docker engine, including the <code>buildx</code> and <code>compose</code> plugins</li> <li><code>eza</code> - list files like <code>ls</code></li> <li><code>fd</code> - find files like <code>find</code></li> <li><code>fzf</code> - fuzzy finder</li> <li><code>gfold</code> - Git status viewer for multiple repositories</li> <li><code>gitui</code> - Git terminal UI</li> <li><code>hyperfine</code> - benchmarking tool</li> <li><code>kubectl</code> - Kubernetes control plane CLI</li> <li><code>jq</code> - JSON processor</li> <li><code>pdu</code> - show disk usage like <code>du</code></li> <li><code>procs</code> - process viewer like <code>ps</code></li> <li><code>rg</code> - search file contents like <code>grep</code></li> <li><code>yazi</code> - file manager UI</li> </ul>"},{"location":"reference/images/dev/#git","title":"Git","text":"<p>Git is configured to use SSH for authentication and, upon start, will use the <code>user.name</code> and <code>user.email</code> global settings from your local machine.</p> <p>Each of the following subcommands are available via <code>git &lt;subcommand&gt;</code>.</p> <ul> <li><code>dd-clone</code> - Performs a shallow clone <sup>1</sup> of a Datadog repository to the proper managed location. The first argument is the repository name and a second optional argument is the branch name. Example invocations:<ul> <li><code>git dd-clone datadog-agent</code></li> <li><code>git dd-clone datadog-agent user/feature</code></li> </ul> </li> <li><code>dd-switch</code> - Emulates the behavior of <code>git switch</code> but smart enough to handle shallow clones. The branch name is the only argument.</li> </ul> <p>Delta is the default pager which provides, as an example, syntax highlighting for <code>git diff</code> and <code>git log</code> commands.</p>"},{"location":"reference/images/dev/#repositories","title":"Repositories","text":"<p>Images assume repositories will be cloned to <code>~/repos</code>. The <code>dd-clone</code> Git extension will clone repositories to this location and <code>gfold</code> is pre-configured to look for repositories in this location.</p>"},{"location":"reference/images/dev/#shells","title":"Shells","text":"<p>Images come with shells based on their platform e.g. Zsh for Linux and PowerShell for Windows. Every image comes with Nushell as well.</p> <p>All shells are pre-configured with Starship prompt and any local configuration will be copied to the container upon start.</p>"},{"location":"reference/images/dev/#fonts","title":"Fonts","text":"<p>Images come with the following fonts.</p> <ul> <li>FiraCode</li> <li>CascadiaCode</li> </ul> <p>All fonts have Nerd Font glyphs, and Noto Emoji is installed for emoji support.</p>"},{"location":"reference/images/dev/#ports","title":"Ports","text":"<p>Images expose the following ports.</p> <ul> <li><code>22</code> for SSH access</li> <li><code>9000</code> for the <code>dda</code> MCP server</li> </ul> <ol> <li> <p>A shallow clone by default matches our use case of ephemeral developer environments. If persistence is desired then developers can easily convert the shallow clone to a full clone by running <code>git fetch --unshallow</code>. More information:</p> <ul> <li>Git clone: a data-driven study on cloning behaviors</li> <li>Get up to speed with partial clone and shallow clone</li> </ul> <p>\u21a9</p> </li> </ol>"},{"location":"setup/manual/","title":"Set up your local machine manually","text":"<p>These instructions are maintained on a best-effort basis. Prefer using the developer environment instead.</p>"},{"location":"setup/manual/#prerequisites","title":"Prerequisites","text":"<p>Be sure that you already set up the development requirements.</p> <p>Warning</p> <p>Building the Agent for Windows requires using the build image. Setting up a local Windows environment is not officially supported.</p>"},{"location":"setup/manual/#build-tools","title":"Build tools","text":"<p>CMake version 3.15 or later and a C++ compiler are required for building the Agent.</p>"},{"location":"setup/manual/#python","title":"Python","text":"<p>The Agent embeds a full-fledged CPython interpreter so it requires the development files to be available in the dev env. The Agent can embed Python 3, you will need development files for the version you want to support.</p> <p>If you're on OSX/macOS, install 3.12 with Homebrew:</p> <pre><code>brew install python@3.12\n</code></pre> <p>On Linux, depending on the distribution, you might need to explicitly install the development files, for example on Ubuntu:</p> <pre><code>sudo apt-get install python3.12-dev\n</code></pre> <p>On Windows, install 3.12 via the official installer brings along all the development files needed:</p> <p>Warning</p> <p>If you don't use one of the Python versions that are explicitly supported, you may have problems running the built Agent's Python checks, especially if using a virtualenv. At this time, only Python 3.12 is confirmed to work as expected in the development environment.</p>"},{"location":"setup/manual/#python-dependencies","title":"Python Dependencies","text":"<p>To protect and isolate your system-wide python installation, a python virtual environment is highly recommended (though optional). It will help keep a self-contained development environment and ensure a clean system Python.</p> <p>Note</p> <p>Due to the way some virtual environments handle executable paths (e.g. <code>python -m venv</code>), not all virtual environment options will be able to run the built Agent correctly. At this time, the only confirmed virtual environment creator that is known for sure to work is <code>virtualenv</code>.</p> <ul> <li>Install the virtualenv module:     <pre><code>python3 -m pip install virtualenv\n</code></pre></li> <li>Create the virtual environment:     <pre><code>virtualenv $GOPATH/src/github.com/DataDog/datadog-agent/venv\n</code></pre></li> <li>Activate the virtualenv (OS-dependent). This must be done for every new terminal before you start.</li> </ul> <p>If using virtual environments when running the built Agent, you may need to override the built Agent's search path for Python check packages using the <code>PYTHONPATH</code> variable (your target path must have the pre-requisite core integration packages installed though).</p> <pre><code>PYTHONPATH=\"./venv/lib/python3.12/site-packages:$PYTHONPATH\" ./agent run ...\n</code></pre> <p>See also some notes in ./checks about running custom python checks.</p>"},{"location":"setup/manual/#golang","title":"Golang","text":"<p>You must install Golang version <code>1.25.7</code> or later. Make sure that <code>$GOPATH/bin</code> is in your <code>$PATH</code>, otherwise tooling cannot use any additional tool it might need.</p> <p>Note</p> <p>Versions of Golang that aren't an exact match to the version specified in our build images (see e.g. here) may not be able to build the agent and/or the rtloader binary properly.</p>"},{"location":"setup/manual/#installing-tools","title":"Installing tools","text":"<p>From the root of <code>datadog-agent</code>, run <code>dda inv install-tools</code> to install go tooling. This uses <code>go</code> to install the necessary dependencies.</p>"},{"location":"setup/manual/#system-or-embedded","title":"System or Embedded?","text":"<p>When working on the Agent codebase you can choose among two different ways to build the binary, informally named System and Embedded builds. For most contribution scenarios you should rely on the System build (the default) and use the Embedded one only for specific use cases. Let's explore the differences.</p>"},{"location":"setup/manual/#system-build","title":"System build","text":"<p>System builds use your operating system's standard system libraries to satisfy the Agent's external dependencies. Since, for example, macOS 10.11 may provide a different version of Python than macOS 10.12, system builds on each of these platforms may produce different Agent binaries. If this doesn't matter to you\u2014perhaps you just want to contribute a quick bugfix\u2014do a System build; it's easier and faster than an Embedded build. System build is the default for all build and test tasks, so you don't need to configure anything there. But to make sure you have system copies of all the Agent's dependencies, skip the Embedded build section below and read on to see how to install them via your usual package manager (apt, yum, brew, etc).</p>"},{"location":"setup/manual/#embedded-build","title":"Embedded build","text":"<p>Embedded builds download specifically-versioned dependencies and compile them locally from sources. We run Embedded builds to create Datadog's official Agent releases (i.e. RPMs, debs, etc), and while you can run the same builds while developing locally, the process is as slow as it sounds. Hence, you should only use them when you care about reproducible builds. For example:</p> <ul> <li>you want to build an agent binary that can be used as-is to replace the binary of an existing agent installation</li> <li>some dependencies are not available on your system</li> <li>you're working or debugging at a very low level: let's say you're adding a function to the Python bindings, you want to make sure you're using the exact same versions of Python as the official Agent packages</li> </ul> <p>Embedded builds rely on Omnibus to download and build dependencies, so you need a recent <code>ruby</code> environment with <code>bundler</code> installed. See How to build agent distribution packages for more details.</p>"},{"location":"setup/manual/#systemd","title":"Systemd","text":"<p>The agent is able to collect systemd journal logs using a wrapper on the systemd utility library.</p> <p>On Ubuntu/Debian:</p> <pre><code>sudo apt-get install libsystemd-dev\n</code></pre> <p>On Redhat/CentOS:</p> <pre><code>sudo yum install systemd-devel\n</code></pre>"},{"location":"setup/manual/#doxygen","title":"Doxygen","text":"<p>We use Doxygen to generate the documentation for the <code>rtloader</code> part of the Agent.</p> <p>To generate it (using the <code>dda inv rtloader.generate-doc</code> command), you'll need to have Doxygen installed on your system and available in your <code>$PATH</code>. You can compile and install Doxygen from source with the instructions available here. Alternatively, you can use already-compiled Doxygen binaries from here.</p> <p>To get the dependency graphs, you may also need to install the <code>dot</code> executable from graphviz and add it to your <code>$PATH</code>.</p>"},{"location":"setup/manual/#pre-commit-hooks","title":"Pre-commit hooks","text":"<p>It is optional but recommended to install <code>pre-commit</code> to run a number of checks done by the CI locally.</p>"},{"location":"setup/manual/#installation","title":"Installation","text":"<p>To install it, run:</p> <pre><code>python3 -m pip install pre-commit\nGOFLAGS=-buildvcs=false pre-commit install  # buildvcs avoids errors when getting go dependencies\n</code></pre> <p>The <code>shellcheck</code> pre-commit hook requires having the <code>shellcheck</code> binary installed and in your <code>$PATH</code>. To install it, run:</p> <pre><code>dda inv install-shellcheck --destination &lt;path&gt;\n</code></pre> <p>(by default, the shellcheck binary is installed in <code>/usr/local/bin</code>).</p>"},{"location":"setup/manual/#skipping-pre-commit","title":"Skipping <code>pre-commit</code>","text":"<p>If you want to skip <code>pre-commit</code> for a specific commit you can add <code>--no-verify</code> to the <code>git commit</code> command.</p>"},{"location":"setup/manual/#running-pre-commit-manually","title":"Running <code>pre-commit</code> manually","text":"<p>If you want to run one of the checks manually, you can run <code>pre-commit run &lt;check name&gt;</code>.</p> <p>You can run it on all files with the <code>--all-files</code> flag.</p> <pre><code>pre-commit run flake8 --all-files  # run flake8 on all files\n</code></pre> <p>See <code>pre-commit run --help</code> for further options.</p>"},{"location":"setup/manual/#setting-up-visual-studio-code-dev-container","title":"Setting up Visual Studio Code Dev Container","text":"<p>Tip</p> <p>Using the developer environment approach is recommended.</p> <p>Microsoft Visual Studio Code with the devcontainer plugin allow to use a container as remote development environment in vscode. It simplify and isolate the dependencies needed to develop in this repository.</p> <p>To configure the vscode editor to use a container as remote development environment you need to:</p> <ul> <li>Install the devcontainer plugin and the golang language plugin.</li> <li>Run the following command <code>dda inv vscode.setup-devcontainer --image \"&lt;image name&gt;\"</code>. This command will create the devcontainer configuration file <code>./devcontainer/devcontainer.json</code>.</li> <li>Start or restart your vscode editor.</li> <li>A pop-up should show-up to propose to \"reopen in container\" your workspace.</li> <li>The first start, it might propose you to install the golang plugin dependencies/tooling.</li> </ul>"},{"location":"setup/optional/","title":"Set up optional development features","text":""},{"location":"setup/optional/#tab-completion","title":"Tab completion","text":"<p>Completion is achieved by saving a script and then executing it as a part of your shell's startup sequence.</p> <p>Afterward, you'll need to start a new shell in order for the changes to take effect.</p> zshbashfish <p>Save the script somewhere:</p> <pre><code>_DDA_COMPLETE=zsh_source dda &gt; ~/.dda-complete.zsh\n</code></pre> <p>Source the file in <code>~/.zshrc</code>:</p> <pre><code>. ~/.dda-complete.zsh\n</code></pre> <p>Save the script somewhere:</p> <pre><code>_DDA_COMPLETE=bash_source dda &gt; ~/.dda-complete.bash\n</code></pre> <p>Source the file in <code>~/.bashrc</code> (or <code>~/.bash_profile</code> if on macOS):</p> <pre><code>. ~/.dda-complete.bash\n</code></pre> <p>Save the script in <code>~/.config/fish/completions</code>:</p> <pre><code>_DDA_COMPLETE=fish_source dda &gt; ~/.config/fish/completions/dda.fish\n</code></pre> <p>Tip</p> <p>There is also limited tab completion support for the legacy <code>invoke</code> tasks. Here's an example:</p> <pre><code>echo \"source &lt;(dda inv --print-completion-script zsh)\" &gt;&gt; ~/.zshrc\n</code></pre>"},{"location":"setup/optional/#pre-commit-hooks","title":"Pre-commit hooks","text":"<p>The CI runs a number of required checks using pre-commit and running those locally can speed up the development process.</p> <ol> <li> <p>Install <code>pre-commit</code> by running the following command.</p> <pre><code>dda self pip install pre-commit\n</code></pre> </li> <li> <p>Use <code>dda</code> to configure the pre-commit hooks.</p> <pre><code>dda inv setup.pre-commit\n</code></pre> </li> </ol>"},{"location":"setup/required/","title":"Set up development requirements","text":""},{"location":"setup/required/#tooling","title":"Tooling","text":"<p>The <code>dda</code> CLI is required in all aspects of development and must be available on <code>PATH</code>.</p>"},{"location":"setup/required/#package-managers","title":"Package managers","text":""},{"location":"setup/required/#homebrew","title":"Homebrew","text":"<p>Install the <code>dda</code> cask using Homebrew.</p> <pre><code>brew install --cask dda\n</code></pre> <p>You can upgrade to the latest version by running the following command.</p> <pre><code>brew upgrade --cask dda\n</code></pre>"},{"location":"setup/required/#winget","title":"WinGet","text":"<p>Install the <code>Datadog.dda</code> package using WinGet.</p> <pre><code>winget install --id Datadog.dda\n</code></pre> <p>You can upgrade to the latest version by running the following command.</p> <pre><code>winget upgrade --id Datadog.dda\n</code></pre> <p>Note</p> <p><code>dda</code> is the name of both the package and binary placed on PATH; the ID selection is for safety.</p>"},{"location":"setup/required/#installers","title":"Installers","text":"macOSWindows GUI installerCommand line installer <ol> <li>In your browser, download the <code>.pkg</code> file: dda-universal.pkg</li> <li>Run your downloaded file and follow the on-screen instructions.</li> <li>Restart your terminal.</li> <li>To verify that the shell can find and run the <code>dda</code> command in your <code>PATH</code>, use the following command.         <pre><code>$ dda --version\n0.31.0\n</code></pre></li> </ol> <ol> <li>Download the file using the <code>curl</code> command. The <code>-o</code> option specifies the file name that the downloaded package is written to. In this example, the file is written to <code>dda-universal.pkg</code> in the current directory.         <pre><code>curl -Lo dda-universal.pkg https://github.com/DataDog/datadog-agent-dev/releases/latest/download/dda-universal.pkg\n</code></pre></li> <li>Run the standard macOS <code>installer</code> program, specifying the downloaded <code>.pkg</code> file as the source. Use the <code>-pkg</code> parameter to specify the name of the package to install, and the <code>-target /</code> parameter for the drive in which to install the package. The files are installed to <code>/usr/local/dda</code>, and an entry is created at <code>/etc/paths.d/dda</code> that instructs shells to add the <code>/usr/local/dda</code> directory to. You must include sudo on the command to grant write permissions to those folders.         <pre><code>sudo installer -pkg ./dda-universal.pkg -target /\n</code></pre></li> <li>Restart your terminal.</li> <li>To verify that the shell can find and run the <code>dda</code> command in your <code>PATH</code>, use the following command.         <pre><code>$ dda --version\n0.31.0\n</code></pre></li> </ol> GUI installerCommand line installer <ol> <li>In your browser, download one the <code>.msi</code> files:<ul> <li>dda-x64.msi</li> </ul> </li> <li>Run your downloaded file and follow the on-screen instructions.</li> <li>Restart your terminal.</li> <li>To verify that the shell can find and run the <code>dda</code> command in your <code>PATH</code>, use the following command.         <pre><code>$ dda --version\n0.31.0\n</code></pre></li> </ol> <ol> <li> <p>Download and run the installer using the standard Windows <code>msiexec</code> program, specifying one of the <code>.msi</code> files as the source. Use the <code>/passive</code> and <code>/i</code> parameters to request an unattended, normal installation.</p> x64x86 <pre><code>msiexec /passive /i https://github.com/DataDog/datadog-agent-dev/releases/latest/download/dda-x64.msi\n</code></pre> <pre><code>msiexec /passive /i https://github.com/DataDog/datadog-agent-dev/releases/latest/download/dda-x86.msi\n</code></pre> </li> <li> <p>Restart your terminal.</p> </li> <li>To verify that the shell can find and run the <code>dda</code> command in your <code>PATH</code>, use the following command.         <pre><code>$ dda --version\n0.31.0\n</code></pre></li> </ol>"},{"location":"setup/required/#standalone-binaries","title":"Standalone binaries","text":"<p>After downloading the archive corresponding to your platform and architecture, extract the binary to a directory that is on your PATH and rename to <code>dda</code>.</p> macOSWindowsLinux <ul> <li>dda-aarch64-apple-darwin.tar.gz</li> <li>dda-x86_64-apple-darwin.tar.gz</li> </ul> <ul> <li>dda-x86_64-pc-windows-msvc.zip</li> <li>dda-i686-pc-windows-msvc.zip</li> </ul> <ul> <li>dda-aarch64-unknown-linux-gnu.tar.gz</li> <li>dda-x86_64-unknown-linux-gnu.tar.gz</li> <li>dda-x86_64-unknown-linux-musl.tar.gz</li> <li>dda-powerpc64le-unknown-linux-gnu.tar.gz</li> </ul>"},{"location":"setup/required/#upgrade","title":"Upgrade","text":"<p>You can upgrade to the latest version by running the following command.</p> <pre><code>dda self update\n</code></pre> <p>If you installed <code>dda</code> using a package manager, prefer its native upgrade mechanism.</p>"},{"location":"setup/required/#docker","title":"Docker","text":"<p>Docker is required for both running the developer environment and building images containing the Agent.</p> macOSWindowsLinux <ol> <li>Install Docker Desktop for Mac.</li> <li>Right-click the Docker taskbar item and update Preferences &gt; File Sharing with any locations you need to open.</li> </ol> <ol> <li>Install Docker Desktop for Windows.</li> <li>Right-click the Docker taskbar item and update Settings &gt; Shared Drives with any locations you need to open e.g. <code>C:\\</code>.</li> </ol> <p>Install Docker Desktop for your distribution:</p> UbuntuDebianFedoraArchRHEL <p>Docker Desktop for Ubuntu</p> <p>Docker Desktop for Debian</p> <p>Docker Desktop for Fedora</p> <p>Docker Desktop for Arch</p> <p>Docker Desktop for RHEL</p>"},{"location":"setup/required/#ssh","title":"SSH","text":"<p>Accessing and contributing to the Datadog Agent repositories requires using SSH. Your local SSH agent must be configured with a key that is added to your GitHub account.</p> <p>If these requirements are met, you can skip this section.</p>"},{"location":"setup/required/#key-generation","title":"Key generation","text":"<p>Run the following command to generate a new SSH key, replacing <code>&lt;EMAIL_ADDRESS&gt;</code> with one of the email addresses associated with your GitHub account. This email will be used for every Git commit you make to the Datadog Agent repositories.</p> <pre><code>ssh-keygen -t ed25519 -C \"&lt;EMAIL_ADDRESS&gt;\"\n</code></pre> <p>It should then ask you for the path in which to save the key. It's recommended to name the key <code>dda</code> and save it in the same directory as the default location.</p> <pre><code>Enter file in which to save the key (/root/.ssh/id_ed25519): /root/.ssh/dda\n</code></pre> <p>Finally, you will be asked to enter an optional passphrase.</p>"},{"location":"setup/required/#key-awareness","title":"Key awareness","text":"<p>Add the key from the previous step to your local SSH agent.</p> <pre><code>ssh-add /root/.ssh/dda\n</code></pre> <p>Running the following command should now display the key.</p> <pre><code>ssh-add -L\n</code></pre> <p>Follow GitHub's guide for adding the key to your GitHub account and test that it works. The path to the public key is the path to the key from the previous step with a <code>.pub</code> file extension e.g. <code>/root/.ssh/dda.pub</code>.</p>"},{"location":"setup/required/#next-steps","title":"Next steps","text":"<p>Follow the developer environment tutorial to get started.</p>"},{"location":"tutorials/dev/env/","title":"Using developer environments","text":"<p>Developer environments are preconfigured workspaces that provide everything required for contributing to the Agent code base, testing changes and building release artifacts. Usually, these are images that run using a container orchestrator like Docker or Podman.</p> <p>This tutorial will walk through how to use such environments with the <code>dda env dev</code> command group.</p> <p>Note</p> <p>This tutorial assumes you have already set up the development requirements.</p>"},{"location":"tutorials/dev/env/#overview","title":"Overview","text":"<p>A developer environment may be used either as an ephemeral or persistent workspace. Multiple environments can be used concurrently, with each using a different checkout of the Agent repository.</p> <p>To get an idea of upcoming interactions, run <code>dda env dev</code> and you should see the following output.</p> <pre><code>$ dda env dev\n\n Usage: dda env dev [OPTIONS] COMMAND [ARGS]...\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help  -h  Show this message and exit.                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 cache   Manage the cache                                             \u2502\n\u2502 code    Open a code editor for the developer environment             \u2502\n\u2502 gui     Access a developer environment through a graphical interface \u2502\n\u2502 remove  Remove a developer environment                               \u2502\n\u2502 run     Run a command within a developer environment                 \u2502\n\u2502 shell   Spawn a shell within a developer environment                 \u2502\n\u2502 show    Show the available developer environments                    \u2502\n\u2502 start   Start a developer environment                                \u2502\n\u2502 status  Check the status of a developer environment                  \u2502\n\u2502 stop    Stop a developer environment                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"tutorials/dev/env/#environment-selection","title":"Environment selection","text":"<p>When an environment is started, there are two options that permanently influence the environment until it is removed.</p> <ul> <li>The <code>--id</code> option sets the identifier that subsequent commands use to target the environment. All commands use the word <code>default</code> if no ID is provided. Environments of different types can have the same ID.</li> <li> <p>The <code>-t</code>/<code>--type</code> option selects the type of developer environment. The following types are available:</p> <ul> <li><code>linux-container</code> - This runs the Linux developer environment and is the default type on non-Windows systems.</li> </ul> <p>More types will be added in the future. All non-macOS cloud environments will simply use the developer environment containers.</p> </li> </ul> <p>Note</p> <p>The default type on Windows is <code>windows-container</code> but that is not yet implemented. As a temporary workaround, use the <code>linux-container</code> type with the <code>-t</code>/<code>--type</code> start command option or configure <code>dda</code> to use it as the default type for developer environments with the following command.</p> <pre><code>dda config set env.dev.default-type linux-container\n</code></pre>"},{"location":"tutorials/dev/env/#starting-environments","title":"Starting environments","text":"<p>You can start an environment with the <code>dda env dev start</code> command. All developer environment types support the following options.</p>"},{"location":"tutorials/dev/env/#repository-location","title":"Repository location","text":"<p>You may select where the code resides when starting an environment.</p>"},{"location":"tutorials/dev/env/#local-checkout","title":"Local checkout","text":"<p>The default behavior assumes a local checkout of the Agent repository. Clone the repo if you haven't already.</p> <pre><code>git clone https://github.com/DataDog/datadog-agent.git\n</code></pre> <p>Then enter the cloned directory and run the following command to start an environment.</p> <pre><code>dda env dev start\n</code></pre> <p>Warning</p> <p>There are two downsides to this approach.</p> <ol> <li>It's not possible to have multiple environments using a separate checkout of the Agent repository i.e. you inherit the limitations of a usual Git checkout.</li> <li>Bind mounts may experience performance issues, particularly for Git operations.</li> </ol>"},{"location":"tutorials/dev/env/#remote-clone","title":"Remote clone","text":"<p>You can start an environment with a fresh clone of the Agent repository by passing the <code>--clone</code> flag.</p> <pre><code>dda env dev start --clone\n</code></pre> <p>You can also configure <code>dda</code> to always clone repositories for developer environments with the following command.</p> <pre><code>dda config set env.dev.clone-repos true\n</code></pre> <p>This will perform a shallow clone of the default branch, <code>main</code>.</p>"},{"location":"tutorials/dev/env/#repository-selection","title":"Repository selection","text":"<p>Note</p> <p>This functionality has no effect when using a local checkout.</p> <p>The <code>-r</code>/<code>--repo</code> option selects the Datadog repositories to clone and may be supplied multiple times. By default, only the <code>datadog-agent</code> repository is chosen.</p> <pre><code>dda env dev start -r datadog-agent -r integrations-core\n</code></pre> <p>The first selected repository becomes the default location for the environment such as when running commands or entering a shell.</p>"},{"location":"tutorials/dev/env/#type-specific-options","title":"Type-specific options","text":"<p>Each developer environment type has additional options that are only available when using that type. For example, the <code>linux-container</code> type supports a <code>--no-pull</code> flag to disable the automatic pull of the latest image.</p> <p>The help text displays options for the default type. In order to see options for a specific type, use the <code>-t</code>/<code>--type</code> option before the help flag like in the following example.</p> <pre><code>dda env dev start -t linux-container -h\n</code></pre>"},{"location":"tutorials/dev/env/#environment-status","title":"Environment status","text":"<p>You can check the status of an environment by running the following command.</p> <pre><code>dda env dev status\n</code></pre> <p>This shows the environment's state and any extra information that may be useful for debugging.</p> <p>Environments have the following possible states.</p> <ul> <li><code>started</code> - The environment is running.</li> <li><code>stopped</code> - The environment is stopped.</li> <li><code>starting</code> - The environment is starting.</li> <li><code>stopping</code> - The environment is stopping.</li> <li><code>error</code> - The environment is in an error state.</li> <li><code>nonexistent</code> - The environment does not exist.</li> <li><code>unknown</code> - The environment is in an unknown state.</li> </ul>"},{"location":"tutorials/dev/env/#entering-a-shell","title":"Entering a shell","text":"<p>You can spawn a shell within an environment by running the following command.</p> <pre><code>dda env dev shell\n</code></pre> <p>This opens a shell within the first defined repository.</p>"},{"location":"tutorials/dev/env/#preparing-for-development","title":"Preparing for development","text":"<p>Run the following command in the environment's shell to install some remaining dependencies.</p> <pre><code>dda inv install-tools\n</code></pre> <p>If you're using a remote clone, some builds may fail due to the shallow cloning. To acquire all Git history, run the following command.</p> <pre><code>git fetch --unshallow\n</code></pre>"},{"location":"tutorials/dev/env/#testing","title":"Testing","text":"<p>Verify your setup by running some unit tests with the following command.</p> <pre><code>dda inv test --targets=pkg/aggregator\n</code></pre>"},{"location":"tutorials/dev/env/#editing-code","title":"Editing code","text":"<p>Exit the environment's shell or open a new session and run the following command locally.</p> <pre><code>dda env dev code\n</code></pre> <p>This opens one of the supported editors for the repository selected with the <code>-r</code>/<code>--repo</code> option, defaulting to the first defined repository. The editor may take a few moments to start up the first time it opens in an environment.</p> <p>The editor may be selected with the <code>-e</code>/<code>--editor</code> option or by configuring <code>dda</code> to use a specific editor for developer environments by default with the following command.</p> <pre><code>dda config set env.dev.editor cursor\n</code></pre> <p>To test functionality, create a file <code>test.txt</code> at the root of the repository and save it.</p>"},{"location":"tutorials/dev/env/#running-commands","title":"Running commands","text":"<p>You can run commands within an environment without entering a shell by passing arbitrary arguments to the <code>dda env dev run</code> command locally. Confirm the file <code>test.txt</code> was created by running the following command.</p> <pre><code>dda env dev run -- ls -l test.txt\n</code></pre> <p>The repository in which to run the command is determined by the <code>-r</code>/<code>--repo</code> option, defaulting to the first defined repository.</p>"},{"location":"tutorials/dev/env/#building","title":"Building","text":"<p>Let's build the default Agent by running the following command locally.</p> <pre><code>dda env dev run -- dda inv agent.build --build-exclude=systemd\n</code></pre> <p>Confirm that the binary was successfully built by running the following command.</p> <pre><code>dda env dev run -- ls -l bin/agent/agent\n</code></pre> <p>If you're using a local checkout you should also see that binary on your local machine. This strategy extends to other build artifacts so you could, for example, build the Debian package and install it on an arbitrary machine running Ubuntu.</p> <p>Caveat</p> <p>Environments using a remote clone have no easier way to share build artifacts with your local machine than to use the <code>docker cp</code> command. This is a temporary limitation.</p>"},{"location":"tutorials/dev/env/#stopping-environments","title":"Stopping environments","text":"<p>You can stop an environment by running the following command.</p> <pre><code>dda env dev stop\n</code></pre> <p>This stops the environment but does not remove it. Environments can be started again at any time from a stopped state. When this happens, the start command only accepts options for environment selection.</p>"},{"location":"tutorials/dev/env/#removing-environments","title":"Removing environments","text":"<p>Environments can be removed by running the following command.</p> <pre><code>dda env dev remove\n</code></pre> <p>This removes the environment and all associated data.</p> <p>Tip</p> <p>The stop command accepts a <code>-r</code>/<code>--remove</code> option to remove the environment after stopping it, useful for ephemeral environments.</p> <pre><code>dda env dev stop -r\n</code></pre>"},{"location":"tutorials/dev/lab/","title":"Lab environments","text":"<p>Lab environments are Kubernetes clusters for testing the Datadog Agent. The <code>dda lab</code> command group manages these environments.</p>"},{"location":"tutorials/dev/lab/#supported-providers","title":"Supported providers","text":"Provider Category Description Kind Local Kubernetes in Docker - lightweight local clusters"},{"location":"tutorials/dev/lab/#common-commands","title":"Common commands","text":""},{"location":"tutorials/dev/lab/#list-environments","title":"List environments","text":"<pre><code>dda lab list\n\n# Filter by type\ndda lab list --type kind\n\n# JSON output for scripting\ndda lab list --json\n</code></pre>"},{"location":"tutorials/dev/lab/#delete-environment","title":"Delete environment","text":"<pre><code>dda lab delete\n\n# Non-interactive deletion\ndda lab delete --id &lt;id&gt; --yes\n</code></pre>"},{"location":"tutorials/dev/lab/#configuration","title":"Configuration","text":"<p>If the environment provider installs the Agent (for example <code>dda lab local kind</code>), you must provide an API key. API keys can be configured via environment variables:</p> <pre><code>export E2E_API_KEY=your_api_key\nexport E2E_APP_KEY=your_app_key  # optional\n</code></pre> <p>Or in <code>~/.test_infra_config.yaml</code>:</p> <pre><code>configParams:\n  agent:\n    apiKey: your_api_key\n    appKey: your_app_key\n</code></pre>"},{"location":"tutorials/dev/lab/kind/","title":"Kind","text":"<p>Kind (Kubernetes in Docker) creates lightweight local Kubernetes clusters for development and testing.</p>"},{"location":"tutorials/dev/lab/kind/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker (or an equivalent container runtime)</li> <li>kind</li> <li>helm</li> <li>kubectl</li> </ul>"},{"location":"tutorials/dev/lab/kind/#quick-start","title":"Quick start","text":"<pre><code># Create a local Kind lab environment (installs the Agent by default)\ndda lab local kind --id dev\n</code></pre>"},{"location":"tutorials/dev/lab/kind/#create-cluster","title":"Create cluster","text":"<pre><code># Create a cluster without installing the Agent\ndda lab local kind --id dev --no-agent\n\n# With specific Kubernetes version\ndda lab local kind --id dev --k8s-version v1.30.0\n\n# Recreate existing cluster\ndda lab local kind --id dev --force\n</code></pre>"},{"location":"tutorials/dev/lab/kind/#deploy-agent","title":"Deploy agent","text":"<p>By default, <code>dda lab local kind</code> installs the Datadog Agent via Helm. Provide an API key with <code>E2E_API_KEY</code> or <code>~/.test_infra_config.yaml</code> (see Lab environments).</p>"},{"location":"tutorials/dev/lab/kind/#from-registry","title":"From registry","text":"<pre><code># Default agent image (Agent installation is enabled by default)\ndda lab local kind --id dev\n\n# Custom image\ndda lab local kind --id dev --agent-image gcr.io/datadoghq/agent:7.50.0\n\n# With custom Helm values\ndda lab local kind --id dev --helm-values ./values.yaml\n</code></pre>"},{"location":"tutorials/dev/lab/kind/#build-locally-custom-build-command","title":"Build locally (custom build command)","text":"<p>If you want to build a local Agent image using a custom command, pass <code>--build-command</code>.</p> <p>Note</p> <p>The build runs inside a developer environment (see Using developer environments). Ensure it is started first:</p> <pre><code>dda env dev start\n</code></pre> <pre><code># Example: build an image tagged datadog/agent-dev:local, then load+install it\ndda lab local kind --id dev \\\n  --build-command \"dda inv agent.hacky-dev-image-build --target-image datadog/agent-dev:local\"\n</code></pre>"},{"location":"tutorials/dev/lab/kind/#load-existing-image","title":"Load existing image","text":"<pre><code># Load pre-built local image\ndda lab local kind --id dev --load-image myagent:dev\n</code></pre>"},{"location":"tutorials/dev/lab/kind/#working-with-the-cluster","title":"Working with the cluster","text":"<pre><code># Switch kubectl context\nkubectl config use-context kind-&lt;id&gt;\n\n# Check agent pods\nkubectl get pods -n datadog\n\n# View agent status\nkubectl exec -n datadog daemonset/datadog-agent -- agent status\n\n# View agent logs\nkubectl logs -n datadog -l app=datadog -f\n</code></pre>"},{"location":"tutorials/dev/lab/kind/#options","title":"Options","text":"Option Description <code>--id</code>, <code>-i</code> Environment id <code>--k8s-version</code> Kubernetes version (default: v1.32.0) <code>--no-agent</code> Do not install the Datadog Agent <code>--agent-image</code> Custom agent image <code>--load-image</code> Load existing local docker image into the cluster <code>--helm-values</code> Path to custom Helm values.yaml file <code>--build-command</code> Command to build the agent image (must output an image tagged <code>datadog/agent-dev:local</code>) <code>--devenv</code> Developer environment ID (see <code>dda env dev</code>) <code>--force</code>, <code>-f</code> Recreate cluster if exists <code>--nodes-count</code> Number of nodes in the cluster (default: 2)"}]}