{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Datadog Agent","text":"<p>Welcome to the wonderful world of developing the Datadog Agent. Here we document how we do things, advanced debugging techniques, coding conventions &amp; best practices, the internals of our testing infrastructure, and so much more.</p> <p>If you are intrigued, continue reading. If not, continue all the same </p>"},{"location":"#getting-started","title":"Getting started","text":"<p>First, you'll want to set up your development environment.</p>"},{"location":"#navigation","title":"Navigation","text":"<p>Desktop readers can use keyboard shortcuts to navigate.</p> Keys Action <ul><li>, (comma)</li><li>p</li></ul> Navigate to the \"previous\" page <ul><li>. (period)</li><li>n</li></ul> Navigate to the \"next\" page <ul><li>/</li><li>s</li></ul> Display the search modal"},{"location":"setup/","title":"Set up development environment","text":""},{"location":"setup/#windows","title":"Windows","text":"<p>To build the agent on Windows, see datadog-agent-buildimages.</p>"},{"location":"setup/#linux-and-macos","title":"Linux and macOS","text":""},{"location":"setup/#python","title":"Python","text":"<p>The Agent embeds a full-fledged CPython interpreter so it requires the development files to be available in the dev env. The Agent can embed Python 2 and/or Python 3, you will need development files for all versions you want to support.</p> <p>If you're on OSX/macOS, installing Python 2.7 and/or 3.11 with Homebrew:</p> <pre><code>brew install python@2\nbrew install python@3.11\n</code></pre> <p>On Linux, depending on the distribution, you might need to explicitly install the development files, for example on Ubuntu:</p> <pre><code>sudo apt-get install python2.7-dev\nsudo apt-get install python3.11-dev\n</code></pre> <p>On Windows, install Python 2.7 and/or 3.11 via the official installer brings along all the development files needed:</p> <p>Warning</p> <p>If you don't use one of the Python versions that are explicitly supported, you may have problems running the built Agent's Python checks, especially if using a virtualenv. At this time, only Python 3.11 is confirmed to work as expected in the development environment.</p>"},{"location":"setup/#python-dependencies","title":"Python Dependencies","text":""},{"location":"setup/#preface","title":"Preface","text":"<p>To protect and isolate your system-wide python installation, a python virtual environment is highly recommended (though optional). It will help keep a self-contained development environment and ensure a clean system Python.</p> <p>Note</p> <p>Due to the way some virtual environments handle executable paths (e.g. <code>python -m venv</code>), not all virtual environment options will be able to run the built Agent correctly. At this time, the only confirmed virtual enviroment creator that is known for sure to work is <code>virtualenv</code>.</p> <ul> <li>Install the virtualenv module:     <pre><code>python3 -m pip install virtualenv\n</code></pre></li> <li>Create the virtual environment:     <pre><code>virtualenv $GOPATH/src/github.com/DataDog/datadog-agent/venv\n</code></pre></li> <li>Activate the virtualenv (OS-dependent). This must be done for every new terminal before you start.</li> </ul> <p>If using virtual environments when running the built Agent, you may need to override the built Agent's search path for Python check packages using the <code>PYTHONPATH</code> variable (your target path must have the pre-requisite core integration packages installed though).</p> <pre><code>PYTHONPATH=\"./venv/lib/python3.11/site-packages:$PYTHONPATH\" ./agent run ...\n</code></pre> <p>See also some notes in ./checks about running custom python checks.</p>"},{"location":"setup/#invoke","title":"Invoke","text":"<p>Invoke is a task runner written in Python that is extensively used in this project to orchestrate builds and test runs. Our invoke tasks are only compatible with Python 3, thus you will need to use Python 3 to run them.</p> <p>Though you may install invoke in a variety of way we suggest you use the provided requirements file and <code>pip</code>:</p> <pre><code>pip install -r tasks/requirements.txt\n</code></pre> <p>This procedure ensures you not only get the correct version of <code>invoke</code>, but also any additional python dependencies our development workflow may require, at their expected versions. It will also pull other handy development tools/deps (<code>reno</code>, or <code>docker</code>).</p>"},{"location":"setup/#golang","title":"Golang","text":"<p>You must install Golang version <code>1.21.7</code> or higher. Make sure that <code>$GOPATH/bin</code> is in your <code>$PATH</code> otherwise <code>invoke</code> cannot use any additional tool it might need.</p> <p>Note</p> <p>Versions of Golang that aren't an exact match to the version specified in our build images (see e.g. here) may not be able to build the agent and/or the rtloader binary properly.</p>"},{"location":"setup/#installing-tooling","title":"Installing tooling","text":"<p>From the root of <code>datadog-agent</code>, run <code>invoke install-tools</code> to install go tooling. This uses <code>go</code> to install the necessary dependencies.</p>"},{"location":"setup/#system-or-embedded","title":"System or Embedded?","text":"<p>When working on the Agent codebase you can choose among two different ways to build the binary, informally named System and Embedded builds. For most contribution scenarios you should rely on the System build (the default) and use the Embedded one only for specific use cases. Let's explore the differences.</p>"},{"location":"setup/#system-build","title":"System build","text":"<p>System builds use your operating system's standard system libraries to satisfy the Agent's external dependencies. Since, for example, macOS 10.11 may provide a different version of Python than macOS 10.12, system builds on each of these platforms may produce different Agent binaries. If this doesn't matter to you\u2014perhaps you just want to contribute a quick bugfix\u2014do a System build; it's easier and faster than an Embedded build. System build is the default for all build and test tasks, so you don't need to configure anything there. But to make sure you have system copies of all the Agent's dependencies, skip the Embedded build section below and read on to see how to install them via your usual package manager (apt, yum, brew, etc).</p>"},{"location":"setup/#embedded-build","title":"Embedded build","text":"<p>Embedded builds download specifically-versioned dependencies and compile them locally from sources. We run Embedded builds to create Datadog's official Agent releases (i.e. RPMs, debs, etc), and while you can run the same builds while developing locally, the process is as slow as it sounds. Hence, you should only use them when you care about reproducible builds. For example:</p> <ul> <li>you want to build an agent binary that can be used as-is to replace the binary of an existing agent installation</li> <li>some dependencies are not available on your system</li> <li>you're working or debugging at a very low level: let's say you're adding a function to the Python bindings, you want to make sure you're using the exact same versions of Python as the official Agent packages</li> </ul> <p>Embedded builds rely on Omnibus to download and build dependencies, so you need a recent <code>ruby</code> environment with <code>bundler</code> installed. See how to build Agent packages with Omnibus for more details.</p>"},{"location":"setup/#systemd","title":"Systemd","text":"<p>The agent is able to collect systemd journal logs using a wrapper on the systemd utility library.</p> <p>On Ubuntu/Debian:</p> <pre><code>sudo apt-get install libsystemd-dev\n</code></pre> <p>On Redhat/CentOS:</p> <pre><code>sudo yum install systemd-devel\n</code></pre>"},{"location":"setup/#docker","title":"Docker","text":"<p>If you want to build a Docker image containing the Agent, or if you wan to run system and integration tests you need to run a recent version of Docker in your dev environment.</p>"},{"location":"setup/#doxygen","title":"Doxygen","text":"<p>We use Doxygen to generate the documentation for the <code>rtloader</code> part of the Agent.</p> <p>To generate it (using the <code>invoke rtloader.generate-doc</code> command), you'll need to have Doxygen installed on your system and available in your <code>$PATH</code>. You can compile and install Doxygen from source with the instructions available here. Alternatively, you can use already-compiled Doxygen binaries from here.</p> <p>To get the dependency graphs, you may also need to install the <code>dot</code> executable from graphviz and add it to your <code>$PATH</code>.</p>"},{"location":"setup/#pre-commit-hooks","title":"Pre-commit hooks","text":"<p>It is optional but recommended to install <code>pre-commit</code> to run a number of checks done by the CI locally.</p>"},{"location":"setup/#installation","title":"Installation","text":"<p>To install it, run:</p> <pre><code>python3 -m pip install pre-commit\npre-commit install\n</code></pre> <p>The <code>shellcheck</code> pre-commit hook requires having the <code>shellcheck</code> binary installed and in your <code>$PATH</code>. To install it, run:</p> <pre><code>inv install-shellcheck --destination &lt;path&gt;\n</code></pre> <p>(by default, the shellcheck binary is installed in <code>/usr/local/bin</code>).</p>"},{"location":"setup/#skipping-pre-commit","title":"Skipping <code>pre-commit</code>","text":"<p>If you want to skip <code>pre-commit</code> for a specific commit you can add <code>--no-verify</code> to the <code>git commit</code> command.</p>"},{"location":"setup/#running-pre-commit-manually","title":"Running <code>pre-commit</code> manually","text":"<p>If you want to run one of the checks manually, you can run <code>pre-commit run &lt;check name&gt;</code>.</p> <p>You can run it on all files with the <code>--all-files</code> flag.</p> <pre><code>pre-commit run flake8 --all-files  # run flake8 on all files\n</code></pre> <p>See <code>pre-commit run --help</code> for further options.</p>"},{"location":"setup/#setting-up-visual-studio-code-dev-container","title":"Setting up Visual Studio Code Dev Container","text":"<p>Microsoft Visual Studio Code with the devcontainer plugin allow to use a container as remote development environment in vscode. It simplify and isolate the dependencies needed to develop in this repository.</p> <p>To configure the vscode editor to use a container as remote development environment you need to:</p> <ul> <li>Install the devcontainer plugin and the golang language plugin.</li> <li>Run the following invoke command <code>invoke vscode.setup-devcontainer --image \"&lt;image name&gt;\"</code>. This command will create the devcontainer configuration file <code>./devcontainer/devcontainer.json</code>.</li> <li>Start or restart your vscode editor.</li> <li>A pop-up should show-up to propose to \"reopen in container\" your workspace.</li> <li>The first start, it might propose you to install the golang plugin dependencies/tooling.</li> </ul>"},{"location":"setup/#windows-development-environment","title":"Windows development environment","text":""},{"location":"setup/#code-editor","title":"Code editor","text":"<p>Microsoft Visual Studio Code is recommended as it's lightweight and versatile.</p> <p>Building on Windows requires multiple 3<sup>rd</sup>-party software to be installed. To avoid the complexity, Datadog recommends to make the code change in VS Code, and then do the build in Docker image. For complete information, see Build the Agent packages</p>"},{"location":"architecture/components/fx/","title":"Overview of Fx","text":"<p>The Agent uses Fx as its application framework. While the linked Fx documentation is thorough, it can be a bit difficult to get started with. This document describes how Fx is used within the Agent in a more approachable style.</p>"},{"location":"architecture/components/fx/#what-is-it","title":"What Is It?","text":"<p>Fx's core functionality is to create instances of required types \"automatically,\" also known as dependency injection. Within the agent, these instances are components, so Fx connects components to one another. Fx creates a single instance of each component, on demand.</p> <p>This means that each component declares a few things about itself to Fx, including the other components it depends on. An \"app\" then declares the components it contains to Fx, and instructs Fx to start up the whole assembly.</p>"},{"location":"architecture/components/fx/#providing-and-requiring","title":"Providing and Requiring","text":"<p>Fx connects components using types. Within the Agent, these are typically interfaces named <code>Component</code>. For example, <code>scrubber.Component</code> might be an interface defining functionality for scrubbing passwords from data structures:</p>  scrubber/component.go <pre><code>type Component interface {\n    ScrubString(string) string\n}\n</code></pre> <p>Fx needs to know how to provide an instance of this type when needed, and there are a few ways:</p> <ul> <li><code>fx.Provide(NewScrubber)</code> where <code>NewScrubber</code> is a constructor that returns a <code>scrubber.Component</code>. This indicates that if and when a <code>scrubber.Component</code> is required, Fx should call <code>NewScrubber</code>. It will call <code>NewScrubber</code> only once, using the same value everywhere it is required.</li> <li><code>fx.Supply(scrubber)</code> where <code>scrubber</code> implements the <code>scrubber.Component</code> interface. When another component requires a <code>scrubber.Component</code>, this is the instance it will get.</li> </ul> <p>The first form is much more common, as most components have constructors that do interesting things at runtime. A constructor can return multiple arguments, in which case the constructor is called if any of those argument types are required. Constructors can also return <code>error</code> as the final return type. Fx will treat an error as fatal to app startup.</p> <p>Fx also needs to know when an instance is required, and this is where the magic happens. In specific circumstances, it uses reflection to examine the argument list of functions, and creates instances of each argument's type. Those circumstances are:</p> <ul> <li>Constructors used with <code>fx.Provide</code>. Imagine <code>NewScrubber</code> depends on the config module to configure secret matchers:       <pre><code>func NewScrubber(config config.Component) Component {\n    return &amp;scrubber{\n        matchers: makeMatchersFromConfig(config),\n    }\n}\n</code></pre></li> <li>Functions passed to <code>fx.Invoke</code>:     <pre><code>fx.Invoke(func(sc scrubber.Component) {\n    fmt.Printf(\"scrubbed: %s\", sc.ScrubString(somevalue))\n})\n</code></pre>     Like constructors, Invoked functions can take multiple arguments, and can optionally return an error. Invoked functions are called automatically when an app is created.</li> <li> <p>Pointers passed to <code>fx.Populate</code>.    <pre><code>var sc scrubber.Component\n// ...\nfx.Populate(&amp;sc)\n</code></pre>    Populate is useful in tests to fill an existing variable with a provided value. It's equivalent to <code>fx.Invoke(func(tmp scrubber.Component) { *sc = tmp })</code>.</p> <p>Functions can take multple arguments of different types, requiring all of them.</p> </li> </ul>"},{"location":"architecture/components/fx/#apps-and-options","title":"Apps and Options","text":"<p>You may have noticed that all of the <code>fx</code> methods defined so far return an <code>fx.Option</code>. They don't actually do anything on their own. Instead, Fx uses the functional options pattern from Rob Pike. The idea is that a function takes a variable number of options, each of which has a different effect on the result.</p> <p>In Fx's case, the function taking the options is <code>fx.New</code>, which creates a new <code>fx.App</code>. It's within the context of an app that requirements are met, constructors are called, and so on.</p> <p>Tying the example above together, a very simple app might look like this:</p> <pre><code>someValue = \"my password is hunter2\"\napp := fx.New(\n    fx.Provide(scrubber.NewScrubber),\n    fx.Invoke(func(sc scrubber.Component) {\n        fmt.Printf(\"scrubbed: %s\", sc.ScrubString(somevalue))\n    }))\napp.Run()\n// Output: scrubbed: my password is *******\n</code></pre> <p>For anything more complex, it's not practical to call <code>fx.Provide</code> for every component in a single source file. Fx has two abstraction mechanisms that allow combining lots of options into one app:</p> <ul> <li><code>fx.Options</code> simply bundles several Option values into a single Option that can be placed in a variable. As the example in the Fx documentation shows, this is useful to gather the options related to a single Go package, which might include un-exported items, into a single value typically named <code>Module</code>.</li> <li><code>fx.Module</code> is very similar, with two additional features. First, it requires a module name which is used in some Fx logging and can help with debugging. Second, it creates a scope for the effects of <code>fx.Decorate</code> and <code>fx.Replace</code>. The second feature is not used in the Agent.</li> </ul> <p>So a slightly more complex version of the example might be:</p>  scrubber/component.go main.go <pre><code>func Module() fxutil.Module {\n    return fx.Module(\"scrubber\",\n    fx.Provide(newScrubber))    // now newScrubber need not be exported\n}\n</code></pre> <pre><code>someValue = \"my password is hunter2\"\napp := fx.New(\n    scrubber.Module(),\n    fx.Invoke(func(sc scrubber.Component) {\n        fmt.Printf(\"scrubbed: %s\", sc.ScrubString(somevalue))\n    }))\napp.Run()\n// Output: scrubbed: my password is *******\n</code></pre>"},{"location":"architecture/components/fx/#lifecycle","title":"Lifecycle","text":"<p>Fx provides an <code>fx.Lifecycle</code> component that allows hooking into application start-up and shut-down. Use it in your component's constructor like this:</p> <pre><code>func newScrubber(lc fx.Lifecycle) Component {\n    sc := &amp;scrubber{..}\n    lc.Append(fx.Hook{OnStart: sc.start, OnStop: sc.stop})\n    return sc\n}\n\nfunc (sc *scrubber) start(ctx context.Context) error { .. }\nfunc (sc *scrubber) stop(ctx context.Context) error { .. }\n</code></pre> <p>This separates the application's lifecycle into a few distinct phases:</p> <ul> <li>Initialization - calling constructors to satisfy requirements, and calling invoked functions that require them.</li> <li>Startup - calling components' OnStart hooks (in the same order the components were initialized)</li> <li>Runtime - steady state</li> <li>Shutdown - calling components' OnStop hooks (reverse of the startup order)</li> </ul>"},{"location":"architecture/components/fx/#ins-and-outs","title":"Ins and Outs","text":"<p>Fx provides some convenience types to help build constructors that require or provide lots of types: <code>fx.In</code> and <code>fx.Out</code>. Both types are embedded in structs, which can then be used as argument and return types for constructors, respectively. By convention, these are named <code>dependencies</code> and <code>provides</code> in Agent code:</p> <pre><code>type dependencies struct {\n    fx.In\n\n    Config config.Component\n    Log log.Component\n    Status status.Component\n)\n\ntype provides struct {\n    fx.Out\n\n    Component\n    // ... (we'll see why this is useful below)\n}\n\nfunc newScrubber(deps dependencies) (provides, error) { // can return an fx.Out struct and other types, such as error\n    // ..\n    return provides {\n        Component: scrubber,\n        // ..\n    }, nil\n}\n</code></pre> <p>In and Out provide a nice way to summarize and document requirements and provided types, and also allow annotations via Go struct tags. Note that annotations are also possible with <code>fx.Annotate</code>, but it is much less readable and its use is discouraged.</p>"},{"location":"architecture/components/fx/#value-groups","title":"Value Groups","text":"<p>Value groups make it easier to produce and consume many values of the same type. A component can add any type into groups which can be consumed by other components.</p> <p>For example:</p> <p>Here, two components add a <code>server.Endpoint</code> type to the <code>server</code> group (note the <code>group</code> label in the <code>fx.Out</code> struct).</p>  todolist/todolist.go users/users.go <pre><code>type provides struct {\n    fx.Out\n    Component\n    Endpoint server.Endpoint `group:\"server\"`\n}\n</code></pre> <pre><code>type provides struct {\n    fx.Out\n    Component\n    Endpoint server.Endpoint `group:\"server\"`\n}\n</code></pre> <p>Here, a component requests all the types added to the <code>server</code> group. This takes the form of a slice received at instantiation (note once again the <code>group</code> label but in <code>fx.In</code> struct).</p>  server/server.go <pre><code>type dependencies struct {\n    fx.In\n    Endpoints []Endpoint `group:\"server\"`\n}\n</code></pre>"},{"location":"architecture/components/fx/#day-to-day-usage","title":"Day-to-Day Usage","text":"<p>Day-to-day, the Agent's use of Fx is fairly formulaic. Following the component guidelines, or just copying from other components, should be enough to make things work without a deep understanding of Fx's functionality.</p>"},{"location":"architecture/components/overview/","title":"Overview of Components","text":"<p>The Agent is structured as a collection of components working together. Depending on how the binary is built, and how it is invoked, different components may be instantiated. The behavior of the components depends on the Agent configuration.</p> <p>Components are structured in a dependency graph. For example, the <code>comp/logs/agent</code> component depends on the <code>comp/core/config</code> component to access Agent configuration. At startup, a few top-level components are requested, and Fx automatically instantiates all of the required components.</p>"},{"location":"architecture/components/overview/#what-is-a-component","title":"What is a Component?","text":"<p>Any well-defined portion of the codebase, with a clearly documented API surface, can be a component. As an aid to thinking about this question, consider four \"levels\" where it might apply:</p> <ol> <li>Meta: large-scale parts of the Agent that use many other components. Example: DogStatsD or Logs-Agent.</li> <li>Service: something that can be used at several locations (for example by different applications). Example: Forwarder.</li> <li>Internal: something that is used to implement a service or meta component, but doesn't make sense outside the component. Examples: DogStatsD's TimeSampler, or a workloadmeta Listener.</li> <li>Implementation: a type that is used to implement internal components. Example: Forwarder's DiskUsageLimit.</li> </ol> <p>In general, meta and service-level functionality should always be implemented as components. Implementation-level functionality should not. Internal functionality is left to the descretion of the implementing team: it's fine for a meta or service component to be implemented as one large, complex component, if that makes the most sense for the team.</p>"},{"location":"architecture/components/overview/#bundles","title":"Bundles","text":"<p>There is a large and growing number of components, and listing those components out repeatedly could grow tiresome and cause bugs. Component bundles provide a way to manipulate multiple components, usually at the meta or service level, as a single unit. For example, while Logs-Agent is internally composed of many components, those can be addressed as a unit with <code>comp/logs.Bundle</code>.</p> <p>Bundles also provide a way to provide parameters to components at instantiation. Parameters can control the behavior of components within a bundle at a coarse scale, such as whether the logs-agent should start or not.</p>"},{"location":"architecture/components/overview/#apps-and-binaries","title":"Apps and Binaries","text":"<p>The build infrastructure builds several agent binaries from the agent source. Some are purpose-specific, such as the serverless agent or dogstatsd, while others such as the core agent support many kinds of functionality. Each build is made from a subset of the same universe of components. For example, the components comprising the DogStatsD build are precisely the same components implementing the DogStatsD functionality in the core agent.</p> <p>Most binaries support subcommands, such as <code>agent run</code> or <code>agent status</code>. Each of these also uses a subset of the components available in the binary, and perhaps some different bundle parameters. For example, <code>agent status</code> does not need the logs-agent bundle (<code>comp/logs.Bundle</code>), and does not need to start core-bundle services like component health monitoring.</p> <p>These subcommands are implemented as Fx apps. An app specifies, the set of components that can be instantiated, their parameters, and the top-level components that should be requested.</p> <p>There are utility functions available in <code>pkg/util/fxutil</code> to eliminate some common boilerplate in creating and running apps.</p>"},{"location":"architecture/components/overview/#build-time-and-runtime-dependencies","title":"Build-Time and Runtime Dependencies","text":"<p>Let's consider sets and subsets of components. Each of the following sets is a subset of the previous set:</p> <ol> <li>All implemented components (everything in this document)</li> <li>All components in a binary (everything directly or indirectly referenced by a binary's <code>main()</code>) -- the build-time dependencies</li> <li>All components available in an app (everything provided by a bundle in the app's <code>fx.New</code> call)</li> <li>All components instantiated in an app (all explicitly required components and their transitive dependencies) -- the runtime dependencies</li> <li>All components started in an app (all instantiated components, except those disabled by their parameters)</li> </ol> <p>The build-time dependencies determine the binary size. For example, omitting container-related components from a binary dramatically reduces binary size by not requiring kubernetes and docker API libraries.</p> <p>The runtime dependencies determine, in part, the process memory consumption. This is a small effect because many components will use only a few Kb if they are not actually doing any work. For example, if the trace-agent's trace-writer component is instantiated, but writes no traces, the peformance impact is trivial.</p> <p>The started components determine CPU usage and consumption of other resources. A component polling a data source unnecessarily is wasteful of CPU resources. But perhaps more critically for correct behavior, a component started when it is not needed may open network ports or engage other resources unnecessarily. For example, <code>agent status</code> should not open a listening port for DogStatsD traffic.</p> <p>It's important to note that the size of the third set in the list above, \"all components available\", has no performance effect. As long as the components would be included in the binary anyway, it does no harm to make them available in the app.</p>"},{"location":"architecture/dogstatsd/internals/","title":"DogStatsD internals","text":"<p>(click to enlarge)</p> <p>Information on DogStatsD, configuration and troubleshooting is available in the Datadog documentation.</p>"},{"location":"architecture/dogstatsd/internals/#packet","title":"Packet","text":"<p>In DogStatsD, a Packet is a bytes array containing one or multiple metrics in the DogStatsD format (separated by a <code>\\n</code> when there are several). Its maximum size is <code>dogstatsd_buffer_size</code>.</p>"},{"location":"architecture/dogstatsd/internals/#packetassembler","title":"PacketAssembler","text":"<ul> <li>Input: a datagram from a UDP socket</li> <li>Output: a Packet containing multiple metrics packed together, separated by a <code>\\n</code></li> </ul> <p>The PacketAssembler gathers multiple datagrams into one Packet of maximum size, <code>dogstatsd_buffer_size</code>, and sends it to the PacketsBuffer which avoids running the whole parsing pipeline with only one metric per packet. The bytes buffer used comes from the PacketPool, which avoids re-allocating the bytes buffer every time.</p> <p>Note</p> <p>The UDS pipeline does not use the PacketAssembler because each UDS packet also contains metadata (origin tags) which are used to enrich the metrics tags, making them impossible to be packed together by the PacketAssembler.</p> <p>The PacketAssembler does not allocate a bytes array every time it has to use one. It retrieves one from a pool containing pre-allocated arrays and this pool never empties. The PacketAssembler allocates a new bytes array when it\u2019s needed. Once fully assembled by the PacketAssembler, the bytes array is sent through the rest of the DogStatsD pipeline and ownership is allocated to each part using it (PacketsBuffer, Worker). Eventually, the Worker takes care of returning it to the pool when the part has processed its content.</p>"},{"location":"architecture/dogstatsd/internals/#packetbuffer","title":"PacketBuffer","text":"<ul> <li>Input: a Packet containing one or several metrics in the DogStatsD format (separated by a <code>\\n</code>)</li> <li>Output: multiple Packets send in a row to the Worker</li> </ul> <p>The PacketsBuffer buffers multiple Packets (in a slice), this way the parsing part of the pipeline is going through several Packets in a row instead of only one each time it is called. This leads to less CPU usage. PacketsBuffer sends the Packets for processing when either:</p> <p>a. The buffer is full (contains <code>dogstatsd_packet_buffer_size, default value: 32</code>)</p> <p>or</p> <p>b. A timer is triggered (i.e. <code>dogstatsd_packer_buffer_flush_timeout, default value: 100ms</code>)</p> <p>The PacketBuffer sends it in a Go buffered channel to the worker / parser, meaning that the channels can buffer the Packets on their own while waiting for the worker to read and process them.</p> <p>In theory, the max memory usage of this Go buffered channel is:</p> <ul> <li>packet buffer size * packet size * channel buffer size</li> <li><code>dogstatsd_packer_buffer_size</code> * <code>dogstatsd_buffer_size</code> * <code>dogstatsd_queue_size</code></li> <li>32 * 8192 * 1024 =  256MB</li> </ul>"},{"location":"architecture/dogstatsd/internals/#worker","title":"Worker","text":"<ul> <li>Input: slice of Packets</li> <li>Output: MetricSample sent</li> </ul> <p>The Worker is the part of the DogStatsD server responsible for parsing the metrics in the bytes array and turning them into MetricSamples.</p> <p>The server spawns multiple workers based on the amount of cores available on the host:</p> <ul> <li>When the server is not running multiple time sampling pipelines: the server creates <code>(number of cores - 2)</code> workers. If this result is less than 2, the server spawns 2 workers.</li> <li>When the server is running multiple time sampling pipelines: the server creates <code>(number of cores / 2)</code> workers.  If this result is less than 2, the server spawns 2 workers.</li> </ul> <p>The Worker is using a system called StringInterner to not allocate memory every time a string is needed. Note that this StringInterner is caching a finite number of strings and when it is full it is emptied to start caching strings again. Its size is configurable with <code>dogstatsd_string_interner_size</code>.</p> <p>The MetricSamples created are not directly sent to the Agent Demultiplexer but first to a part called the Batcher.</p>"},{"location":"architecture/dogstatsd/internals/#batcher","title":"Batcher","text":"<ul> <li>Input: MetricSample from the Worker</li> <li>Output: slices of MetricSample sent to the Agent Demultiplexer (which is distributing them to the TimeSamplerWorkers)</li> </ul> <p>The role of the Batcher is to accumulate multiple MetricSamples before sending them to the Agent Demultiplexer. Every time it accumulates 32 MetricSamples, the Batcher sends them to the Demultiplexer. The Batcher sends 32 MetricSamples in a channel buffering 100 sets. There is one channel per TimeSampler.</p> <p>The size of a MetricSample depends on the size of the host's hostname, its metric name, and its number of tags. An example MetricSample with a 20 character hostname, 40 character metric name, and 200 characters of tags has a size of approximately 264 bytes. A Batcher can use a maximum of 844kb of memory.</p>"},{"location":"architecture/dogstatsd/internals/#timesamplerworker","title":"TimeSamplerWorker","text":"<ul> <li>Input: slice of MetricSamples</li> </ul> <p>The TimeSamplerWorker runs in an infinite loop. It is responsible for the following:</p> <ul> <li>Process slices of MetricSamples sent by the Batcher (through the AgentDemultiplexer). A TimeSampler embedded in the TimeSamplerWorker actually does the sampling.</li> <li>Flush on a signal.</li> <li>Stop on a signal.</li> </ul> <p>The following calculations determine the number of TimeSamplerWorker and TimeSampler instances:</p> <ul> <li>If <code>dogstatsd_pipeline_autoadjust</code> is <code>true</code> then the workers count will be automatically adjusted.</li> <li>If <code>dogstatsd_pipeline_count</code> has a value, the number of TimeSampler pipelines equals that value.</li> <li>If neither condition above is true, one TimeSampler pipeline runs.</li> </ul> <p><code>dogstatsd_pipeline_autoadjust_strategy</code> can be set to the following values:</p> <ul> <li><code>max_throughput</code>: The number of TimeSampler pipelines is adjusted to maximize throughput. There are <code>(number of core/2) - 1</code> instances of TimeSampler.</li> <li><code>per_origin</code>: The number of TimeSampler pipelines is adjusted to improve data locality. The number of dsdWorker instances is equal to half the number of cores.         and the number of TimeSampler pipelines is equal <code>dogstatsd_pipeline_count</code> or twice the number of cores. This strategy will provide a better compression         ratio in shared environments and improve resource allocation fairness within the agent.</li> </ul>"},{"location":"architecture/dogstatsd/internals/#noaggregationstreamworker","title":"NoAggregationStreamWorker","text":"<ul> <li>Input: slice of MetricSamples</li> </ul> <p>The NoAggregationStreamWorker runs an infinite loop in a goroutine. It receives metric samples with timestamps, and it batches them to be sent as quickly as possible to the intake. It performs no aggregation nor extra processing, except from adding tags to the metrics.</p> <p>It runs only when <code>dogstatsd_no_aggregation_pipeline</code> is set to <code>true</code>.</p> <p>The payload being sent to the intake (through the normal <code>Serializer</code>/<code>Forwarder</code> pieces) contains, at maximum, <code>dogstatsd_no_aggregation_pipeline_batch_size</code> metrics. This value defaults to <code>2048</code>.</p>"},{"location":"guidelines/contributing/","title":"Contributing to Datadog Agent","text":"<p>First of all, thanks for contributing!</p> <p>This document provides some basic guidelines for contributing to this repository. To propose improvements, feel free to submit a PR.</p>"},{"location":"guidelines/contributing/#submitting-issues","title":"Submitting issues","text":"<ul> <li>If you think you've found an issue, please search the Agent Troubleshooting section to see if it's known.</li> <li>If you\u2019re still unsure about the issue, you may reach out to the Datadog support team with a flare from your Agent.</li> <li>Finally, you can open a Github issue.</li> </ul>"},{"location":"guidelines/contributing/#pull-requests","title":"Pull Requests","text":"<p>Have you fixed a bug or written a new check and want to share it? Many thanks!</p> <p>In order to ease/speed up our review, here are some items you can check/improve when submitting your PR:</p> Contributor ChecklistReviewer Checklist <ul> <li> <p> Have a proper commit history (we advise you to rebase if needed) with clear commit messages.</p> </li> <li> <p> Write tests for the code you wrote.</p> </li> <li> <p> Preferably make sure that all tests pass locally.</p> </li> <li> <p> Summarize your PR with an explanatory title and a message describing your changes, cross-referencing any related bugs/PRs.</p> </li> <li> <p> Use Reno to create a release note.</p> </li> <li> <p> Open your PR against the <code>main</code> branch.</p> </li> <li> <p> Provide adequate QA/testing plan information.</p> </li> </ul> <ul> <li> <p> The added code comes with tests.</p> </li> <li> <p> The CI is green, all tests are passing (required or not).</p> </li> <li> <p> All applicable labels are set on the PR (see PR labels list).</p> </li> <li> <p> If applicable, the config template has been updated.</p> </li> </ul> <p>Note</p> <p>Adding GitHub labels is only possible for contributors with write access.</p> <p>Your pull request must pass all CI tests before we will merge it. If you're seeing an error and don't think it's your fault, it may not be! Join us on Slack or send us an email, and together we'll get it sorted out.</p>"},{"location":"guidelines/contributing/#keep-it-small-focused","title":"Keep it small, focused","text":"<p>Avoid changing too many things at once. For instance if you're fixing the NTP check and at the same time shipping a dogstatsd improvement, it makes reviewing harder and the time-to-release longer.</p>"},{"location":"guidelines/contributing/#commit-messages","title":"Commit Messages","text":"<p>Please don't be this person: <code>git commit -m \"Fixed stuff\"</code>. Take a moment to write meaningful commit messages.</p> <p>The commit message should describe the reason for the change and give extra details that will allow someone later on to understand in 5 seconds the thing you've been working on for a day.</p> <p>This includes editing the commit message generated by GitHub from:</p> <pre><code>Including new features\n\n* Fix linter\n* WIP\n* Add test for x86\n* Fix licenses\n* Cleanup headers\n</code></pre> <p>to:</p> <pre><code>Including new features\n\nThis feature does this and that. Some tests are excluded on x86 because of ...\n</code></pre> <p>If your commit is only shipping documentation changes or example files, and is a complete no-op for the test suite, please add [skip ci] in the commit message body to skip the build and give that slot to someone else who does need it.</p>"},{"location":"guidelines/contributing/#pull-request-workflow","title":"Pull request workflow","text":"<p>The goals ordered by priority are:</p> <ul> <li>Make PR reviews (both initial and follow-up reviews) easy for reviewers using GitHub</li> <li>On the <code>main</code> branch, have a meaningful commit history that allows understanding (even years later) what each commit does, and why.</li> </ul> <p>You must open the PR when the code is reviewable or you must set the PR as draft if you want to share code before it's ready for actual reviews.</p>"},{"location":"guidelines/contributing/#before-the-first-pr-review","title":"Before the first PR review","text":"<p>Before the first PR review, meaningful commits are best: logically-encapsulated commits help the reviews go quicker and make the job for the reviewer easier. Conflicts with <code>main</code> can be resolved with a <code>git rebase origin/main</code> and a force push if it makes future review(s) easier.</p>"},{"location":"guidelines/contributing/#after-the-first-review","title":"After the first review","text":"<p>After the first review, to make follow-up reviews easier:</p> <ul> <li>Avoid force pushes: rewriting the history that was already reviewed makes follow-up reviews painful as GitHub loses track of each comment. Instead, address reviews with additional commits on the PR branch.</li> <li>Resolve merge conflicts with <code>main</code> using <code>git merge origin/main</code></li> </ul>"},{"location":"guidelines/contributing/#how-to-merge-to-main","title":"How to merge to <code>main</code>","text":"<p>Once reviews are complete, the merge to <code>main</code> should be done with either:</p> <ul> <li>the squash-merge option, to keep the history of <code>main</code> clean (even though some context/details are lost in the squash). The commit message for this squash should always be edited to concisely describe the commit without extraneous \u201caddress review comments\u201d text.</li> <li>the \u201crebase-merge\u201d option, after manually rewriting the PR\u2019s commit history and force-pushing to the branch. When using this option, the branch must have a clean history.</li> </ul>"},{"location":"guidelines/contributing/#reno","title":"Reno","text":"<p>We use <code>Reno</code> to create our CHANGELOG. Reno is a pretty simple tool.</p> <p>Each PR should include a <code>releasenotes</code> file created with <code>reno</code>, unless the PR doesn't have any impact on the behavior of the Agent and therefore shouldn't be mentioned in the CHANGELOG (examples: repository documentation updates, changes in code comments). PRs that don't require a release note file will be labeled <code>changelog/no-changelog</code> by maintainers.</p> <p>To install reno: <code>pip install reno</code></p> <p>Ultra quick <code>Reno</code> HOWTO:</p> <pre><code>$&gt; reno new &lt;topic-of-my-pr&gt; --edit\n[...]\n# Remove unused sections and fill the relevant ones.\n# Reno will create a new file in releasenotes/notes.\n#\n# Each section from every release note are combined when the CHANGELOG.rst is\n# rendered. So the text needs to be worded so that it does not depend on any\n# information only available in another section. This may mean repeating some\n# details, but each section must be readable independently of the other.\n#\n# Each section note must be formatted as reStructuredText.\n[...]\n</code></pre> <p>Then just add and commit the new releasenote (located in <code>releasenotes/notes/</code>) with your PR. If the change is on the <code>trace-agent</code> (folders <code>cmd/trace-agent</code> or <code>pkg/trace</code>) please prefix the release note with \"APM :\" and the  argument with \"apm-\"."},{"location":"guidelines/contributing/#reno-sections","title":"Reno sections","text":"<p>The main thing to keep in mind is that the CHANGELOG is written for the agent's users and not its developers.</p> <ul> <li> <p><code>features</code>: describe shortly what your feature does.</p> <p>example: <pre><code>features:\n  - |\n    Introducing the Datadog Process Agent for Windows.\n</code></pre></p> </li> <li> <p><code>enhancements</code>: describe enhancements here: new behavior that are too small to be considered a new feature.</p> <p>example: <pre><code>enhancements:\n  - |\n    Windows: Add PDH data to flare.\n</code></pre></p> </li> <li> <p><code>issues</code>: describe known issues or limitation of the agent.</p> <p>example: <pre><code>issues:\n  - |\n    Kubernetes 1.3 &amp; OpenShift 3.3 are currently not fully supported: docker\n    and kubelet integrations work OK, but apiserver communication (event\n    collection, `kube_service` tagging) is not implemented\n</code></pre></p> </li> <li> <p><code>upgrade</code>: List actions to take or limitations that could arise upon upgrading the Agent. Notes here must include steps that users can follow to 1. know if they're affected and 2. handle the change gracefully on their end.</p> <p>example: <pre><code>upgrade:\n  - |\n    If you run a Nomad agent older than 0.6.0, the `nomad_group`\n    tag will be absent until you upgrade your orchestrator.\n</code></pre></p> </li> <li> <p><code>deprecations</code>: List deprecation notes here.</p> <p>example: <pre><code>deprecations:\n- |\n  Changed the attribute name to enable log collection from YAML configuration\n  file from \"log_enabled\" to \"logs_enabled\", \"log_enabled\" is still\n  supported.\n</code></pre></p> </li> <li> <p><code>security</code>: List security fixes, issues, warning or related topics here.</p> <p>example: <pre><code>security:\n  - |\n    The /agent/check-config endpoint has been patched to enforce\n    authentication of the caller via a bearer session token.\n</code></pre></p> </li> <li> <p><code>fixes</code>: List the fixes done in your PR here. Remember to be clear and give a minimum of context so people reading the CHANGELOG understand what the fix is about.</p> <p>example: <pre><code>fixes:\n  - |\n    Fix EC2 tags collection when multiple marketplaces are set.\n</code></pre></p> </li> <li> <p><code>other</code>: Add here every other information you want in the CHANGELOG that don't feat in any other section. This section should rarely be used.</p> <p>example: <pre><code>other:\n  - |\n    Only enable the ``resources`` metadata collector on Linux by default, to match\n    Agent 5's behavior.\n</code></pre></p> </li> </ul>"},{"location":"guidelines/contributing/#pr-labels","title":"PR labels","text":"<p>For internal PRs (from people in the Datadog organisation), you have few extra labels that can be use:</p> <ul> <li><code>community/help-wanted</code>: for community PRs where help is needed to finish it.</li> <li><code>community</code>: for community PRs.</li> <li><code>changelog/no-changelog</code>: for PRs that don't require a reno releasenote (useful for PRs only changing documentation or tests).</li> <li><code>qa/done</code>, <code>qa/no-code-change</code>: if either the <code>qa/no-code-change</code> label or the <code>qa/done</code> label is set, it will skip the creation of a QA card related to this PR during next release process (example: documentation-only PRs).</li> <li><code>major_change</code>: to flag the PR as a major change impacting many/all teams working on the agent and will require deeper QA (example: when we change the Python version shipped in the agent).</li> <li><code>need-change/operator</code>, <code>need-change/helm</code>: indicate that the configuration needs to be modified in the operator / helm chart as well.</li> <li><code>k8s/&lt;min-version&gt;</code>: indicate the lowest Kubernetes version compatible with the PR's feature.</li> <li><code>backport/&lt;branch-name&gt;</code>: Add this label to have your changes automatically backported to <code>&lt;branch-name&gt;</code>.</li> </ul>"},{"location":"guidelines/contributing/#integrations","title":"Integrations","text":"<p>Also called checks, all officially supported Agent integrations live in the integrations-core repo. Please look there to submit related issues, PRs, or review the latest changes. For new integrations, please open a pull request in the integrations-extras repo.</p>"},{"location":"guidelines/docs/","title":"Writing developer docs","text":"<p>This site is built by MkDocs and uses the Material for MkDocs theme.</p> <p>You can serve documentation locally with the <code>docs.serve</code> invoke task.</p>"},{"location":"guidelines/docs/#organization","title":"Organization","text":"<p>The site structure is defined by the <code>nav</code> key in the <code>mkdocs.yml</code> file.</p> <p>When adding new pages, first think about what it is exactly that you are trying to document. For example, if you intend to write about something everyone must follow as a standard practice it would be classified as a guideline whereas a short piece about performing a particular task would be a how-to.</p> <p>After deciding the kind of content, strive to further segment the page under logical groupings for easier navigation.</p>"},{"location":"guidelines/docs/#line-continuations","title":"Line continuations","text":"<p>For prose where the rendered content should have no line breaks, always keep the Markdown on the same line. This removes the need for any stylistic enforcement and allows for IDEs to intelligently wrap as usual.</p> <p>Tip</p> <p>When you wish to force a line continuation but stay within the block, indent by 2 spaces from the start of the text and end the block with a new line. For example, the following shows how you would achieve a multi-line ordered list item:</p> Markdown <pre><code>1. first line\n\n     second line\n\n1. third line\n</code></pre> Rendered <ol> <li> <p>first line</p> <p>second line</p> </li> <li> <p>third line</p> </li> </ol>"},{"location":"guidelines/docs/#emphasis","title":"Emphasis","text":"<p>When you want to call something out, use admonitions rather than making large chunks of text bold or italicized. The latter is okay for small spans within sentences.</p> <p>Here's an example:</p> <ul> <li> <p>Markdown</p> <pre><code>!!! info\n    Lorem ipsum ...\n</code></pre> </li> <li> <p>Rendered</p> <p>Info</p> <p>Lorem ipsum ...</p> </li> </ul>"},{"location":"guidelines/docs/#links","title":"Links","text":"<p>Always use inline links rather than reference links.</p> <p>The only exception to that rule is links that many pages may need to reference. Such links may be added to this file that all pages are able to reference.</p>"},{"location":"guidelines/docs/#abbreviations","title":"Abbreviations","text":"<p>Abbreviations like DSD may be added to this file which will make it so that a tooltip will be displayed on hover.</p>"},{"location":"guidelines/components/defining-apps/","title":"Defining Apps and Binaries","text":""},{"location":"guidelines/components/defining-apps/#binaries","title":"Binaries","text":"<p>Each binary is defined as a <code>main</code> package in the <code>cmd/</code> directory, such as <code>cmd/iot-agent</code>. This top-level package contains only a simple <code>main</code> function (or often, one for Windows and one for *nix) which performs any platform-specific initialization and then creates and executes a Cobra command.</p>"},{"location":"guidelines/components/defining-apps/#binary-size","title":"Binary Size","text":"<p>Consider carefully the tree of Go imports that begins with the <code>main</code> package. While the Go linker does some removal of unused symbols, the safest means to ensure a particular package isn't occuping space in the resulting binary is to not include it.</p>"},{"location":"guidelines/components/defining-apps/#simple-binaries","title":"Simple Binaries","text":"<p>A \"simple binary\" here is one that does not have subcommands.</p> <p>The Cobra configuration for the binary is contained in the <code>command</code> subpackage of the main package (<code>cmd/&lt;binary&gt;/command</code>). The <code>main</code> function calls this package to create the command, and then executes it:</p>  cmd/&lt;binary&gt;/main.go <pre><code>func main() {\n    if err := command.MakeCommand().Execute(); err != nil {\n        os.Exit(-1)\n    }\n}\n</code></pre> <p>The <code>command.MakeCommand</code> function creates the <code>*cobra.Command</code> for the binary, with a <code>RunE</code> field that defines an app, as described below.</p>"},{"location":"guidelines/components/defining-apps/#binaries-with-subcommands","title":"Binaries With Subcommands","text":"<p>Many binaries have a collection of subcommands, along with some command-line flags defined at the binary level. For example, the <code>agent</code> binary has subcommands like <code>agent flare</code> or <code>agent diagnose</code> and accepts global <code>--cfgfile</code> and <code>--no-color</code> arguments.</p> <p>As with simple binaries, the top-level Cobra command is defined by a <code>MakeCommand</code> function in <code>cmd/&lt;binary&gt;/command</code>. This <code>command</code> package should also define a <code>GlobalParams</code> struct and a <code>SubcommandFactory</code> type:</p>  cmd/&lt;binary&gt;/command/command.go <pre><code>// GlobalParams contains the values of agent-global Cobra flags.\n//\n// A pointer to this type is passed to SubcommandFactory's, but its contents\n// are not valid until Cobra calls the subcommand's Run or RunE function.\ntype GlobalParams struct {\n    // ConfFilePath holds the path to the folder containing the configuration\n    // file, to allow overrides from the command line\n    ConfFilePath string\n\n    // ...\n}\n\n// SubcommandFactory is a callable that will return a slice of subcommands.\ntype SubcommandFactory func(globalParams *GlobalParams) []*cobra.Command\n</code></pre> <p>Each subcommand is implemented in a subpackage of <code>cmd/&lt;binary&gt;/subcommands</code>, such as <code>cmd/&lt;binary&gt;/subcommands/version</code>. Each such subpackage contains a <code>command.go</code> defining a <code>Commands</code> function that defines the subcommands for that package:</p>  cmd/&lt;binary&gt;/subcommands/&lt;command&gt;/command.go <pre><code>func Commands(globalParams *command.GlobalParams) []*cobra.Command {\n    cmd := &amp;cobra.Command { .. }\n    return []*cobra.Command{cmd}\n}\n</code></pre> <p>While <code>Commands</code> typically returns only one command, it may make sense to return multiple commands when the implementations share substantial amounts of code, such as starting, stopping and restarting a service.</p> <p>The <code>main</code> function supplies a slice of subcommand factories to <code>command.MakeCommand</code>, which calls each one and adds the resulting subcommands to the root command.</p>  cmd/&lt;binary&gt;/main.go <pre><code>subcommandFactories := []command.SubcommandFactory{\n    frobnicate.Commands,\n    ...,\n}\nif err := command.MakeCommand(subcommandFactories).Execute(); err != nil {\n    os.Exit(-1)\n}\n</code></pre> <p>The <code>GlobalParams</code> type supports Cobra arguments that are global to all subcommands. It is passed to each subcommand factory so that the defined <code>RunE</code> callbacks can access these arguments. If the binary has no global command-line arguments, it's OK to omit this type.</p> <pre><code>func MakeCommand(subcommandFactories []SubcommandFactory) *cobra.Command {\n    globalParams := GlobalParams{}\n\n    cmd := &amp;cobra.Command{ ... }\n    cmd.PersistentFlags().StringVarP(\n        &amp;globalParams.ConfFilePath, \"cfgpath\", \"c\", \"\",\n        \"path to directory containing datadog.yaml\")\n\n    for _, sf := range subcommandFactories {\n        subcommands := sf(&amp;globalParams)\n        for _, cmd := range subcommands {\n            agentCmd.AddCommand(cmd)\n        }\n    }\n\n    return cmd\n}\n</code></pre> <p>If the available subcommands depend on build flags, move the creation of the subcommand factories to the <code>subcommands/&lt;command&gt;</code> package and create the slice there using source files with <code>//go:build</code> directives. Your factory can return <code>nil</code> if your command is not compatible with the current build flag. In all cases, the subcommands build logic should be constrained to its package. See <code>cmd/agent/subcommands/jmx/command_nojmx.go</code> for an example.</p>"},{"location":"guidelines/components/defining-apps/#apps","title":"Apps","text":"<p>Apps map directly to <code>fx.App</code> instances, and as such they define a set of provided components and instantiate some of them.</p> <p>The <code>fx.App</code> is always created after Cobra has parsed the command-line, within a <code>cobra.Command#RunE</code> function. This means that the components supplied to an app, and any BundleParams values, are specific to the invoked command or subcommand.</p>"},{"location":"guidelines/components/defining-apps/#one-shot-apps","title":"One-Shot Apps","text":"<p>A one-shot app is one which performs some task and exits, such as <code>agent status</code>. The <code>pkg/util/fxutil.OneShot</code> helper function provides a convenient shorthand to run a function only after all components have started. Use it like this:</p> <pre><code>cmd := cobra.Command{\n    Use: \"foo\", ...,\n    RunE: func(cmd *cobra.Command, args []string) error {\n        return fxutil.OneShot(run,\n            fx.Supply(core.BundleParams{}),\n            core.Bundle(),\n            ..., // any other bundles needed for this app\n        )\n    },\n}\n\nfunc run(log log.Component) error {\n    log.Debug(\"foo invoked!\")\n    ...\n}\n</code></pre> <p>The <code>run</code> function typically also needs some command-line values. To support this, create a (sub)command-specific <code>cliParams</code> type containing the required values, and embedding a pointer to GlobalParams:</p> <pre><code>type cliParams struct {\n    *command.GlobalParams\n    useTLS bool\n    args []string\n}\n</code></pre> <p>Populate this type within <code>Commands</code>, supply it as an Fx value, and require that value in the <code>run</code> function:</p> <pre><code>func Commands(globalParams *command.GlobalParams) []*cobra.Command {\n    cliParams := &amp;cliParams{\n        GlobalParams: globalParams,\n    }\n    var useTLS bool\n    cmd := cobra.Command{\n        Use: \"foo\", ...,\n        RunE: func(cmd *cobra.Command, args []string) error {\n            cliParams.args = args\n            return fxutil.OneShot(run,\n                fx.Supply(cliParams),\n                fx.Supply(core.CreateaBundleParams()),\n                core.Bundle(),\n                ..., // any other bundles needed for this app\n            )\n        },\n    }\n    cmd.PersistentFlags().BoolVarP(&amp;cliParams.useTLS, \"usetls\", \"\", \"\", \"force TLS use\")\n\n    return []*cobra.Command{cmd}\n}\n\nfunc run(cliParams *cliParams, log log.Component) error {\n    if (cliParams.Verbose) {\n        log.Info(\"executing foo\")\n    }\n    ...\n}\n</code></pre> <p>This example includes cli params drawn from GlobalParams (<code>Verbose</code>), from subcommand-specific args (<code>useTLS</code>), and from Cobra (<code>args</code>).</p>"},{"location":"guidelines/components/defining-apps/#daemon-apps","title":"Daemon Apps","text":"<p>A daemon app is one that runs \"forever\", such as <code>agent run</code>. Use the <code>fxutil.Run</code> helper function for this variety of app:</p> <pre><code>cmd := cobra.Command{\n    Use: \"foo\", ...,\n    RunE: func(cmd *cobra.Command, args []string) error {\n        return fxutil.Run(\n            fx.Supply(core.BundleParams{}),\n            core.Bundle(),\n            ..., // any other bundles needed for this app\n            fx.Supply(foo.BundleParams{}),\n            foo.Bundle(), // the bundle implementing this app\n        )\n    },\n}\n</code></pre>"},{"location":"guidelines/components/defining-bundles/","title":"Defining Component Bundles","text":"<p>A bundle is defined in a dedicated package named <code>comp/&lt;bundleName&gt;</code>. The package must have the following defined in <code>bundle.go</code>:</p> <ul> <li>Extensive package-level documentation. This should define:<ul> <li>The purpose of the bundle</li> <li>What components are and are not included in the bundle. Components might be omitted in the interest of binary size, as discussed in the component overview.</li> <li>Which components are automatically instantiated.</li> <li>Which other bundles this bundle depends on. Bundle dependencies are always expressed at a bundle level.</li> </ul> </li> <li>A team-name comment of the form <code>// team: &lt;teamname&gt;</code>. This is used to generate CODEOWNERS information.</li> <li>An optional <code>BundleParams</code> -- the type of the bundle's parameters (see below). This item should have a formulaic doc string like <code>// BundleParams defines the parameters for this bundle.</code></li> <li><code>Bundle</code> -- an <code>fx.Option</code> that can be included in an <code>fx.App</code> to make this bundle's components available. To assist with debugging, use <code>fxutil.Bundle(options...)</code>. Use <code>fx.Invoke(func(componentpkg.Component) {})</code> to instantiate components automatically. This item should have a formulaic doc string like <code>// Module defines the fx options for this component.</code></li> </ul> <p>Typically, a bundle will automatically instantiate the top-level components that represent the bundle's purpose. For example, the trace-agent bundle <code>comp/trace</code> might automatically instantiate <code>comp/trace/agent</code>.</p> <p>You can use the invoke task <code>inv components.new-bundle comp/&lt;bundleName&gt;</code> to generate a pre-filled <code>bundle.go</code> file for the given bundle.</p>"},{"location":"guidelines/components/defining-bundles/#bundle-parameters","title":"Bundle Parameters","text":"<p>Apps can provide some intialization-time parameters to bundles. These parameters are limited to two kinds:</p> <ul> <li>Parameters specific to the app, such as whether to start a network server; and</li> <li>Parameters from the environment, such as command-line options.</li> </ul> <p>Anything else is runtime configuration and should be handled vi <code>comp/core/config</code> or another mechanism.</p> <p>Bundle parameters must stored only <code>Params</code> types for sub components. The reason is that each sub component must be usable without <code>BundleParams</code>.</p>  comp/&lt;bundleName&gt;/bundle.go <pre><code>import \".../comp/&lt;bundleName&gt;/foo\"\nimport \".../comp/&lt;bundleName&gt;/bar\"\n// ...\n\n// BundleParams defines the parameters for this bundle.\ntype BundleParams struct {\n    Foo foo.Params\n    Bar bar.Params\n}\n\nvar Bundle = fxutil.Bundle(\n    // You must tell to fx how to get foo.Params from BundleParams.\n    fx.Provide(func(params BundleParams) foo.Params { return params.Foo }),\n    foo.Module(),\n    // You must tell to fx how to get bar.Params from BundleParams.\n    fx.Provide(func(params BundleParams) bar.Params { return params.Bar }),\n    bar.Module(),\n)\n</code></pre>"},{"location":"guidelines/components/defining-bundles/#testing","title":"Testing","text":"<p>A bundle should have a test file, <code>bundle_test.go</code>, to verify the documentation's claim about its dependencies. This simply uses <code>fxutil.TestBundle</code> to check that all dependencies are satisfied when given the full set of required bundles.</p>  bundle_test.go <pre><code>func TestBundleDependencies(t *testing.T) {\n    fxutil.TestBundle(t, Bundle)\n}\n</code></pre>"},{"location":"guidelines/components/defining-components/","title":"Defining Components","text":"<p>You can use the invoke task <code>inv components.new-component comp/&lt;bundleName&gt;/&lt;component&gt;</code> to generate a scaffold for your new component.</p> <p>Below is a description of the different folders and files of your component.</p> <p>Every public variable, function, struct, and interface of your component must be documented. Please refer to the Documentation section below for details.</p> <p>A component is defined in a dedicated package named <code>comp/&lt;bundleName&gt;/&lt;component&gt;</code>, where <code>&lt;bundleName&gt;</code> names the bundle that contains the component. The package must have the following defined in:</p> <ul> <li> <p><code>comp/&lt;bundleName&gt;/&lt;component&gt;/component.go</code></p> <ul> <li> <p>A team-name comment of the form <code>// team: &lt;teamname&gt;</code>. This is used to generate CODEOWNERS information.</p> </li> <li> <p><code>Component</code> -- The component interface. This is the interface that other components can reference when declaring the component as a dependency via <code>fx</code>. It can be an empty interface, if there is no need for any methods.</p> </li> </ul> </li> <li> <p><code>comp/&lt;bundleName&gt;/&lt;component&gt;/&lt;component&gt;impl/&lt;component&gt;.go</code></p> <ul> <li><code>Module</code> -- an <code>fx.Option</code> that can be included in the bundle's <code>Module</code> or an <code>fx.App</code> to make this component available. The <code>Module</code> is defined in a separate package from the component, allowing a package to import the interface without having to import the entire implementation. To assist with debugging, declare your Module using <code>fxutil.Component(options...)</code>.</li> </ul> </li> </ul> <p>Warning</p> <p>Components should not be nested; that is, no component's Go path should be a prefix of another component's Go path.</p>"},{"location":"guidelines/components/defining-components/#implementation","title":"Implementation","text":"<p>The Component interface and the <code>Module</code> definition are implemented in the file <code>comp/&lt;bundleName&gt;/&lt;component&gt;/&lt;component&gt;impl/&lt;component&gt;.go</code>.</p> <p>Important</p> <p>The Module definition function must be private.</p>  comp/&lt;bundleName&gt;/&lt;component&gt;/&lt;component&gt;impl/&lt;component&gt;.go <pre><code>package config\n\n// Module defines the fx options for this component.\nfunc Module() fxutil.Module {\n    return fxutil.Component(\n        fx.Provide(newFoo),\n    )\n}\n\ntype foo struct {\n    foos []string\n}\n\ntype dependencies struct {\n    fx.In\n\n    Log log.Component\n    Config config.Component\n    // ...\n}\n\ntype provides struct {\n    fx.Out\n\n    Comp comp.Component\n    // ...\n}\n\nfunc newFoo(deps dependencies) Component { ...  }\n\n// foo implements Component#Foo.\nfunc (f *foo) Foo(key string) provides { ... }\n</code></pre> <p>The constructor <code>newFoo</code> is an <code>fx</code> constructor. It can refer to other dependencies and expect them to be automatically supplied via <code>fx</code>.</p> <p>See Using Components for more details.</p> <p>The constructor can return either a <code>Component</code>, if it is infallible, or <code>(Component, error)</code>, if it could fail. In the second form, a non-nil error will crash the agent at startup with a message containing the error. It is possible, and often necessary, to return multiple values. If the list of return values grows unwieldy, <code>fx.Out</code> can be used to create an output struct.</p> <p>The constructor may call methods on other components, as long as the called method's documentation indicates it is OK.</p>"},{"location":"guidelines/components/defining-components/#testing-support","title":"Testing Support","text":"<p>To support testing, components can optionally provide a mock implementation, with the following in:</p> <ul> <li><code>comp/&lt;bundleName&gt;/&lt;component&gt;/component_mock.go</code><ul> <li><code>Mock</code> -- the type implemented by the mock version of the component. This should embed <code>pkg.Component</code>, and provide additional exported methods for manipulating the mock for use by other packages.</li> </ul> </li> <li><code>comp/&lt;bundleName&gt;/&lt;component&gt;/&lt;component&gt;impl/&lt;component&gt;_mock.go</code><ul> <li><code>MockModule</code> -- an <code>fx.Option</code> that can be included in a test <code>App</code> to get the component's mock implementation.</li> </ul> </li> </ul>  comp/&lt;bundleName&gt;/&lt;component&gt;/&lt;component_mock.go <pre><code>//go:build test\n\npackage foo\n\n// Mock implements mock-specific methods.\ntype Mock interface {\n    // Component methods are included in Mock.\n    Component\n\n    // AddedFoos returns the foos added by AddFoo calls on the mock implementation.\n    AddedFoos() []Foo\n}\n</code></pre>  comp/&lt;bundleName&gt;/&lt;component&gt;/&lt;component&gt;impl/&lt;component&gt;_mock.go <pre><code>//go:build test\n\npackage foo\n\n// MockModule defines the fx options for the mock component.\nfunc MockModule() fxutil.Module {\n    return fxutil.Component(\n        fx.Provide(newMock),\n    )\n}\n</code></pre> <pre><code>type mock struct { ... }\n\n// Foo implements Component#Foo.\nfunc (m *mock) Foo(key string) string { ... }\n\n// AddedFoos implements Mock#AddedFoos.\nfunc (m *mock) AddedFoos() []Foo { ... }\n\nfunc newMock(deps dependencies) Component {\n    return &amp;mock{ ... }\n}\n</code></pre> <p>Users of the mock module can cast the <code>Component</code> to a <code>Mock</code> to access the mock methods, as described in Using Components.</p>"},{"location":"guidelines/components/defining-components/#documentation","title":"Documentation","text":"<p>The documentation (both package-level and method-level) should include everything a user of the component needs to know. In particular, any assumptions that might lead to panics if violated by the user should be documented.</p> <p>Detailed documentation of how to avoid bugs in using a component is an indicator of excessive complexity and should be treated as a bug. Simplifying the usage will improve the robustness of the Agent.</p> <p>Documentation should include:</p> <ul> <li>Precise information on when each method may be called. Can methods be called concurrently? Are some methods invalid before the component has started? Such assumptions are difficult to verify. Where possible, try to make every method callable concurrently, at all times.</li> <li> <p>Precise information about data ownership of passed values and returned values. Users can assume that any mutable value returned by a component will not be modified by the user or the component after it is returned. Similarly, any mutable value passed to a component will not be later modified either by the component or the caller. Any deviation from these defaults should be documented.</p> <p>Note</p> <p>It can be surprisingly hard to avoid mutating data -- for example, <code>append(..)</code> surprisingly mutates its first argument. It is also hard to detect these bugs, as they are often intermittent, cause silent data corruption, or introduce rare data races. Where performance is not an issue, prefer to copy mutable input and outputs to avoid any potential bugs.</p> </li> <li> <p>Precise information about goroutines and blocking. Users can assume that methods do not block indefinitely, so blocking methods should be documented as such. Methods that invoke callbacks should be clear about how the callback is invoked, and what it might do. For example, document whether the callback can block, and whether it might be called concurrently with other code.</p> </li> <li>Precise information about channels. Is the channel buffered? What happens if the channel is not read from quickly enough, or if reading stops? Can the channel be closed by the sender, and if so, what does that mean?</li> </ul>"},{"location":"guidelines/components/purpose/","title":"Purpose of component guidelines","text":"<p>This section describes the mechanics of implementing apps, components, and bundles.</p> <p>The guidelines are quite prescriptive, with the intent of making all components \"look the same\". This reduces cognitive load when using components -- no need to remember one component's peculiarities. It also allows Agent-wide changes, where we make the same formulaic change to each component. If a situation arises that contradicts the guidelines, then we can update the guidelines (and change all affected components).</p>"},{"location":"guidelines/components/registrations/","title":"Component Registrations","text":"<p>Components generally need to talk to one another! In simple cases, that occurs by method calls. But in many cases, a single component needs to communicate with a number of other components that all share some characteristics. For example, the <code>comp/core/health</code> component monitors the health of many other components, and <code>comp/workloadmeta/scheduler</code> provides workload events to an arbitrary number of subscribers.</p> <p>The convention in the Agent codebase is to use value groups to accomplish this. The collecting component requires a slice of some collected type, and the providing components provide values of that type. Consider an example case of an HTTP server component to which endpoints can be attached. The server is the collecting component, requiring a slice of type <code>[]*endpoint</code>, where <code>*endpoint</code> is the collected type. Providing components provide values of type <code>*endpoint</code>.</p> <p>The convention is to \"wrap\" the collected type in a <code>Registration</code> struct type which embeds <code>fx.Out</code> and has tag <code>group:\"pkgname\"</code>, where <code>pkgname</code> is the short package name (Fx requires a group name, and this is as good as any). This helps providing components avoid the common mistake of omitting the tag. Because it is wrapped in an exported <code>Registration</code> type, the collected type can be an unexported type, as in the example below.</p> <p>The collecting component should define the registration type and a constructor for it:</p>  comp/server/component.go <pre><code>// ...\n// Server endpoints are provided by other components, by providing a server.Registration\n// instance.\n// ...\npackage server\n\ntype endpoint struct {  // (the collected type)\n    ...\n}\n\ntype Registration struct {\n    fx.Out\n\n    Endpoint endpoint `group:\"server\"`\n}\n\n// NewRegistration creates a new Registration instance for the given endpoint.\nfunc NewRegistration(route string, handler func()) Registration { ... }\n</code></pre> <p>Its implementation then requires a slice of the collected type (<code>endpoint</code>), again using <code>group:\"server\"</code>:</p>  comp/server/server.go <pre><code>// endpoint defines an endpoint on this server.\ntype endpoint struct { ... }\n\ntype dependencies struct {\n    fx.In\n\n    Registrations []endpoint `group:\"server\"`\n}\n\nfunc newServer(deps dependencies) Component {\n    // ...\n    for _, e := range deps.Registrations {\n        if e.handler == nil {\n            continue\n        }\n        // ...\n    }\n    // ...\n}\n</code></pre> <p>It's good practice to ignore zero values, as that allows providing components to skip the registration if desired.</p> <p>Finally, the providing component (in this case, <code>foo</code>) includes a registration in its output as an additional provided type, beyond its <code>Component</code> type:</p>  comp/foo/foo.go <pre><code>func newFoo(deps dependencies) (Component, server.Registration) {\n    // ...\n    return foo, server.NewRegistration(\"/things/foo\", foo.handler)\n}\n</code></pre> <p>This technique has some caveats to be aware of:</p> <ul> <li>The providing components are instantiated before the collecting component.</li> <li>Fx treats value groups as the collecting component depending on all of the providing components. This means that the providing components cannot depend on the collecting component, as this would represent a dependency cycle.</li> <li>Fx will instantiate all declared providing components before the collecting component, regardless of whether their <code>Component</code> type is required. This may lead to components being instantiated in unexpected circumstances.</li> </ul>"},{"location":"guidelines/components/subscriptions/","title":"Component Subscriptions","text":"<p>Subscriptions are a common form of registration, and have support in the <code>pkg/util/subscriptions</code> package.</p> <p>In defining subscriptions, the component that transmits messages is the collecting component, and the processes receiving components are the providing components. These are matched using the message type, which must be unique across the codebase, and should not be a built-in type like <code>string</code>. Providing components provide a <code>subscriptions.Receiver[coll.Message]</code> which has a <code>Ch</code> channel from which to receive messages. Collecting components require a <code>subscriptions.Transmitter[coll.Message]</code> which has a <code>Notify</code> method to send messages.</p>  announcer/component.go announcer/announcer.go listener/listener.go <pre><code>// ...\n// To subscribe to these announcements, provide a subscriptions.Subscription[announcer.Announcement].\n// ...\npackage announcer\n</code></pre> <pre><code>func newAnnouncer(tx subscriptions.Transmitter[Anouncement]) Component {\n    return &amp;announcer{announcementTx: tx}  // (store the transmitter)\n}\n\n// ... later send messages with\nfunc (ann *announcer) announce(a announcement) {\n    ann.annoucementTx.Notify(a)\n}\n</code></pre> <pre><code>func newListener() (Component, subscriptions.Receiver[announcer.Announcement]) {\n    rx := subscriptions.Receiver[Event]() // create a receiver\n    return &amp;listener{announcementRx: rx}, rx  // capture the receiver _and_ return it\n}\n\n// ... later receive messages (usually in an actor's main loop)\nfunc (l *listener) run() {\n    loop {\n        select {\n        case a := &lt;- l.announcementRx.Ch:\n            ...\n        }\n    }\n}\n</code></pre> <p>Any component receiving messages via a subscription will automatically be instantiated by Fx if it is delcared in the app, regardless of whether its Component type is required by some other component. The workaround for this is to return a zero-valued Receiver when the component does not actually wish to receive messages (such as when the component is disabled by user configuration).</p> <p>If a receiving component does not subscribe (for example, if it is not started), it can return the zero value, <code>subscriptions.Receiver[Event]{}</code>, from its constructor. If a component returns a non-nil subscriber, it must consume messages from the receiver or risk blocking the transmitter.</p> <p>See the <code>pkg/util/subscriptions</code> documentation for more details.</p>"},{"location":"guidelines/components/using-components/","title":"Using Components and Bundles","text":""},{"location":"guidelines/components/using-components/#component-dependencies","title":"Component Dependencies","text":"<p>Component dependencies are automatically determined from the arguments to a component constructor. Most components have a few dependencies, and use a struct named <code>dependencies</code> to represent them:</p> <pre><code>type dependencies struct {\n    fx.In\n\n    Lc fx.Lifecycle\n    Params internal.BundleParams\n    Config config.Module\n    Log log.Module\n    // ...\n}\n\nfunc newThing(deps dependencies) Component {\n    t := &amp;thing{\n        log: deps.Log,\n        ...\n    }\n    deps.Lc.Append(fx.Hook{OnStart: t.start})\n    return t\n}\n</code></pre>"},{"location":"guidelines/components/using-components/#testing","title":"Testing","text":"<p>Testing for a component should use <code>fxtest</code> to create the component. This focuses testing on the API surface of the component against which other components will be built. Per-function unit tests are, of course, also great where appropriate!</p> <p>Here's an example testing a component with a mocked dependency on <code>other</code>:</p> <pre><code>func TestMyComponent(t *testing.T) {\n    var comp Component\n    var other other.Component\n    app := fxtest.New(t,\n        Module,              // use the real version of this component\n        other.MockModule(),  // use the mock version of other\n        fx.Populate(&amp;comp),  // get the instance of this component\n        fx.Populate(&amp;other), // get the (mock) instance of the other component\n    )\n\n    // start and, at completion of the test, stop the components\n    defer app.RequireStart().RequireStop()\n\n    // cast `other` to its mock interface to call mock-specific methods on it\n    other.(other.Mock).SetSomeValue(10)                      // Arrange\n    comp.DoTheThing()                                        // Act\n    require.Equal(t, 20, other.(other.Mock).GetSomeResult()) // Assert\n}\n</code></pre> <p>If the component has a mock implementation, it is a good idea to test that mock implementation as well.</p>"},{"location":"how-to/components/migration/","title":"Migrating to Components","text":"<p>After your component has been created you can link it to other components such as flares (others like status pages, or health will come later).</p> <p>This page documents how to fully integrate your component in the Agent life cycle.</p>"},{"location":"how-to/components/migration/#flare","title":"Flare","text":"<p>The general idea is to register a callback within your component to be called each time a flare is created. This uses Fx groups under the hood, but helpers are there to abstract all of that for you.</p> <p>Then, migrate the code related to your component's domain from <code>pkg/flare</code> to your component and delete it from <code>pkg/flare</code> once done.</p>"},{"location":"how-to/components/migration/#creating-a-callback","title":"Creating a callback","text":"<p>To add data to a flare you will first need to register a callback, aka a <code>FlareBuilder</code>.</p> <p>Within your component create a method with the following signature <code>func (c *yourComp) fillFlare(fb flaretypes.FlareBuilder) error</code>.</p> <p>This function is called every time the Agent generates a flare, either from the CLI or from the running Agent. This callback receives a <code>comp/core/flare/helpers:FlareBuilder</code>. The <code>FlareBuilder</code> interface provides all the helpers functions needed to add data to a flare (adding files, copying directories, scrubbing data, and so on).</p> <p>Example:</p> <pre><code>import (\n    yaml \"gopkg.in/yaml.v2\"\n\n    flaretypes \"github.com/DataDog/datadog-agent/comp/core/flare/types\"\n)\n\nfunc (c *myComponent) fillFlare(fb flaretypes.FlareBuilder) error {\n    fb.AddFileFromFunc(\n        \"runtime_config_dump.yaml\",\n        func () ([]byte, error) {\n            return yaml.Marshal(c.AllSettings())\n        },\n    )\n\n    fb.CopyFile(\"/etc/datadog-agent/datadog.yaml\")\n    return nil\n}\n</code></pre> <p>Read the package documentation for <code>FlareBuilder</code> for more information on the API.</p> <p>All errors returned by the <code>FlareBuilder</code> are automatically added to a log file shipped within the flare. Ship as much data as possible in a flare instead of stopping at the first error. Returning an error does not stop the flare from being created or sent.</p> <p>While you can register multiple callbacks from the same component, keep all the flare code in a single callback.</p>"},{"location":"how-to/components/migration/#register-your-callback","title":"Register your callback","text":"<p>Now you need to register your callback to be called each time a flare is created. To do so your component constructor need to provide a new <code>comp/core/flare/helpers:Provider</code>. Use <code>comp/core/flare/helpers:NewProvider</code> for this.</p> <p>Example: <pre><code>import (\n    flaretypes \"github.com/DataDog/datadog-agent/comp/core/flare/types\"\n)\n\ntype provides struct {\n    fx.Out\n\n    // [...]\n    FlareProvider flaretypes.Provider // Your component will provides a new Provider\n    // [...]\n}\n\nfunc newComponent(deps dependencies) (provides, error) {\n    // [...]\n    return provides{\n        // [...]\n        FlareProvider: flaretypes.NewProvider(myComponent.fillFlare), // NewProvider will wrap your callback in order to be use as a 'Provider'\n        // [...]\n    }, nil\n}\n</code></pre></p>"},{"location":"how-to/components/migration/#migrating-your-code","title":"Migrating your code","text":"<p>Now migrate the require code from <code>pkg/flare</code> to you component callback. The code in <code>pkg/flare</code> already uses the <code>FlareBuilder</code> interface, simplifying migration. Don't forget to migrate the tests too and expand them (most of the flare features are not tested). <code>comp/core/flare/helpers::NewFlareBuilderMock</code> will provides helpers for your tests.</p> <p>Keep in mind that the goal is to delete <code>pkg/flare</code> once the migration to component is done.</p>"}]}