# SPDX-FileCopyrightText: 2025-present Datadog, Inc. <dev@datadoghq.com>
#
# SPDX-License-Identifier: MIT
from __future__ import annotations

from pathlib import Path
from typing import TYPE_CHECKING

import click
from ci_utils import (
    FileReader,
    GitFileReader,
    LocalFileReader,
    Pipeline,
    PipelinesConfig,
    dump_yaml,
    get_commit_info,
    merge_pipeline_configs,
)
from dda.cli.base import dynamic_command, pass_app
from utils.changes import get_changed_files

if TYPE_CHECKING:
    from dda.cli.application import Application


def find_project_root(start: Path) -> Path | None:
    """Find the project root by looking for .git folder."""
    current = start
    while current != current.parent:
        if (current / ".git").exists():
            return current
        current = current.parent
    return None


def build_header(
    pipelines: list[Pipeline],
    project_root: Path,
    ref: str | None = None,
) -> str:
    """Build the header comment for the generated file."""
    lines = [
        "Generated by: dda ci generate",
        f"Pipelines: {', '.join(p.name for p in pipelines)}",
    ]
    if ref:
        commit_info = get_commit_info(project_root, ref)
        lines.append(f"Git ref: {ref} ({commit_info.get('sha', '')[:12]})")
    lines.append("DO NOT EDIT - This file is auto-generated")
    return "\n".join(lines)


def load_pipelines_config(
    project_root: Path,
    file_reader: FileReader,
) -> PipelinesConfig:
    """
    Load pipelines config from .pipelines folder.

    Args:
        project_root: Root path of the project.
        file_reader: FileReader for reading files.
    """
    # Load from file reader
    pipelines = []
    files = file_reader.list_files(".pipelines")
    for file_path in files:
        if not file_path.endswith((".yml", ".yaml")):
            continue
        full_path = f".pipelines/{file_path}"
        try:
            data = file_reader.load_yaml(full_path)
        except FileNotFoundError:
            continue
        if not data:
            continue
        if "name" not in data:
            data["name"] = Path(file_path).stem
        pipeline = PipelinesConfig._parse_pipeline_data(data)
        if pipeline.name:
            pipelines.append(pipeline)
    return PipelinesConfig(pipelines=pipelines)


@dynamic_command(
    short_help="Generate CI pipeline",
)
@click.option(
    "--output",
    "-o",
    "output_file",
    type=str,
    default="generated-pipeline.yml",
    help="Output file for the generated pipeline (default: generated-pipeline.yml)",
)
@click.option(
    "--resolve-includes/--no-resolve-includes",
    "should_resolve_includes",
    default=True,
    help="Whether to resolve local include directives (default: true)",
)
@click.option(
    "--resolve-extends/--no-resolve-extends",
    "should_resolve_extends",
    default=True,
    help="Whether to resolve extends directives (default: true)",
)
@click.option(
    "--pipeline",
    "-p",
    "pipeline_names",
    type=str,
    multiple=True,
    help="Generate only specific pipeline(s) (can be specified multiple times)",
)
@click.option(
    "--all",
    "generate_all",
    is_flag=True,
    default=False,
    help="Generate all pipelines merged together",
)
@click.option(
    "--ref",
    type=str,
    default=None,
    help="Git ref (branch, tag, or commit) to generate the pipeline from",
)
@click.option(
    "--show-pipelines",
    is_flag=True,
    default=False,
    help="Show available pipelines without generating",
)
@click.option(
    "--filter-by-changes",
    is_flag=True,
    default=False,
    help="Only generate pipelines affected by changed files",
)
@click.option(
    "--compare-branch",
    type=str,
    default="main",
    help="Branch to compare against for change detection (default: main)",
)
@click.option(
    "--include-all-triggered-pipelines",
    is_flag=True,
    default=False,
    help="Include all triggered pipelines in the output (outputs multi-entry YAML)",
)
@pass_app
def cmd(
    app: Application,
    *,
    output_file: str,
    should_resolve_includes: bool,
    should_resolve_extends: bool,
    pipeline_names: tuple[str, ...],
    generate_all: bool,
    ref: str | None,
    show_pipelines: bool,
    filter_by_changes: bool,
    compare_branch: str,
    include_all_triggered_pipelines: bool,
) -> None:
    """
    Generate GitLab CI pipeline configuration.

    This command reads pipeline configurations from .pipelines/ folder and
    merges their GitLab CI entrypoint files into a single pipeline.

    Examples:

    \b
    # Generate all pipelines merged together
    dda ci generate --all

    \b
    # Generate a specific pipeline
    dda ci generate --pipeline main

    \b
    # Generate only pipelines affected by changed files
    dda ci generate --filter-by-changes

    \b
    # Generate pipeline from a specific git ref (branch/tag/commit)
    dda ci generate --all --ref main
    dda ci generate --all --ref v7.50.0
    dda ci generate --all --ref abc123def

    \b
    # Show available pipelines
    dda ci generate --show-pipelines

    \b
    # Show pipelines available at a specific ref
    dda ci generate --show-pipelines --ref main

    \b
    # Generate all pipelines including triggered sub-pipelines
    dda ci generate --all --include-all-triggered-pipelines
    """
    # Find project root
    project_root = find_project_root(Path.cwd())
    if not project_root:
        project_root = Path.cwd()

    # Set up file reader (git or local)
    file_reader: FileReader
    if ref:
        try:
            file_reader = GitFileReader(project_root, ref, app)
            commit_info = get_commit_info(project_root, ref)
            app.display_info(f"Reading from git ref: {ref}")
            if commit_info:
                app.display_info(f"  Commit: {commit_info.get('sha', '')[:12]}")
                app.display_info(f"  Author: {commit_info.get('author', '')}")
                app.display_info(f"  Date: {commit_info.get('date', '')}")
                app.display_info(f"  Message: {commit_info.get('message', '')}")
            app.display_info("")
        except ValueError as e:
            app.abort(str(e))
    else:
        file_reader = LocalFileReader(project_root)

    output_path = project_root / output_file

    # Determine which pipelines to generate
    pipelines_to_generate: list[Pipeline] = []

    # Load pipelines configuration
    pipelines_config = load_pipelines_config(project_root, file_reader)

    if not pipelines_config.pipelines:
        app.display_warning(
            "No pipelines found. Create .pipelines/*.yml files to define your pipelines. Considering it is empty"
        )
        dump_yaml({}, output_path, header=build_header(pipelines_to_generate, project_root, ref))
        return

    # Handle --show-pipelines
    if show_pipelines:
        app.display_info("Available pipelines:")
        app.display_info("")
        for p in pipelines_config.pipelines:
            entrypoint = f" → {p.entrypoint}" if p.entrypoint else ""
            app.display_info(f"  • {p.name}{entrypoint}")
        return

    if generate_all:
        pipelines_to_generate = list(pipelines_config.pipelines)
        app.display_info("Generating all pipelines")
    elif pipeline_names:
        for name in pipeline_names:
            pipeline = next((p for p in pipelines_config.pipelines if p.name == name), None)
            if not pipeline:
                available = ", ".join(p.name for p in pipelines_config.pipelines)
                app.abort(f"Pipeline '{name}' not found. Available: {available}")
            pipelines_to_generate.append(pipeline)
        app.display_info(f"Generating pipelines: {', '.join(pipeline_names)}")
    elif filter_by_changes:
        app.display_info(f"Detecting changes compared to {compare_branch}...")
        changed_files = get_changed_files(app, project_root, compare_branch)
        app.display_info(f"Changed files: {changed_files}")

        if changed_files:
            app.display_info(f"Found {len(changed_files)} changed files:")
            for f in changed_files[:10]:
                app.display_info(f"  • {f}")
            if len(changed_files) > 10:
                app.display_info(f"  ... and {len(changed_files) - 10} more")
            app.display_info("")

        pipelines_to_generate = pipelines_config.get_triggered_pipelines(changed_files)
        triggered_names = [p.name for p in pipelines_to_generate]

        if not pipelines_to_generate:
            app.display_warning("No pipelines triggered by changes")
            # Generate an empty pipeline
            dump_yaml({}, output_path, header=build_header(pipelines_to_generate, project_root, ref))
            return

        app.display_info(f"Triggered pipelines: {', '.join(triggered_names)}")
    else:
        # Show help if no action specified
        click.echo(click.get_current_context().get_help())
        return

    # Merge configs from all pipelines to generate
    app.display_info("")
    content = merge_pipeline_configs(
        pipelines_to_generate,
        project_root,
        should_resolve_includes,
        should_resolve_extends,
        app,
        file_reader,
    )

    if not content:
        app.abort("No content to generate - all pipeline configs are empty or missing")

    # Handle triggered pipelines if requested
    if include_all_triggered_pipelines:
        from ci_utils.triggers import get_all_triggered_configurations

        app.display_info("")
        app.display_info("Finding triggered pipelines...")

        all_configs = get_all_triggered_configurations(
            main_config=content,
            file_reader=file_reader,
            project_root=project_root,
            app=app,
        )

        if len(all_configs) > 1:
            app.display_info(f"Found {len(all_configs) - 1} triggered pipeline(s)")
            app.display_info("")

            # Write multi-entry YAML with all configurations
            output_content = {}
            for entry_point, config in all_configs.items():
                output_content[entry_point] = config

            dump_yaml(output_content, output_path, header=build_header(pipelines_to_generate, project_root, ref))

            app.display_info("")
            app.display_success(f"Generated {len(all_configs)} configurations written to {output_path}")
            app.display_info("")
            app.display_info("Entry points:")
            for entry_point in all_configs:
                app.display_info(f"  • {entry_point}")
        else:
            app.display_info("No triggered pipelines found")
            dump_yaml({"main": content}, output_path, header=build_header(pipelines_to_generate, project_root, ref))
            app.display_info("")
            app.display_success(f"Generated pipeline written to {output_path}")
    else:
        dump_yaml(content, output_path, header=build_header(pipelines_to_generate, project_root, ref))
        app.display_info("")
        app.display_success(f"Generated pipeline written to {output_path}")
