# CPU Throttling Scenario
#
# Deploys a pod with CPU limit too low for its workload,
# causing constant throttling and performance degradation.
#
# Expected behavior:
# - Container performs CPU-intensive work requiring ~300m CPU
# - CPU limit set to only 100m (10% of 1 core)
# - Constant throttling occurs as workload tries to exceed limit
# - No crashes or restarts - just degraded performance
# - Request latencies increase significantly
# - Application continues running but slowly
#
# Observable in fine-grained-monitor:
# - cpu_percentage consistently at/near limit (100m = 10%)
# - cgroup.v2.cpu.stat.throttled_usec accumulating rapidly
# - cgroup.v2.cpu.pressure.some.avg10 elevated (15-30%)
# - Application logs show slow operation completion times
# - No memory pressure or OOM kills
# - No process crashes
#
# Root cause: CPU limit misconfigured too low for actual workload needs
#
# Correct remediation: Increase CPU limit to 300m or higher to match usage
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: data-processor-config
  labels:
    app: data-processor
data:
  # Script content will be injected from process.py by cluster.py
  process.py: __INJECT_SCRIPT__
---
apiVersion: v1
kind: Pod
metadata:
  name: data-processor
  labels:
    app: data-processor
spec:
  restartPolicy: Always
  volumes:
    - name: script
      configMap:
        name: data-processor-config
        defaultMode: 0755
  containers:
    - name: processor
      image: python:3.11-slim
      command: ["python3", "/scripts/process.py"]
      volumeMounts:
        - name: script
          mountPath: /scripts
      resources:
        requests:
          memory: "64Mi"
          cpu: "50m"
        limits:
          memory: "128Mi"
          cpu: "100m"  # Too low - workload needs ~300m
