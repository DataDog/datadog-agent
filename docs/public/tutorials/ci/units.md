# Managing CI units

-----

This tutorial will walk through how to configure a new [CI unit](../../reference/ci/structure.md#units) scheduled by GitLab CI for an example project named `foo` that lives in the `/projects/foo` directory.

## Purpose

Nearly everything related to developing and releasing the [Datadog Agent](https://docs.datadoghq.com/agent/) lives in the `datadog-agent` monorepo. Although the projects that make up the agent are coupled to varying degrees, there are many situations in which it is useful to run independent sets of jobs a.k.a. pipelines. Some examples include:

- Building documentation only when docs change, and not the full test suite
- Running tests outside of what the build graph deems necessary
- Allowing teams to own and configure CI for their specific needs

CI units enable independent pipelines that run in parallel, resulting in faster feedback for developers.

## Configuration

Each CI unit is defined by a single TOML file in the `/ci/units` directory and owned by the team responsible for the unit. The stem of each unit's filename is treated as its ID, which is used in the following ways:

- Generating the name of each unit's trigger job(s)
- Manually triggering the unit
- Setting the `UNIT_ID` environment variable in each unit's jobs

Unit [configuration](../../reference/ci/units/config.md) requires at least the `name`, `description`, `trigger` and `provider` fields.

For our example project `foo`, we'll start by creating a new file named `foo.toml` (ID: `foo`) in the `/ci/units` directory with the following contents.

/// tab | :octicons-file-code-16: /ci/units/foo.toml
```toml
name = "Foo CI"
description = '''
This is for advanced testing of the Foo project.

Any changes to code in the `foo` project should trigger this unit.
'''
```
///

The `name` field is available to the unit at runtime as the `UNIT_DISPLAY_NAME` environment variable. The `description` field is used for documentation purposes such as the [page](../../reference/ci/units/defined.md) that lists all units.

//// note
Since the default provider of CI unit scheduling is GitLab CI, we can omit the `provider.name` field.

/// tab | :octicons-file-code-16: /ci/units/foo.toml
```toml
[provider]
name = "gitlab"
```
///
////

## Pipelines

Each CI unit is associated with a pipeline that is defined in the `provider.pipeline` table. The `source` field determines whether the pipeline is statically defined in a file or dynamically generated by a command.

### Static pipelines

For static pipelines, the `path` field specifies the pipeline file location relative to the repository root.

/// tab | :octicons-file-code-16: /ci/units/foo.toml
```toml
[provider.pipeline]
source = "file"
path = "projects/foo/pipeline.yml"
```
///

The contents of a static pipeline file might look like the following.

/// tab | :octicons-file-code-16: /projects/foo/pipeline.yml
```yaml
stages:
- test

test:
  stage: test
  script:
  - echo "Hello, world!"
```
///

### Dynamic pipelines

Dynamic pipelines are configured by a command that is executed to generate the pipeline definition. The `command` field specifies the shell command to execute, which must output the pipeline definition in YAML format to `stdout`.

The most basic example of a dynamic pipeline is to output the contents of a static pipeline file.

/// tab | :octicons-file-code-16: /ci/units/foo.toml
```toml
[provider.pipeline]
source = "command"
command = "cat projects/foo/pipeline.yml"
```
///

A dynamic pipeline will usually use a script to generate the pipeline definition based on changes to the repository and/or configuration.

/// tab | :octicons-file-code-16: /ci/units/foo.toml
```toml
[provider.pipeline]
source = "command"
command = "python projects/foo/generate_pipeline.py"
```
///

#### Skipping the unit

If the generator determines that the unit should not run, it cannot skip the unit's pipeline from running as GitLab CI does not support modifying the job graph of a pipeline after it has been created.

The recommended way to skip the unit is to generate a pipeline with a single no-op job, since a triggered pipeline without jobs will fail. There is a template for this that can be used as follows.

```yaml
include:
- local: ci/templates/jobs/units/skip.yml
```

## Trigger

CI units configure the `trigger` table to define the conditions under which it will run. Most commonly, this is used to trigger the unit only when certain paths are changed.

### Changes

The `trigger.patterns` field is an array of glob patterns relative to the repository root that will trigger the unit if any changed paths match. If a branch is associated with an open pull request, the comparison is made against the target branch of the pull request. Otherwise, it is made against the `main` branch.

The following configuration will trigger the unit when any changes are made to the `/projects/foo/src` directory.

/// tab | :octicons-file-code-16: /ci/units/foo.toml
```toml
[trigger]
patterns = [
  "projects/foo/src/**/*",
]
```
///

By default, the unit's config file is added to the trigger's patterns. This can be disabled with the `trigger.watch-config` field.

/// tab | :octicons-file-code-16: /ci/units/foo.toml
```toml
[trigger]
watch-config = false
```
///

### Custom rules

The `provider.rules` field is an array of tables that [support](../../reference/ci/units/config.md#constrainedworkflowrule) a subset of the [workflow rule](https://docs.gitlab.com/ci/yaml/#workflowrules) settings. These rules are considered before the [change detection trigger](#changes) is evaluated.

The following configuration will always trigger the unit on release branches and never trigger it if the environment variable `FOO` is set to `1`.

/// tab | :octicons-file-code-16: /ci/units/foo.toml
```toml
[[provider.rules]]
if = '$CI_COMMIT_BRANCH =~ /^[0-9]+\.[0-9]+\.x$/'

[[provider.rules]]
if = '$FOO != "1"'
when = "never"
```
///

//// details | Why not support `variables`?
CI unit triggers are only meant to configure the conditions under which a unit will run. Any configuration should be done in the unit's pipeline definition.

For example, if you need to set variables for a unit's pipeline when it runs on the `main` branch you can do so using its workflow rule's `variables` [field](https://docs.gitlab.com/ci/yaml/#workflowrulesvariables).

/// tab | :octicons-file-code-16: /projects/foo/pipeline.yml
```yaml
workflow:
  rules:
  - if: $CI_COMMIT_BRANCH == "main"
    variables:
      VAR1: value1
      VAR2: value2
  - when: always
```
///
////

//// details | Why not support `changes`?
The [inclusion patterns](#changes) defined by the `trigger.patterns` field are meant to be the source of truth for change detection to prevent a common source of misconfiguration.

Let's say that you have files that should not trigger the unit when only they are changed. You may be inclined to exclude them using a custom rule, like the following example that attempts to run the unit only when the `/projects/foo` directory is changed except for its `README.md` file.

/// tab | :octicons-file-code-16: /ci/units/foo.toml
```toml
[trigger]
patterns = [
  "projects/foo/**/*",
]

[[provider.rules]]
when = "never"
changes = [
  "projects/foo/README.md",
]
```
///

Since custom rules are evaluated before change detection, if only the `README.md` file was changed then the unit will not run as expected. However, the unit will also not run if a commit changes both the `README.md` file and some other file in the `/projects/foo` directory that should trigger the unit.

The recommended way to exclude such files is to make the `trigger.patterns` field more granular, restructuring the code base as needed. As a last resort, you can [skip](#skipping-the-unit) the unit during generation of dynamic pipelines.
////

## Registration

If we defined a unit for the `foo` project as a [static](#static-pipelines) pipeline that is triggered by any changes to its `src` directory, its full configuration would look like the following.

/// tab | :octicons-file-code-16: /ci/units/foo.toml
```toml
name = "Foo CI"
description = '''
This is for advanced testing of the Foo project.

Any changes to code in the `foo` project should trigger this unit.
'''

[provider.pipeline]
source = "file"
path = "projects/foo/pipeline.yml"

[trigger]
patterns = [
  "projects/foo/src/**/*",
]
```
///

Unit configuration alone does not influence CI execution. Each unit's configuration is used to generate assets within the `/.ci/units` directory that are used to register the unit with the CI provider. These files must be regenerated after any changes to the unit's configuration by running the following command.

```
dda check ci units --fix
```

## Advanced

### Parent pipeline

Each unit's pipeline has access to the `PARENT_PIPELINE_ID` environment variable which contains the ID of the parent pipeline. This can be useful aggregating telemetry collection, embedding in notifications, or even as a means to fetch artifacts from the parent pipeline.

### Manual trigger

Any unit can be triggered on the pipeline creation page by using the `TRIGGER_UNITS` variable. The variable is a comma-separated list of unit IDs. If any element is the string `all` then all units will be triggered.

You can disable manual triggering capabilities with the `trigger.allow-manual` field.

/// tab | :octicons-file-code-16: /ci/units/foo.toml
```toml
[trigger]
allow-manual = false
```
///

### Pipeline nesting

Each unit runs as a child pipeline of the root pipeline defined by the `/.gitlab-ci.yml` file. Since pipelines can only be [nested twice](https://docs.gitlab.com/ci/pipelines/downstream_pipelines/#nested-child-pipelines), a unit's pipeline can only trigger child pipelines that cannot trigger their own.

Let's say you have the following structure where each level represents a pipeline and its stages.

```
build_binaries
├── agent:linux-aarch64
├── agent:windows-x86_64
├── agent-otel:linux-x86_64
├── dogstatsd:linux-aarch64
└── ...
build_distributions
├── agent:deb:aarch64
├── agent:msi:x86_64
├── agent-otel:deb:x86_64
├── dogstatsd:deb:aarch64
└── ...
build_containers
├── agent:linux-aarch64
├── agent:windows-x86_64
├── agent-otel:linux-aarch64
├── dogstatsd:linux-aarch64
└── ...
test_unit
├── agent:linux-aarch64
├── agent:windows-x86_64
├── agent-otel:linux-aarch64
├── dogstatsd:linux-aarch64
└── ...
test_e2e
├── agent:linux-aarch64
├── agent:windows-x86_64
├── agent-otel:linux-aarch64
├── dogstatsd:linux-aarch64
└── ...
```

If you wanted to only run required jobs based on changes, you could trigger dynamic child pipelines for each stage. For example, if all changed files only affect the Windows distribution then your computed jobs might look like the following.

```
build_binaries
└── agent:windows-x86_64
build_distributions
└── agent:msi:x86_64
build_containers
└── agent:windows-x86_64
test_unit
└── agent:windows-x86_64
test_e2e
└── agent:windows-x86_64
```

Now let's say you wanted to group the stages based on their purpose, like having high-level `build` and `test` stages. It would be impossible to do this with static pipelines because it already exhausts its nesting limit with the conditional job design.

You can "gain back" an extra level of nesting by defining the pipeline as [dynamic](#dynamic-pipelines). The pipeline would define the stages conditionally based on changes, with each stage's nested jobs being dynamically computed as before. The previous example might look something like the following.

```
build
├── binaries
│   └── agent:windows-x86_64
├── distributions
│   └── agent:msi:x86_64
└── containers
    └── agent:windows-x86_64
test
├── unit
│   └── agent:windows-x86_64
└── e2e
    └── agent:windows-x86_64
```

This is not only useful for UI improvements but it also enables completely conditional stages. For example, if only a small part of the final packaging process is modified then almost all jobs and therefore stages can be skipped.
