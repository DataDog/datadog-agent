// Unless explicitly stated otherwise all files in this repository are licensed
// under the Apache License Version 2.0.
// This product includes software developed at Datadog (https://www.datadoghq.com/).
// Copyright 2016-present Datadog, Inc.

package aggregator

import (
	"fmt"
	"sync"
	"time"

	"github.com/DataDog/datadog-agent/pkg/aggregator/tags"
	"github.com/DataDog/datadog-agent/pkg/collector/check"
	"github.com/DataDog/datadog-agent/pkg/config"
	"github.com/DataDog/datadog-agent/pkg/config/resolver"
	"github.com/DataDog/datadog-agent/pkg/containerlifecycle"
	"github.com/DataDog/datadog-agent/pkg/epforwarder"
	"github.com/DataDog/datadog-agent/pkg/forwarder"
	"github.com/DataDog/datadog-agent/pkg/metrics"

	agentruntime "github.com/DataDog/datadog-agent/pkg/runtime"
	"github.com/DataDog/datadog-agent/pkg/serializer"
	"github.com/DataDog/datadog-agent/pkg/util/log"
)

// DemultiplexerInstance is a shared global demultiplexer instance.
// Initialized by InitAndStartAgentDemultiplexer or InitAndStartServerlessDemultiplexer,
// could be nil otherwise.
//
// The plan is to deprecated this global instance at some point.
var demultiplexerInstance Demultiplexer

var demultiplexerInstanceMu sync.Mutex

// Demultiplexer is composed of multiple samplers (check and time/dogstatsd)
// a shared forwarder, the event platform forwarder, orchestrator data buffers
// and other data that need to be sent to the forwarders.
// DemultiplexerOptions let you configure which forwarders have to be started.
type Demultiplexer interface {
	// General

	Run()
	Stop(flush bool)

	// Aggregation API

	// AddTimeSample sends a MetricSample to the time sampler.
	// In sharded implementation, the metric is sent to the first time sampler.
	AddTimeSample(sample metrics.MetricSample)
	// AddTimeSampleBatch sends a batch of MetricSample to the given time
	// sampler shard.
	// Implementation not supporting sharding may ignore the `shard` parameter.
	AddTimeSampleBatch(shard TimeSamplerID, samples metrics.MetricSampleBatch)
	// AddCheckSample adds check sample sent by a check from one of the collectors into a check sampler pipeline.
	AddCheckSample(sample metrics.MetricSample)
	// ForceFlushToSerializer flushes all the aggregated data from the different samplers to
	// the serialization/forwarding parts.
	ForceFlushToSerializer(start time.Time, waitForSerializer bool)
	// GetMetricSamplePool returns a shared resource used in the whole DogStatsD
	// pipeline to re-use metric samples slices: the server is getting a slice
	// and filling it with samples, the rest of the pipeline process them the
	// end of line (the time sampler) is putting back the slice in the pool.
	// Main idea is to reduce the garbage generated by slices allocation.
	GetMetricSamplePool() *metrics.MetricSamplePool

	// Aggregator returns an aggregator that anyone can use. This method exists
	// to keep compatibility with existing code while introducing the Demultiplexer,
	// however, the plan is to remove it anytime soon.
	//
	// Deprecated.
	Aggregator() *BufferedAggregator
	// Serializer returns a serializer that anyone can use. This method exists
	// to keep compatibility with existing code while introducing the Demultiplexer,
	// however, the plan is to remove it anytime soon.
	//
	// Deprecated.
	Serializer() serializer.MetricSerializer

	// Senders API, mainly used by collectors/checks

	GetSender(id check.ID) (Sender, error)
	SetSender(sender Sender, id check.ID) error
	DestroySender(id check.ID)
	GetDefaultSender() (Sender, error)
	ChangeAllSendersDefaultHostname(hostname string)
	cleanSenders()
}

// AgentDemultiplexer is the demultiplexer implementation for the main Agent
type AgentDemultiplexer struct {
	m sync.Mutex

	// stopChan completely stops the flushLoop of the Demultiplexer when receiving
	// a message, not doing anything else.
	stopChan chan struct{}
	// flushChan receives a trigger to run an internal flush of all
	// samplers (TimeSampler, BufferedAggregator (CheckSampler, Events, ServiceChecks))
	// to the shared serializer.
	flushChan chan trigger

	// options are the options with which the demultiplexer has been created
	options    DemultiplexerOptions
	aggregator *BufferedAggregator
	dataOutputs
	*senders

	// sharded statsd time samplers
	statsd
}

// DemultiplexerOptions are the options used to initialize a Demultiplexer.
type DemultiplexerOptions struct {
	SharedForwarderOptions         *forwarder.Options
	UseNoopEventPlatformForwarder  bool
	UseEventPlatformForwarder      bool
	UseOrchestratorForwarder       bool
	UseContainerLifecycleForwarder bool
	FlushInterval                  time.Duration

	DontStartForwarders bool // unit tests don't need the forwarders to be instanciated
}

type statsd struct {
	// how many sharded statsdSamplers exists.
	// len(workers) would return the same result but having it stored
	// it will provide more explicit visiblility / no extra function call for
	// every metric to distribute.
	pipelinesCount int
	workers        []*timeSamplerWorker
	// shared metric sample pool between the dogstatsd server & the time sampler
	metricSamplePool *metrics.MetricSamplePool
}

type forwarders struct {
	shared             *forwarder.DefaultForwarder
	orchestrator       *forwarder.DefaultForwarder
	eventPlatform      epforwarder.EventPlatformForwarder
	containerLifecycle *forwarder.DefaultForwarder
}

type dataOutputs struct {
	forwarders       forwarders
	sharedSerializer serializer.MetricSerializer
}

// trigger be used to trigger something in the TimeSampler or the BufferedAggregator.
// If `blockChan` is not nil, a message is expected on this chan when the action is done.
// See `flushTrigger` to see the usage in a flush trigger.
type trigger struct {
	time time.Time

	// if not nil, the flusher will send a message in this chan when the flush is complete.
	blockChan chan struct{}

	// used by the BufferedAggregator to know if serialization of events,
	// service checks and such have to be waited for before returning
	// from Flush()
	waitForSerializer bool
}

// flushTrigger is a trigger used to flush data, results is expected to be written
// in flushedSeries (or seriesSink depending on the implementation) and flushedSketches.
type flushTrigger struct {
	trigger

	flushedSeries   *[]metrics.Series
	flushedSketches *[]metrics.SketchSeriesList
	seriesSink      metrics.SerieSink
}

// DefaultDemultiplexerOptions returns the default options to initialize a Demultiplexer.
func DefaultDemultiplexerOptions(options *forwarder.Options) DemultiplexerOptions {
	if options == nil {
		options = forwarder.NewOptions(nil)
	}

	return DemultiplexerOptions{
		SharedForwarderOptions:         options,
		FlushInterval:                  DefaultFlushInterval,
		UseEventPlatformForwarder:      true,
		UseOrchestratorForwarder:       true,
		UseContainerLifecycleForwarder: false,
	}
}

// InitAndStartAgentDemultiplexer creates a new Demultiplexer and runs what's necessary
// in goroutines. As of today, only the embedded BufferedAggregator needs a separate goroutine.
// In the future, goroutines will be started for the event platform forwarder and/or orchestrator forwarder.
func InitAndStartAgentDemultiplexer(options DemultiplexerOptions, hostname string) *AgentDemultiplexer {
	demultiplexerInstanceMu.Lock()
	defer demultiplexerInstanceMu.Unlock()

	demux := initAgentDemultiplexer(options, hostname)

	if demultiplexerInstance != nil {
		log.Warn("A DemultiplexerInstance is already existing but InitAndStartAgentDemultiplexer has been called again. Current instance will be overridden")
	}
	demultiplexerInstance = demux

	go demux.Run()
	return demux
}

func initAgentDemultiplexer(options DemultiplexerOptions, hostname string) *AgentDemultiplexer {

	// prepare the multiple forwarders
	// -------------------------------

	log.Debugf("Creating forwarders")
	// orchestrator forwarder
	var orchestratorForwarder *forwarder.DefaultForwarder
	if options.UseOrchestratorForwarder {
		orchestratorForwarder = buildOrchestratorForwarder()
	}

	// event platform forwarder
	var eventPlatformForwarder epforwarder.EventPlatformForwarder
	if options.UseNoopEventPlatformForwarder {
		eventPlatformForwarder = epforwarder.NewNoopEventPlatformForwarder()
	} else if options.UseEventPlatformForwarder {
		eventPlatformForwarder = epforwarder.NewEventPlatformForwarder()
	}

	// setup the container lifecycle events forwarder
	var containerLifecycleForwarder *forwarder.DefaultForwarder
	if options.UseContainerLifecycleForwarder {
		containerLifecycleForwarder = containerlifecycle.NewForwarder()
	}

	sharedForwarder := forwarder.NewDefaultForwarder(options.SharedForwarderOptions)

	// prepare the serializer
	// ----------------------

	sharedSerializer := serializer.NewSerializer(sharedForwarder, orchestratorForwarder, containerLifecycleForwarder)

	// prepare the embedded aggregator
	// --

	agg := InitAggregatorWithFlushInterval(sharedSerializer, eventPlatformForwarder, hostname, options.FlushInterval)

	// statsd samplers
	// ---------------

	bufferSize := config.Datadog.GetInt("aggregator_buffer_size")
	metricSamplePool := metrics.NewMetricSamplePool(MetricSamplePoolBatchSize)

	_, statsdPipelinesCount := GetDogStatsDWorkerAndPipelineCount()
	log.Debug("the Demultiplexer will use", statsdPipelinesCount, "pipelines")

	statsdWorkers := make([]*timeSamplerWorker, statsdPipelinesCount)

	for i := 0; i < statsdPipelinesCount; i++ {
		// the sampler
		tagsStore := tags.NewStore(config.Datadog.GetBool("aggregator_use_tags_store"), fmt.Sprintf("timesampler #%d", i))
		statsdSampler := NewTimeSampler(TimeSamplerID(i), bucketSize, tagsStore)

		// its worker (process loop + flush/serialization mechanism)

		statsdWorkers[i] = newTimeSamplerWorker(statsdSampler, options.FlushInterval,
			bufferSize, metricSamplePool, agg.flushAndSerializeInParallel, tagsStore)
	}

	// --

	demux := &AgentDemultiplexer{
		options:   options,
		stopChan:  make(chan struct{}),
		flushChan: make(chan trigger),

		// Input
		aggregator: agg,

		// Output
		dataOutputs: dataOutputs{

			forwarders: forwarders{
				shared:             sharedForwarder,
				orchestrator:       orchestratorForwarder,
				eventPlatform:      eventPlatformForwarder,
				containerLifecycle: containerLifecycleForwarder,
			},

			sharedSerializer: sharedSerializer,
		},

		senders: newSenders(agg),

		// statsd time samplers
		statsd: statsd{
			pipelinesCount:   statsdPipelinesCount,
			workers:          statsdWorkers,
			metricSamplePool: metricSamplePool,
		},
	}

	return demux
}

// AddAgentStartupTelemetry adds a startup event and count (in a time sampler)
// to be sent on the next flush.
func (d *AgentDemultiplexer) AddAgentStartupTelemetry(agentVersion string) {
	if agentVersion != "" {
		d.AddTimeSample(metrics.MetricSample{
			Name:       fmt.Sprintf("datadog.%s.started", d.aggregator.agentName),
			Value:      1,
			Tags:       d.aggregator.tags(true),
			Host:       d.aggregator.hostname,
			Mtype:      metrics.CountType,
			SampleRate: 1,
			Timestamp:  0,
		})

		if d.aggregator.hostname != "" {
			// Send startup event only when we have a valid hostname
			d.aggregator.eventIn <- metrics.Event{
				Text:           fmt.Sprintf("Version %s", agentVersion),
				SourceTypeName: "System",
				Host:           d.aggregator.hostname,
				EventType:      "Agent Startup",
			}
		}
	}
}

// Run runs all demultiplexer parts
func (d *AgentDemultiplexer) Run() {
	if !d.options.DontStartForwarders {
		log.Debugf("Starting forwarders")

		// orchestrator forwarder
		if d.forwarders.orchestrator != nil {
			d.forwarders.orchestrator.Start() //nolint:errcheck
		} else {
			log.Debug("not starting the orchestrator forwarder")
		}

		// event platform forwarder
		if d.forwarders.eventPlatform != nil {
			d.forwarders.eventPlatform.Start()
		} else {
			log.Debug("not starting the event platform forwarder")
		}

		// container lifecycle forwarder
		if d.forwarders.containerLifecycle != nil {
			if err := d.forwarders.containerLifecycle.Start(); err != nil {
				log.Errorf("error starting container lifecycle forwarder: %w", err)
			}
		} else {
			log.Debug("not starting the container lifecycle forwarder")
		}

		// shared forwarder
		if d.forwarders.shared != nil {
			d.forwarders.shared.Start() //nolint:errcheck
		} else {
			log.Debug("not starting the shared forwarder")
		}
		log.Debug("Forwarders started")
	}

	if d.options.UseContainerLifecycleForwarder {
		d.aggregator.contLcycleDequeueOnce.Do(func() { go d.aggregator.dequeueContainerLifecycleEvents() })
	}

	for _, w := range d.statsd.workers {
		go w.run()
	}

	go d.aggregator.run()
	d.flushLoop() // this is the blocking call
}

func (d *AgentDemultiplexer) flushLoop() {
	var flushTicker <-chan time.Time
	if d.options.FlushInterval > 0 {
		flushTicker = time.NewTicker(d.options.FlushInterval).C
	} else {
		log.Debugf("flushInterval set to 0: will never flush automatically")
	}

	for {
		select {
		// stop sequence
		case <-d.stopChan:
			return
		// manual flush sequence
		case trigger := <-d.flushChan:
			d.flushToSerializer(trigger.time, trigger.waitForSerializer)
			if trigger.blockChan != nil {
				trigger.blockChan <- struct{}{}
			}
		// automatic flush sequence
		case t := <-flushTicker:
			d.flushToSerializer(t, false)
		}
	}
}

// Stop stops the demultiplexer.
// Resources are released, the instance should not be used after a call to `Stop()`.
func (d *AgentDemultiplexer) Stop(flush bool) {
	timeout := config.Datadog.GetDuration("aggregator_stop_timeout") * time.Second

	// do a manual complete flush then stop
	// stop all automatic flush & the mainloop,
	if flush {
		trigger := trigger{
			time:              time.Now(),
			blockChan:         make(chan struct{}),
			waitForSerializer: flush,
		}

		d.flushChan <- trigger
		select {
		case <-trigger.blockChan:
		case <-time.After(timeout):
			log.Errorf("flushing data on Stop() timed out")
		}
	}

	// stops the flushloop and makes sure no automatic flushes will happen anymore
	d.stopChan <- struct{}{}

	d.m.Lock()
	defer d.m.Unlock()

	// aggregated data
	for _, worker := range d.statsd.workers {
		worker.stop()
	}
	if d.aggregator != nil {
		d.aggregator.Stop()
	}
	d.aggregator = nil

	// forwarders

	if !d.options.DontStartForwarders {
		if d.dataOutputs.forwarders.orchestrator != nil {
			d.dataOutputs.forwarders.orchestrator.Stop()
			d.dataOutputs.forwarders.orchestrator = nil
		}
		if d.dataOutputs.forwarders.eventPlatform != nil {
			d.dataOutputs.forwarders.eventPlatform.Stop()
			d.dataOutputs.forwarders.eventPlatform = nil
		}
		if d.dataOutputs.forwarders.containerLifecycle != nil {
			d.dataOutputs.forwarders.containerLifecycle.Stop()
			d.dataOutputs.forwarders.containerLifecycle = nil
		}
		if d.dataOutputs.forwarders.shared != nil {
			d.dataOutputs.forwarders.shared.Stop()
			d.dataOutputs.forwarders.shared = nil
		}
	}

	// misc

	d.dataOutputs.sharedSerializer = nil
	d.senders = nil
	demultiplexerInstance = nil
}

// ForceFlushToSerializer triggers the execution of a flush from all data of samplers
// and the BufferedAggregator to the serializer.
// Safe to call from multiple threads.
func (d *AgentDemultiplexer) ForceFlushToSerializer(start time.Time, waitForSerializer bool) {
	trigger := trigger{
		time:              start,
		waitForSerializer: waitForSerializer,
		blockChan:         make(chan struct{}),
	}
	d.flushChan <- trigger
	<-trigger.blockChan
}

// flushToSerializer flushes all data from the aggregator and time samplers
// to the serializer.
//
// Best practice is that this method is *only* called by the flushLoop routine.
// It technically works if called from outside of this routine, but beware of
// deadlocks with the parallel stream series implementation.
//
// This implementation is not flushing the TimeSampler and the BufferedAggregator
// concurrently because the IterableSeries is not thread safe / supporting concurrent usage.
// If one day a better (faster?) solution is needed, we could either consider:
// - to have an implementation of SendIterableSeries listening on multiple sinks in parallel, or,
// - to have a thread-safe implementation of the underlying `util.BufferedChan`.
func (d *AgentDemultiplexer) flushToSerializer(start time.Time, waitForSerializer bool) {
	d.m.Lock()
	defer d.m.Unlock()

	if d.aggregator == nil {
		// NOTE(remy): we could consider flushing only the time samplers
		return
	}

	logPayloads := config.Datadog.GetBool("log_payloads")
	flushedSeries := make([]metrics.Series, 0)
	flushedSketches := make([]metrics.SketchSeriesList, 0)

	// only used when we're using flush/serialize in parallel feature
	var seriesSink *metrics.IterableSeries
	var done chan struct{}

	if d.aggregator.flushAndSerializeInParallel.enabled {
		seriesSink = metrics.NewIterableSeries(func(se *metrics.Serie) {
			if logPayloads {
				log.Debugf("Flushing serie: %s", se)
			}
			tagsetTlm.updateHugeSerieTelemetry(se)
		}, d.aggregator.flushAndSerializeInParallel.bufferSize, d.aggregator.flushAndSerializeInParallel.channelSize)
		done = make(chan struct{})
		go d.sendIterableSeries(start, seriesSink, done)
	}

	// flush DogStatsD pipelines (statsd/time samplers)
	// ------------------------------------------------

	for _, worker := range d.statsd.workers {
		// order the flush to the time sampler, and wait, in a different routine
		t := flushTrigger{
			trigger: trigger{
				time:      start,
				blockChan: make(chan struct{}),
			},
			flushedSeries:   &flushedSeries,
			flushedSketches: &flushedSketches,
			seriesSink:      seriesSink,
		}

		worker.flushChan <- t
		<-t.trigger.blockChan
	}

	// flush the aggregator (check samplers)
	// -------------------------------------

	if d.aggregator != nil {
		t := flushTrigger{
			trigger: trigger{
				time:              start,
				blockChan:         make(chan struct{}),
				waitForSerializer: waitForSerializer,
			},
			flushedSeries:   &flushedSeries,
			flushedSketches: &flushedSketches,
			seriesSink:      seriesSink,
		}

		d.aggregator.flushChan <- t
		<-t.trigger.blockChan
	}

	if d.aggregator.flushAndSerializeInParallel.enabled {
		seriesSink.SenderStopped()
		<-done
	}

	// collect the series and sketches that the multiple samplers may have reported
	// ------------------------------------------------------

	var series metrics.Series
	var sketches metrics.SketchSeriesList

	for _, s := range flushedSeries {
		series = append(series, s...)
	}
	for _, s := range flushedSketches {
		sketches = append(sketches, s...)
	}

	// debug flag to log payloads
	// --------------------------

	if logPayloads {
		log.Debug("Flushing the following Series:")
		for _, s := range series {
			log.Debugf("%s", s)
		}

		log.Debug("Flushing the following Sketches:")
		for _, s := range sketches {
			log.Debugf("%s", s)
		}
	}

	// send these to the serializer
	// ----------------------------

	addFlushCount("Series", int64(len(series)))
	if len(series) > 0 {
		log.Debugf("Flushing %d series to the serializer", len(series))
		err := d.sharedSerializer.SendSeries(series)
		updateSerieTelemetry(start, uint64(len(series)), err)
		tagsetTlm.updateHugeSeriesTelemetry(&series)
	}

	addFlushCount("Sketches", int64(len(sketches)))
	if len(sketches) > 0 {
		log.Debugf("Flushing %d sketches to the serializer", len(sketches))
		err := d.sharedSerializer.SendSketch(sketches)
		updateSketchTelemetry(start, uint64(len(sketches)), err)
		tagsetTlm.updateHugeSketchesTelemetry(&sketches)
	}

	addFlushTime("MainFlushTime", int64(time.Since(start)))
	aggregatorNumberOfFlush.Add(1)
}

// sendIterableSeries is continuously sending series to the serializer, until another routine calls SenderStopped on the
// series sink.
// Mainly meant to be executed in its own routine, sendIterableSeries is closing the `done` channel once it has returned
// from SendIterableSeries (because the SenderStopped methods has been called on the sink).
func (d *AgentDemultiplexer) sendIterableSeries(start time.Time, series *metrics.IterableSeries, done chan<- struct{}) {
	log.Debug("Demultiplexer: sendIterableSeries: start sending iterable series to the serializer")
	err := d.sharedSerializer.SendIterableSeries(series)
	// if err == nil, SenderStopped was called and it is safe to read the number of series.
	count := series.SeriesCount()
	addFlushCount("Series", int64(count))
	updateSerieTelemetry(start, count, err)
	close(done)
	log.Debug("Demultiplexer: sendIterableSeries: stop routine")
}

// AddTimeSampleBatch adds a batch of MetricSample into the given time sampler shard.
// If you have to submit a single metric sample see `AddTimeSample`.
func (d *AgentDemultiplexer) AddTimeSampleBatch(shard TimeSamplerID, samples metrics.MetricSampleBatch) {
	// distribute the samples on the different statsd samplers using a channel
	// (in the time sampler implementation) for latency reasons:
	// its buffering + the fact that it is another goroutine processing the samples,
	// it should get back to the caller as fast as possible once the samples are
	// in the channel.
	d.statsd.workers[shard].samplesChan <- samples
}

// AddTimeSample adds a MetricSample in the first time sampler.
func (d *AgentDemultiplexer) AddTimeSample(sample metrics.MetricSample) {
	batch := d.GetMetricSamplePool().GetBatch()
	batch[0] = sample
	d.statsd.workers[0].samplesChan <- batch[:1]
}

// AddCheckSample adds check sample sent by a check from one of the collectors into a check sampler pipeline.
func (d *AgentDemultiplexer) AddCheckSample(sample metrics.MetricSample) {
	panic("not implemented yet.")
}

// GetDogStatsDPipelinesCount returns how many sampling pipeline are running for
// the DogStatsD samples.
func (d *AgentDemultiplexer) GetDogStatsDPipelinesCount() int {
	return d.statsd.pipelinesCount
}

// Serializer returns a serializer that anyone can use. This method exists
// to keep compatibility with existing code while introducing the Demultiplexer,
// however, the plan is to remove it anytime soon.
//
// Deprecated.
func (d *AgentDemultiplexer) Serializer() serializer.MetricSerializer {
	return d.dataOutputs.sharedSerializer
}

// Aggregator returns an aggregator that anyone can use. This method exists
// to keep compatibility with existing code while introducing the Demultiplexer,
// however, the plan is to remove it anytime soon.
//
// Deprecated.
func (d *AgentDemultiplexer) Aggregator() *BufferedAggregator {
	return d.aggregator
}

// GetMetricSamplePool returns a shared resource used in the whole DogStatsD
// pipeline to re-use metric samples slices: the server is getting a slice
// and filling it with samples, the rest of the pipeline process them the
// end of line (the time sampler) is putting back the slice in the pool.
// Main idea is to reduce the garbage generated by slices allocation.
func (d *AgentDemultiplexer) GetMetricSamplePool() *metrics.MetricSamplePool {
	return d.statsd.metricSamplePool
}

// GetDogStatsDWorkerAndPipelineCount returns how many routines should be spawned
// for the DogStatsD workers and how many DogStatsD pipeline should be running.
func GetDogStatsDWorkerAndPipelineCount() (int, int) {
	return getDogStatsDWorkerAndPipelineCount(agentruntime.NumVCPU())
}

func getDogStatsDWorkerAndPipelineCount(vCPUs int) (int, int) {
	var dsdWorkerCount int
	var pipelineCount int
	autoAdjust := config.Datadog.GetBool("dogstatsd_pipeline_autoadjust")

	// no auto-adjust of the pipeline count:
	// we use the pipeline count configuration
	// to determine how many workers should be running
	// ------------------------------------

	if !autoAdjust {
		pipelineCount = config.Datadog.GetInt("dogstatsd_pipeline_count")
		if pipelineCount <= 0 { // guard against configuration mistakes
			pipelineCount = 1
		}

		// - a core for the listener goroutine
		// - one per aggregation pipeline (time sampler)
		// - the rest for workers
		// But we want at minimum 2 workers.
		dsdWorkerCount = vCPUs - 1 - pipelineCount

		if dsdWorkerCount < 2 {
			dsdWorkerCount = 2
		}

		return dsdWorkerCount, pipelineCount
	}

	// we will auto-adjust the pipeline and workers count
	//
	// Benchmarks have revealed that 3 very busy workers can be processed
	// by 2 pipelines DogStatsD and have a good ratio execution / scheduling / waiting.
	// To keep this simple for now, we will try running 1 less pipeline than workers.
	// (e.g. for 4 workers, 3 pipelines)
	// Use Go routines analysis with pprof to look at execution time if you want
	// adapt this heuristic.
	//
	// Basically the formula is:
	//  - half the amount of vCPUS for the amount of workers routines
	//  - half the amount of vCPUS - 1 for the amount of pipeline routines
	//  - this last routine for the listener routine

	dsdWorkerCount = vCPUs / 2
	if dsdWorkerCount < 2 { // minimum 2 workers
		dsdWorkerCount = 2
	}

	pipelineCount = dsdWorkerCount - 1
	if pipelineCount <= 0 { // minimum 1 pipeline
		pipelineCount = 1
	}

	if config.Datadog.GetInt("dogstatsd_pipeline_count") > 1 {
		log.Warn("DogStatsD pipeline count value ignored since 'dogstatsd_pipeline_autoadjust' is enabled.")
	}

	return dsdWorkerCount, pipelineCount
}

// ------------------------------

// ServerlessDemultiplexer is a simple demultiplexer used by the serverless flavor of the Agent
type ServerlessDemultiplexer struct {
	// shared metric sample pool between the dogstatsd server & the time sampler
	metricSamplePool *metrics.MetricSamplePool

	serializer    *serializer.Serializer
	forwarder     *forwarder.SyncForwarder
	statsdSampler *TimeSampler
	statsdWorker  *timeSamplerWorker

	flushLock *sync.Mutex

	aggregator *BufferedAggregator
	*senders
}

// InitAndStartServerlessDemultiplexer creates and starts new Demultiplexer for the serverless agent.
func InitAndStartServerlessDemultiplexer(domainResolvers map[string]resolver.DomainResolver, hostname string, forwarderTimeout time.Duration) *ServerlessDemultiplexer {
	bufferSize := config.Datadog.GetInt("aggregator_buffer_size")
	forwarder := forwarder.NewSyncForwarder(domainResolvers, forwarderTimeout)
	serializer := serializer.NewSerializer(forwarder, nil, nil)
	aggregator := InitAggregator(serializer, nil, hostname)
	metricSamplePool := metrics.NewMetricSamplePool(MetricSamplePoolBatchSize)
	tagsStore := tags.NewStore(config.Datadog.GetBool("aggregator_use_tags_store"), "timesampler")

	statsdSampler := NewTimeSampler(TimeSamplerID(0), bucketSize, tagsStore)
	statsdWorker := newTimeSamplerWorker(statsdSampler, DefaultFlushInterval, bufferSize, metricSamplePool, flushAndSerializeInParallel{enabled: false}, tagsStore)

	demux := &ServerlessDemultiplexer{
		aggregator:       aggregator,
		forwarder:        forwarder,
		statsdSampler:    statsdSampler,
		statsdWorker:     statsdWorker,
		serializer:       serializer,
		metricSamplePool: metricSamplePool,
		senders:          newSenders(aggregator),
		flushLock:        &sync.Mutex{},
	}

	// set the global instance
	demultiplexerInstance = demux

	// start routines
	go statsdWorker.run()
	go demux.Run()

	// we're done with the initialization
	return demux
}

// Run runs all demultiplexer parts
func (d *ServerlessDemultiplexer) Run() {
	if d.forwarder != nil {
		d.forwarder.Start() //nolint:errcheck
		log.Debug("Forwarder started")
	} else {
		log.Debug("not starting the forwarder")
	}

	log.Debug("Demultiplexer started")
	d.aggregator.run()
}

// Stop stops the wrapped aggregator and the forwarder.
func (d *ServerlessDemultiplexer) Stop(flush bool) {
	if flush {
		d.ForceFlushToSerializer(time.Now(), true)
	}

	d.statsdWorker.stop()

	// no need to flush the aggregator, it doesn't contain any data
	// for the serverless agent
	d.aggregator.Stop()

	if d.forwarder != nil {
		d.forwarder.Stop()
	}
}

// ForceFlushToSerializer flushes all data from the time sampler to the serializer.
func (d *ServerlessDemultiplexer) ForceFlushToSerializer(start time.Time, waitForSerializer bool) {
	d.flushLock.Lock()
	defer d.flushLock.Unlock()

	flushedSeries := make([]metrics.Series, 0)
	flushedSketches := make([]metrics.SketchSeriesList, 0)

	trigger := flushTrigger{
		trigger: trigger{
			time:              start,
			blockChan:         make(chan struct{}),
			waitForSerializer: waitForSerializer,
		},
		flushedSeries:   &flushedSeries,
		flushedSketches: &flushedSketches,
	}

	d.statsdWorker.flushChan <- trigger
	<-trigger.blockChan

	var series metrics.Series
	for _, s := range flushedSeries {
		series = append(series, s...)
	}
	var sketches metrics.SketchSeriesList
	for _, s := range flushedSketches {
		sketches = append(sketches, s...)
	}

	d.serializer.SendSeries(series) //nolint:errcheck
	log.DebugfServerless("Sending sketches payload : %+v", sketches)
	if len(sketches) > 0 {
		d.serializer.SendSketch(sketches) //nolint:errcheck
	}
}

// AddTimeSample send a MetricSample to the TimeSampler.
func (d *ServerlessDemultiplexer) AddTimeSample(sample metrics.MetricSample) {
	d.flushLock.Lock()
	defer d.flushLock.Unlock()
	batch := d.GetMetricSamplePool().GetBatch()
	batch[0] = sample
	d.statsdWorker.samplesChan <- batch[:1]
}

// AddTimeSampleBatch send a MetricSampleBatch to the TimeSampler.
// The ServerlessDemultiplexer is not using sharding in its DogStatsD pipeline,
// the `shard` parameter is ignored.
// In the Serverless Agent, consider using `AddTimeSample` instead.
func (d *ServerlessDemultiplexer) AddTimeSampleBatch(shard TimeSamplerID, samples metrics.MetricSampleBatch) {
	d.flushLock.Lock()
	defer d.flushLock.Unlock()
	d.statsdWorker.samplesChan <- samples
}

// AddCheckSample doesn't do anything in the Serverless Agent implementation.
func (d *ServerlessDemultiplexer) AddCheckSample(sample metrics.MetricSample) {
	panic("not implemented.")
}

// Serializer returns the shared serializer
func (d *ServerlessDemultiplexer) Serializer() serializer.MetricSerializer {
	return d.serializer
}

// Aggregator returns the main buffered aggregator
func (d *ServerlessDemultiplexer) Aggregator() *BufferedAggregator {
	return d.aggregator
}

// GetMetricSamplePool returns a shared resource used in the whole DogStatsD
// pipeline to re-use metric samples slices: the server is getting a slice
// and filling it with samples, the rest of the pipeline process them the
// end of line (the time sampler) is putting back the slice in the pool.
// Main idea is to reduce the garbage generated by slices allocation.
func (d *ServerlessDemultiplexer) GetMetricSamplePool() *metrics.MetricSamplePool {
	return d.metricSamplePool
}
